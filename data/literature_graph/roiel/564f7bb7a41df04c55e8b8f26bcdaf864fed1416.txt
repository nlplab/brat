Title: Changepoint detection for high-dimensional time series with missing data

Abstract: This paper describes a novel approach to changepoint detection when the observed high-dimensional data may have missing elements. The performance of classical methods for changepoint detection typically scales poorly with the dimensionality of the data, so that a large number of observations are collected after the true changepoint before it can be reliably detected. Furthermore, missing components in the observed data handicap conventional approaches. The proposed method addresses these challenges by modeling the dynamic distribution underlying the data as lying close to a time-varying low-dimensional submanifold embedded within the ambient observation space. Specifically, streaming data is used to track a submanifold approximation, measure deviations from this approximation, and calculate a series of statistics of the deviations for determining when the underlying manifold has changed in a sharp or unexpected manner. The approach described in this paper leverages several recent results in the field of high-dimensional data analysis, including subspace tracking with missing data, multiscale analysis techniques for point clouds, online optimization, and changepoint detection performance analysis. Simulations and experiments highlight the robustness and efficacy of the proposed approach in detecting an abrupt change in an otherwise slowly-varying low-dimensional manifold.

Content: Introduction

Changepoint detection is a form of anomaly detection where the anomalies of interest are abrupt temporal changes in a stochastic process [1] [2]. A good changepoint detection algorithm will accept a sequence of random variables whose distribution may change abruptly at one time, detect such a change as soon as possible, and also have long period between false detections. In many modern applications, the stochastic process is non-stationary away from the changepoints and very high dimensional, resulting in significant statistical and computational challenges. For instance, we may wish to quickly identify changes in network traffic patterns [3], social network interactions [4], surveillance video [5], or solar flare imagery [6] [7]. Traditional changepoint detection methods typically deal with a sequence of low-dimensional, often scalar, random variables. Na¨ıvely applying these approaches to high-dimensional data is impractical because the underlying highdimensional distribution cannot be accurately estimated and used for developing test statistics. This results in detection delays and false alarm rates that scale poorly with the dimensionality of the problem. Thus the primary challenge here is to develop a rigorous method for extracting meaningful low-dimensional statistics from the highdimensional data stream without making restrictive modeling assumptions. Our method addresses these challenges by using multiscale online manifold learning to extract univariate changepoint detection test statistics from high-dimensional data. We model the dynamic distribution underlying the data as lying close to a time-varying, low-dimensional submanifold embedded within the ambient observation space. This submanifold model, while non-parametric, allows us to generate meaningful test statistics for robust and reliable changepoint detection, and the multiscale structure allows for fast, memory-efficient computations. Furthermore, these statistics can be calculated even when elements are missing from the observation vector. While manifold learning has received significant attention in the machine learning literature [8] [9] [10] [11] [12] [13] [14], online learning of a dynamic manifold remains a significant challenge, both algorithmically and statistically. Most existing methods are " batch " , in that they are designed to process a collection of independent observations all lying near the same static submanifold, and all data is available for processing simultaneously. In contrast, our interest lies with " online " algorithms, which accept streaming data and sequentially update an estimate of the underlying dynamic submanifold structure, and changepoint detection methods which identify significant changes in the submanifold structure rapidly and reliably. Recent progress for a very special case of submanifolds appears in the context of subspace tracking. For example, the Grassmannian Rank-One Update Subspace Estimation (GROUSE) [15] and Parallel Estimation and Tracking by REcursive Least Squares (PETRELS) [16] [17] effectively track a single subspace using incomplete data vectors. The subspace model used in these methods, however, provides a poor fit to data sampled from a manifold with non-negligible curvature.

Related work

At its core, our method basically tracks a time-varying probability distribution underlying the observed data, and uses this distribution to generate statistics for effective changepoint detection. For sequential density estimation problems such as this, it is natural to consider an online kernel density estimation (KDE) method. A naive variant of online KDEs would be quite challenging in our setting, however, because if we model the density using a kernel at each observed data point, then the amount of memory and computation required increases linearly with time and is poorly suited to large-scale streaming data problems. Ad-hoc " compression " or " kernel herding " methods for online kernel density estimation address this challenge [18] but face computational hurdles. Furthermore, choosing the kernel bandwidth, and particularly allowing it to vary spatially and temporally, is a significant challenge. Recent works consider variable bandwidth selection using expert strategies which increase memory requirements [19] [20]. Some of these issues are addressed by the RODEO method [21], but the sparse additive model assumed in that work limits the applicability of the approach; our proposed method is applicable to much broader classes of highdimensional densities. Finally, in high-dimensional settings asymmetric kernels which are not necessarily symmetric or coordinate-aligned appear essential for approximating densities on low-dimensional manifolds, but learning timevarying , spatially-varying, and anisotropic kernels remains an open problem. In a sense, our approach can be considered a memory-efficient sparse online kernel density estimation method, where we only track a small number of kernels, and we allow the number of kernels, the center of each kernel, and the shape of each kernel to adapt to new data over time. Our approach also has close connections with Gaussian Mixture Models (GMMs) [22] [23] [24]. The basic idea here is to approximate a probability density with a mixture of Gaussian distributions, each with its own mean and covariance matrix. The number of mixture components is typically fixed, limiting the memory demands of the estimate, and online expectation-maximization algorithms can be used to track a time-varying density [25]. However, this approach faces several challenges in our setting. In particular, choosing the number of mixture components is challenging even in batch settings, and the issue is aggravated in online settings where the ideal number of mixture components may vary over time. Furthermore, without additional modeling assumptions, tracking the covariance matrices for each of the mixture components is very ill-posed in high-dimensional settings. Our approach is closely related to Geometric Multi-Resolution Analysis (GMRA) [14], which was developed for analyzing intrinsically low-dimensional point clouds in high-dimensional spaces. The basic idea of GMRA is to first iteratively partition a dataset to form a multiscale collection of subsets of the data, then find a low-rank approximation for the data in each subset, and finally efficiently encode the difference between the low-rank approximations at different scales. This approach is a batch method without a straightforward extension to online settings.

Motivating applications

The proposed method is applicable in a wide variety of settings. Consider a video surveillance problem. Many modern sensors collect massive video streams which cannot be analyzed by human due to the sheer volume of data; for example, the ARGUS system developed by BAE Systems is reported to collect video-rate gigapixel imagery [26] [27], and the Solar Dynamics Observatory (SDO) collects huge quantities of solar motion imagery " in multiple wavelengths to [help solar physicists] link changes in the surface to interior changes " [28]. Solar flares have a close connection with geomagnetic storms, which can potentially cause large-scale power-grid failure. In recent years the sun has entered a phase of intense activity, which makes monitoring of solar flare bursts an even more important task [7]. With these issues in mind, it is clear that somehow prioritizing the available data for detailed analysis is an essential step in the timely analysis of such data. If we can reliably detect statistically significant changes in the video, we can focus analysts' attention on salient aspects of the dynamic scene. For example, we may wish to detect a solar flare in a sequence of solar images in real time without an explicit model for flares, or detect anomalous behaviours in surveillance video [29]. Saliency detection has been tackled previously [30] [31], but most methods do not track gradual changes in the scene composition and do not detect temporal changepoints. A second motivating example is credit history monitoring, where we are interested in monitoring the spending pattern of a user and raising an alarm if a user's spending pattern is likely to result a default [32]. Here normal spending patterns may evolve over time, but we would expect a sharp change in the case of a stolen identity. An additional potential application arises in computer network anomaly detection [33]. Malicious attacks or network failure can significantly affect the characteristics of a network [3] [34]. Recent work has shown that network traffic data is well-characterized using submanifold structure [35], and using such models may lead to more rapid detection of changepoints with fewer false alarms.

Contributions and paper organization

The primary contributions of this work are two-fold: we present (a) a fast method for online tracking of a dynamic submanifold underlying very high-dimensional noisy data with missing elements and (b) a principled changepoint detection method based on easily computed residuals of our online submanifold approximation. These methods are supported by both theoretical analyses and numerical experiments on simulated and real data. The paper is organized as follows. In Section 2 we formally define our setting and problem. Section 3 describes our multiscale submanifold model and tracking algorithm, which is used to generate the statistics used in the changepoint detection component described in Section 4. Several theoretical aspects of the performance of our method are described in Section 5, and the performance is illustrated in several numerical examples in Section 6.

Problem Formulation

Suppose we are given a sequence of data x 1 , x 2 , . . . ,, and for t = 1, 2, . . ., x t ∈ R D , where D denotes the ambient dimension. The data are noisy measurements of points lying on a submanifold v t ∈ M t : x t = v t + w t . (1) The intrinsic dimension of the submanifold M t is d. We assume d ≪ D. The noise w t is a zero mean white Gaussian random vector with covariance matrix σ 2 I. The underlying submanifold M t may vary slowly with time. At each time t, we only observe a partial vector x t at locations Ω t ∈ {1, . . . , D}. Let P Ωt represent the |Ω t | × D matrix that selects the axes of R D indexed by Ω t ; we observe P Ωt x t . Our goal is to design an online algorithm that generates a sequence of approximations M t which tracks M t when it varies slowly, and detects anomalies as soon as possible when the submanifold changes abruptly. The premise is that the statistical properties of the tracking error will be different when the submanifold varies slowly versus when it changes abruptly. Define the operator P M x t = arg min x∈M x − x t 2 (2) as the projection of observation x t on to M. If we had access to all the data simultaneously without any memory constraints, we might solve the following batch optimization problem using all data up to time t for an approximation: M • t arg min M t i=1 α t−i P Ωi (x i − P M x i ) 2 + µ pen(M) , (3) where x denotes the Euclidean norm of a vector x, pen(M) denotes a regularization term which penalizes the complexity of M, α ∈ (0, 1] is a discounting factor on the approximation error at each time t, and µ is a userdetermined constant that specifies the relative weights of the data fit and regularization terms. Note that (3) cannot be solved without retaining all previous data in memory, which is impractical for the applications of interest. To address this, we instead consider an approximation to the cost function in (3) of the form F (M) + µ pen(M). To develop an online algorithm, instead of solving (3), we find a sequence of approximations M 1 , . . . , M t (without storing historic data), such that M t+1 is computed by updating the previous approximation M t using F (M) and the current datum x t+1 . In Section 3, we will present several forms of F (M) that lead to recursive updates and efficient tracking algorithms. One example of a MOUSSE approximation is illustrated in Figure 1; the context is described in more detail in Section 6.2. Given the sequence of submanifold estimates M 1 , . . . , M t , we can compute the distance of each x t to M t , which we denote {e t }. We then apply changepoint detection methods to the sequence of tracking errors {e t }. In particular, we assume that when there is no anomaly, the e t are i.i.d. with distribution ν 0 . When there is an anomaly, there exists an unknown time κ < t such that before the changepoint e 1 , . . . , e κ are i.i.d. with distribution ν 0 , and after the changepoint, e κ+1 , . . . are i.i.d. with distribution ν 1 . Our goal is to detect the anomaly as quickly as possible after it occurs, and make as few false alarms as possible. 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 Figure 1: Approximation of

MOUSSE

at t = 250 (upper) and t = 1150 (lower) of a 100-dimensional submanifold. In this figure we project everything into three-dimensional space. The blue curve corresponds to true submanifold, the dots are noisy samples from the submanifold (the lighter dots are more dated than the darker dots), and the red line segments are the approximation with MOUSSE. As the curvature of the submanifold increases, MOUSSE also adapts in the number of line segments. In the following we describe the Multiscale Online Union of SubSpaces Estimation (MOUSSE) method, including the underlying multiscale model and online update approaches. 3.1 Multiscale union of subspaces model MOUSSE uses a union of low-dimensional subsets M t to approximate M t , and organizes these subsets using a tree structure. The idea for a multiscale tree structure is drawn from the multiscale harmonic analysis literature [36]. The leaves of the tree are subsets that are used for the submanifold approximation. Each node in the tree represents a local approximation to the submanifold at one scale. The parent nodes are subspaces that contain coarser approximations to the submanifold than their children. The subset associated with a parent node roughly covers the subsets associated with its two children. More specifically, our approximation at each time t consists of a union of subspaces S j,k,t that is organized using a tree structure. Here j ∈ {1, . . . , J t } denotes the scale or level of the subset in the tree, where J t is the tree depth at time t, and k ∈ {1, . . . , 2 j } denotes the index of the subset for that level. The approximation M t at time t is given by: M t = (j,k)∈At S j,k,t . (4) where A t contains the indices of all leaf nodes used for approximation at time t. Also define T t to be the set of indices of all nodes in the tree at time t, with A t ⊂ T t . Each of these subsets lies on a low-dimensional hyperplane with dimension d and is parameterized as S j,k,t = {v ∈ R D : v = U j,k,t z + c j,k,t , z ⊤ Λ −1 j,k,t z ≤ 1, z ∈ R d }. (5) where the notation ⊤ denotes transpose of a matrix or vector. The matrix U j,k,t ∈ R D×d is the subspace basis, and c j,k,t ∈ R D is the offset of the hyperplane from the origin. The diagonal matrix Λ j,k,t diag{λ (1) j,k,t , . . . , λ (d) j,k,t } ∈ R d×d , with λ (1) j,k,t ≥ . . . ≥ λ (d) j,k,t ≥ 0, contains eigenvalues of the covariance matrix of the projected data onto each subspace. This parameter specifies the shape of the ellipsoid by capturing the spread of the data within the subset. In summary, the parameters for S j,k,t are {U j,k,t , c j,k,t , Λ j,k,t } (j,k)∈Tt , and these parameters will be updated online. In our tree structure, the leaf nodes of the tree also have two virtual children nodes that keep necessary information used when further partitioning is needed. The complexity of the approximation is defined to be the total number of subsets used for approximation at time t: K t |A t |, (6) which is used as the complexity regularization term in ( 3) pen( M t ) K t . (7) The tree structure is illustrated in Figure 2. S 0,0,t S 1,0,t S 1,1,t S 2,0,t S 2,1,t S 2,2,t S 2,3,t S 3,4,t S 3,5,t S 3,6,t S 3,7,t Virtual nodes keep track of statistics used for tree splitting Leaf nodes form current aproximation Ancestor nodes give coarser approximation and facilitate merging leaf nodes Figure 2: Illustration of tree structure for subspaces. The subspaces used in our approximation are {S 1,0,t ∪ S 2,2,t ∪ S 2,3,t }.

MOUSSE Algorithm

When a new sample x t+1 becomes available, MOUSSE updates M t to obtain M t+1 . The update steps are presented in Algorithm 1; there are three main steps, detailed in the below subsections: (a) find the subset in the M t which is closest to x t+1 , (b) update a tracking estimate of that closest subset and its ancestors, and (c) grow or prune the tree structure to preserve a balance between fit to data and complexity. We use [z] m to denote the m-th element of a vector z, and z ⊤ to denote the transpose of a matrix or vector z.

Distances for MOUSSE

To update the submanifold approximation, we first determine the affinity of x t+1 to each subset. We might use Euclidean distance, but this distance is problematic since in our approximation each subspace is local with boundary defined by an ellipsoid. Hence, a point can be close to a hyperplane but far away from the center of the ellipsoid. An alternative choice is the Mahalanobis distance, but this distance is only finite and well-defined for points lying in one of the low dimensional subspaces in our approximation. Since our construction is a piecewise linear approximation to a submanifold which may have some curvature, we anticipate many observations which are near but not in our collection of subspaces, and we need a well-defined, finite distance measure for such points. To address these challenges, we introduce the approximate Mahalanobis distance of a point x to a subspace S. Assume x with support Ω, parameter δ, and the parameters for a set S is given by {U, c, Λ}. Define U Ω P Ω U ∈ R |Ω|×d , c Ω P Ω c ∈ R |Ω| , and x Ω = P Ω x ∈ R |Ω| . Define the pseudoinverse operator that computes the coefficients of a vector in the subspace spanned by V as V # (V ⊤ V ) −1 V ⊤ . (8) Algorithm 1 MOUSSE 1: Input: error tolerance ǫ, step size α, relative weight µ 2: Initialize tree structure, set ǫ 0 = 0 3: for t = 0, 1, . . . do

4:

Given new data x t+1 and its support Ω t+1 , find the minimum distance set S j * ,k * ,t according to (j * , k * ) = arg min (j,k)∈At ρ δ j,k,t (x t+1 , S j,k,t ) using ( 13) 5: Calculate: e t+1 = β t,⊥ using (15) 6: Update all ancestor nodes and closest virtual child node of (j * , k * ) using Algorithm 2 7: Calculate: ǫ t+1 = αǫ t + (1 − α)e 2 t+1 8: Denote parent node of (j * , k * ) as (j * − 1, k p ) and virtual child node closest to x t+1 as (j * + 1, k v )

9:

if ǫ t+1 > ǫ and d(x t+1 , S j * +1,kv ,t ) + µ(K t + 1) < e 2 t+1 + µK t then 10: Split (j * , k * ) using Algorithm 3 11: end if 12: if ǫ t+1 < ǫ and d(x t+1 , S j * −1,kp,t ) + µ(K t − 1) < e 2 t+1 + µK t then 13: Merge (j * , k * ) and its sibling using Algorithm 4 14: end if 15: Update A t and T t 16: end for Algorithm 2 Update node 1: Input: node index (j, k), α, δ and subspace parameters 2: Calculate: β and β ⊥ using (9) and (10) 3: Update: [c j,k,t+1 ] m = α[c j,k,t ] m + (1 − α)[x t+1 ] m , m ∈ Ω t+1 4: Update: λ (m) j,k,t+1 = αλ (m) j,k,t + (1 − α)[β] 2 m , m = 1, . . . , d 5: Update: δ j,k,t+1 = αδ j,k,t + (1 − α)β ⊥ 2 /(D − d) 6: Update basis U j,k,t using (modified) subspace tracking algorithm Algorithm 3 Split node (j, k) 1: Turn two virtual children nodes (j + 1, 2k) and (j + 1, 2k + 1) of node (j, k) into leaf nodes 2: Initialize virtual nodes (j + 1, 2k) and (j + 1, 2k + 1): k 1 = 2k k 2 = 2k + 1 c j+1,k1,t+1 = c j,k,t + λ (1) j,k,t u (1) j,k,t /2 c j+1,k2,t+1 = c j,k,t − λ (1) j,k,t u (1) j,k,t /2 U j+1,ki,t+1 = U j,k,t , i = 1, 2 λ (1) j+1,ki ,t+1 = λ (1) j,k,t /2, i = 1, 2 λ (m) j+1,ki ,t+1 = λ (m) j,k,t , m = 2, . . . , d, i = 1, 2 Algorithm 4 Merge (j, k) and its sibling 1: Make the parent node of (j, k) into a leaf node 2: Make (j, k) and its sibling into virtual children nodes of the newly created leaf 3: Delete all four virtual children nodes of (j, k) and its sibling When U is an orthogonal matrix, we have U # ≡ U ⊤ , but in general U # Ω = U ⊤ Ω . Let β = U # Ω (x Ω − c Ω ), (9) and β ⊥ = (I − U Ω U # Ω )(x Ω − c Ω ). (10) In this definition, β is the projection of x on U , and β ⊥ captures the energy of the projection residual. We denote Euclidean distance between x with support Ω and the subspace where S lies on as d(x, S) x Ω − c Ω − U Ω U # Ω (x Ω − c Ω ) 2 = β ⊥ 2 . (11) Next we introduce the approximate Mahalanobis distance, which is a hybrid of Euclidean distance and Mahalanobis distance. Mahalanobis distance is commonly used for data classification, which measures the quadratic distance of x to a set S of data with mean c = E{x} and covariance Σ = E{(x − c)(x − c) ⊤ }. Specifically, the Mahalanobis distance is defined as ̺(x, S) = (x − c) ⊤ Σ −1 (x − c). Assuming the covariance matrix has a low-rank structure with d large eigenvalues and D − d small eigenvalues, we can write the eigendecomposition of the covariance matrix Σ as Σ U U ⊥ Λ U U ⊥ ⊤ = U Λ 1 U ⊤ + U ⊥ Λ 2 U ⊤ ⊥ , where Λ = diag{λ 1 , . . . , λ D }, λ 1 ≥ . . . ≥ λ D , Λ 1 = diag{λ 1 , . . . , λ d }, Λ 2 = diag{λ d+1 , . . . , λ D }. If we further assume that the D − d small eigenvalues are all approximately equal to δ, i.e., Λ 2 ≈ δI, then ̺(x, S) ≈ (x − c) ⊤ U Λ −1 1 U ⊤ (x − c) + δ −1 U ⊤ ⊥ (x − c) 2 . (12) From here we introduce the approximate Mahalanobis distance when the data is low-dimensional: ρ δ (x, S) β ⊤ Λ −1 β + δ −1 β ⊥ 2 . (13) Note that when the data is complete, ρ δ (x, S) is equal to the right-hand-side of (12). With missing data, ρ δ (x, S) is an approximation to ̺(x, S).

Update subset parameters

When updating subspaces, we can update all subspaces in our multiscale representation and make the update step-size to be inversely proportional to the approximate Mahalanobis distance between the new sample and each subspace, which we refer to as the " update-all " approach. Alternatively, we can just update the subspace closest to x t+1 , its virtual children, and all its ancestor nodes, which we refer to as the " update-nearest " approach. The update-all approach is computationally more expensive, especially for high dimensional problems, so we focus our attention on the greedy update-nearest approach. The below approaches extend readily to the update-all setting, however. With the approximate Mahalanobis distance defined above, we can find the subset with minimum distance to the new datum x t+1 : (j * , k * ) = arg min (j,k) ρ δ j,k,t (x t+1 , S j,k,t ), and then update the parameters of that subset, all its ancestors in the tree, and its two virtual children. The update algorithm is summarized in Algorithm 2 which denotes the parameters associated with S j * ,k * ,t as (c, U, Λ, δ), and drops the j * , k * , and t indices for simplicity of presentation. The update of the center c, Λ and δ are straightforward. Using the definition in (2), we have P Mt x = P ⊤ Ω U Ω U # Ω (x Ω − c Ω ) + c; (14) that is, the projection onto the submanifold approximation is the projection onto the nearest subset. We further define the instantaneous approximation error of the submanifold at time t as: e t P Ωt (x t − P Mt x t ), (15) and note that this is equivalent to the norm of the orthogonal projection of x t onto S j * ,k * ,t , denoted β t,⊥ ; i.e., e t ≡ β t,⊥ . (16) Next we will focus on three approaches for updating U .

GROUSE

To use GROUSE subspace tracking in this context, we approximate the first term in (3) as Note the first term is a constant with respect to M, so we need only to consider the second term in computing an update. The basic idea is now to take a step in the direction of the instantaneous gradient of this cost function. Since M is constrained to be a union of subsets and the projection operator maps to the closest subset, this task corresponds to the basis update of GROUSE [15] with the cost function (assuming U is orthonormal and including the offset vector c). Following the same derivation as in [15], we have that df dU = −2P Ωt+1 (x t+1 − c − U β)β ⊤ −2rβ ⊤ , (19) where β is defined in (9), and r = P Ωt+1 (x t+1 − c − U β). since U ⊤ r = 0. We obtain that the update of U t using the Grassmannian gradient is given by U t+1 = U t + cos(ξη) − 1 β 2 U t ββ ⊤ + sin(ξη) r r β ⊤ β . where η > 0 is the step-size, and ξ = rU t β. The step-size η is chosen to be η = η 0 /x t+1 , for a constant η 0 > 0. 3.4.2 PETRELS Let x (1) , x (2) , x (3) , . . . denote a subsequence of the data such that each x (i) was drawn from the (j * , k * ) node in our multiscale approximation, and let n t denote the length of this subsequence. Then we can approximate F (M) as F (M) = nt i=1 α nt−i min z P Ω (i) (x (i) − c − U z) 2 , (20) where, as before, U and c correspond to the subspace parameters for subset S j * ,k * ,t . The minimization of F (M) in (20) can be accomplished using the PETRELS algorithm [37], yielding a solution which can be expressed recursively as follows. Denoting by [U ] m the m-th column of U , we have the update of U given by [U t+1 ] m = [U t ] m + I m∈Ωt ([U t a t+1 ] m − a ⊤ t+1 [U t ] m )(R m,t+1 ) # a t+1 , (21) for m = 1, . . . , D, where I A is the indicator function for event A, (·) # denotes pseudo-inverse of a matrix, and a t+1 = (U ⊤ t P Ωt+1 U t ) # U ⊤ t x t+1 . The second-order information in R m,t+1 can be computed recursively as (R m,t+1 ) # = α −1 (R m,t ) # + α −2 p m,t+1 1 + α −1 a ⊤ t+1 (R m,t ) # a t+1 (R m,t ) # a t a ⊤ t (R m,t ) # . (22) Note that PETRELS does not guarantee the orthogonality of U t+1 , which is important for quickly computing projections onto our submanifold approximation. To obtain orthonormal U t+1 , we may apply Gram-Schmidt orthonormalization after each update. We refer to this modification of PETRELS as PETRELS-GS. This orthogonalization requires an extra computational cost on the order of O(Dd 2 ) and may compromise the continuity of U t , i.e., the Frobenius norm U t+1 − U t F after the orthogonalization may not be small even when the corresponding subspaces are very close [38]. As a result, this orthogonalization may change the optimality of U t . A faster orthonormalization (FO) strategy with less computation which also preserves the continuity of U t is given in [38] For each update with complete data (the maximum computational complexity), the computational complexity of GROUSE is on the order of O(Dd), PETRELS-GS is O(Dd 2 ), and PETRELS-FO is O(Dd). More details about the relative performance of these three subspace update methods can be found in Section 6.

Tree structure update

When the curvature of the submanifold changes and cannot be sufficiently characterized by the current subset approximations, we must perform adaptive model selection. This can be accomplished within our framework by updating the tree structure – growing the tree or pruning the tree, which we refer to as " splitting " and " merging " branches, respectively. Previous work has derived finite sample bounds and convergence rates of adaptive model selection in nonparametric time series prediction [39]. To decide whether to change the tree structure, we introduce the average approximation error: ǫ t t i=1 α t−i P Ωi (x i − P Mixi ) 2 = αǫ t−1 + (1 − α)e 2 t . (23) This error is an approximation to the first term in (3), where we replace P M with the projection onto a sequence of approximations P

Mi

. We will consider changing the tree structure when ǫ t is greater than our prescribed error tolerance ǫ > 0. Splitting tree branches increases the resolution of the approximation at the cost of higher estimator complexity. Merging reduces resolution but lowers complexity. When making decisions on splitting or merging, we take into consideration the approximation errors as well as the model complexity (the number of subspaces K t used in the approximation). This is related to complexity-regularized tree estimation methods [36, 40, 41] and the notion of minimum description length (MDL) in compression theory [42, 43]. In particular, we use the sum of the average fitting error and a penalty proportional to the number of subspaces used for approximation as the cost function when deciding to split or merge. The splitting and merging are summarized in Algorithm 3 and Algorithm 4. The splitting process mimics the k-means algorithm. In these algorithms, note that for node (j, k) the parent is node (j − 1, ⌊k/2⌋) and the sibling node is (j, k + 1) for k even or (j, k − 1) for k odd.

Initialization

To initialize MOUSSE, we assume a small initial training set of samples, and perform a nested bi-partition of the training data set to form a tree structure, as shown in Figure 2. The root of the tree represents the entire data set, and the children of each node represent a bipartition of the data in the parent node. The bipartition of the data can be performed by the k-means algorithm. We start with the entire data, estimate the sample covariance matrix, perform an eigendecomposition, extract the d-largest eigenvectors and eigenvalues and use them for U 1,1,0 and Λ 1,1,0 , respectively. The average of the (D − d) minor eigenvalues are used for δ 1,1,0 . If the approximation error is greater than the prescribed error tolerance ǫ, we further partition the data into two clusters using k-means (for k = 2) and repeat the above process. We keep partitioning the data until δ j,k,0 is less than ǫ for all leaf nodes. Then we further partition the data one level down and form the virtual nodes. This tree construction is similar to that used in [14]. In principle, it is possible to bypass this training phase and just initialize the tree with a single root node and two random virtual children nodes. However, the training phase makes it much easier to select algorithm parameters such as ǫ and provides more meaningful initial virtual nodes, thereby shortening the " burn in " time of the algorithm.

Choice of parameters

In general, α should be close to 1, as in the Recursive Least Squares (RLS) algorithm [44]. In the case when the submanifold changes quickly, we would expect smaller weights for approximation based on historical data and thus a smaller α. In contrast, a slowly evolving submanifold requires a larger α. In our experiments, α ranges from 0.8 to 0.95. ǫ controls the data fit error, which varies from problem to problem according to the smoothness of the submanifold underlying the data and the noise variance. Since the tree's complexity is controlled and pen(M) in (3) is roughly on the order of O(1), we usually set µ close to ǫ.

Changepoint detection

We are interested in detecting changes to the submanifold that arise abruptly and change the statistics of the data. When the submanifold varies slowly in time, MOUSSE described in Section 3 can track the submanifold and produce a sequence of stationary tracking errors. When an abrupt change occurs, MOUSSE loses track of the manifold and results in an abrupt increase in tracking errors. Hence, using tracking errors, we can develop a changepoint detection algorithm to detect abrupt changes in the submanifold.

CUSUM procedure

We adopt the widely used statistical CUSUM procedure [2, 45] for changepoint detection. In particular, we assume that ν 0 is a normal distribution with mean µ 0 and variance σ 2 0 , and ν 1 is a normal distribution with mean µ 1 and the same variance σ 2 0 . Then we can formulate the changepoint detection problem as the following hypothesis test: H 0 : e 1 , . . . , e t ∼ N (µ 0 , σ 2 0 ) H 1 : e 1 , . . . , e κ ∼ N (µ 0 , σ 2 0 ), e κ+1 , . . . , e t ∼ N (µ 1 , σ 2 0 ) (24) We assume µ 0 and σ 2 0 are known since typically there is enough normal data to estimate these parameters. (When the training phase is too short for this to be the case, these quantities can be estimated online, as described in [46].) However, we assume µ 1 is unknown since the magnitude of the changepoint can vary from one instance to another. In forming the detection statistic, we replace µ 1 by its maximum likelihood estimate (for each fixed changepoint time κ = k): ˆ µ 1 = S t − S k t − k , where S t t i=1 e i . This leads to the generalized CUSUM procedure, which computes the CUSUM statistic at each time t and stops the first time when the statistic hits threshold b: T = inf t ≥ 1 : max t−w≤k<t |(S t − S k ) − µ 0 (t − k)| σ 0 √ t − k ≥ b , (25) where w is a time-window length such that we only consider the most recent w errors for changepoint detection, and the threshold b is chosen to control the false-alarm-rate, which is characterized using average-run-length (ARL) in the changepoint detection literature [47]. Typically we would choose w to be several times (for example, 5 to 10 times) of the expected detection, then the window length will almost have no effect on the detection delay [48]. This threshold choice is detailed in Section 5.5.

Distribution of e t

In deriving the CUSUM statistics we have assumed that e t are i.i.d. Gaussian distributed. A fair question to ask is whether e t is truly Gaussian distributed, or even to ask whether e t is a good statistic to use. Consider the complete data case. ˆ e t P

Mt

(x t ) − x t . (26) In generaî e t has non-zero mean, since we approximate the manifold using a union of subspaces. Moreover, due to curvature of the manifold, in general each dimension of the error vector, [ˆ e t ] m , are not independent and the variances of [ˆ e t ] m may not be identical. In fact, no closed form expression for the distribution of e t = ˆ e t exists. However, a Gaussian distribution is a good approximation for the distribution of e t , since when D is large, e t averages over errors across D dimensions. The QQ-plot of e t from one of our numerical examples in Section 6 when D = 100 is shown in Figure 3. We will also demonstrate in Section 6.4 that the theoretical approximation for ARL using Gaussian assumption for e t is quite accurate. −4 −2 0 2 4 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 0.32

Quantile of Standard Gaussian Quantile of e t

Figure 3: Q-Q plot of e t , for a D = 100 submanifold.

Performance Analysis

In this section, we first study the performance of MOUSSE, and then study the choice for the threshold parameter of the changepoint detection algorithm and provide theoretical approximations. A complete proof of convergence of MOUSSE (or GROUSE or PETRELS) is hard since the space of submanifold approximations we consider is non-convex. Nevertheless, we can still characterize several aspects of our approach. In Sections 5.1 and 5.2 below, we assume that there is complete data, and we restrict our approximation to a single subspace so that K t = 1. Assume the mean and covariance matrix of the data are given by c ⋆ and Σ ⋆ , respectively. Assume the covariance matrix has low-rank structure: Σ ⋆ = diag{λ ⋆ 1 , . . . , λ ⋆ D } with λ m = δ ⋆ for m = d + 1, . . . , D. 5.1 Optimality of estimator for c ⋆ When there is only one subspace and the data are complete, the cost function (3) without the penalty term becomes min U,c t i=1 α t−i (I − U U ⊤ )(x i − c) 2 . (27) By writing each term as (I − U U ⊤ )(x i − c) 2 = (I − U U ⊤ )(x i − ¯ x + ¯ x − c) 2 , we can write the cost function as tr[(I − U U ⊤ )S(I − U U ⊤ )] + 1 − α t 1 − α (¯ x − c) ⊤ (I − U U ⊤ )(¯ x − c), (28) where ¯ x = t i=1 α t−i x i , S = t i=1 α t−i (x i − ¯ x)(x i − ¯ x) ⊤ . (29) Since the second term in (28) is quadratic in c, it is minimized by c = ¯ x. Also note that in this case the optimization problem (3) decouples in U and c. Since our online update for c t is given by c t+1 = αc t + (1 − α)x t , we have c t = (1 − α) t i=1 α t−i x i + α t c 0 , where the term α t c 0 is a bias introduced by initial condition c 0 . Since E{x t } = c ⋆ , E{c t } → (1 − α) · 1 1−α c ⋆ = c ⋆ . Hence our estimator for c is proportional to the minimizer for (3) and is asymptotically unbiased.

5.2

Consistency of estimators for Λ ⋆ and δ

⋆

In the following, we show that when there is complete data, if we have correct U = U ⋆ , then for each sample x t , its projection [β t ] m is an unbiased estimator for λ ⋆ m , and β t,⊥ 2 is an unbiased estimator for D m=d+1 λ ⋆ m . First note E{|[β t ] m | 2 } = E{[e ⊤ m U ⊤ (x t − c)] 2 } = e ⊤ m U ⊤ Σ ⋆ U e m = λ ⋆ m [U ] ⊤ m [U ] m = λ ⋆ m , (30) for m = 1, . . . , d, where e m denotes the m-th row of an identity matrix. We also have that E{β t,⊥ 2 } =E{(I − U U ⊤ )(x t − c) 2 } =tr{(I − U U ⊤ )Σ ⋆ (I − U U ⊤ )} = D m=d+1 λ ⋆ m . (31) Then from the MOUSSE update equations, as t → ∞ E{λ (m) t } = E{(1 − α) t i=1 α t−i |[β t ] m | 2 + α t λ (m) 0 } → λ ⋆ m , (32) for m = 1, . . . , d and E{δ t } = E{(1 − α) t i=1 α t−i β t,⊥ 2 /(D − d) + α t δ 0 } → 1 D−d D m=d+1 λ ⋆ m = δ ⋆ . (33) Hence our estimators for λ ⋆ m and δ ⋆ are asymptotically unbiased. Moreover, from (16), we have that E{e 2 t } = E{β t,⊥ 2 } = D m=d+1 λ ⋆ m . (34) Hence, the average approximation error of MOUSSE also converges E{ǫ t } = (1 − α) t i=1 α t−i E{e 2 i } → D m=d+1 λ ⋆ m .

MOUSSE approximation errors

As mentioned earlier, our multiscale subset model is closely related to geometric multiresolution analysis (GMRA) [14]. In that work, the authors characterize the favorable approximation capabilities of the proposed multiscale model. In particular, they prove that the magnitudes of the geometric wavelet coefficients associated with their algorithm decay asymptotically as a function of scale, so a collection of data lying on a smooth submanifold can be well-approximated with a small number (depending on the submanifold curvature) of relatively large geometric wavelets. These geometric wavelets are akin to the leaf nodes in our approximation, so the approximation results of [14] suggest that our model admits accurate approximations of data on smooth submanifolds with a small number of leafs.

Missing data

In Section 5.2 we have shown that with full data, our estimators are consistent. Note these estimators depend on β and β ⊥ . In the following we show that β and β ⊥ , when using a missing data projection, are close to their counterparts when using a complete data projection. Hence, when the fraction of missing data is not large, the performance of MOUSSE with missing data is also consistent. In this section, we omit the subscripts j, k and t, and denote Ω t by Ω to simplify notation. Define the coherence of the basis U as [49] coh(U ) = D d max m U P U e m 2 2 . (35) Theorem 1. Let ε > 0. Given x = v + w, and w is a white Gaussian noise with zero mean and covariance matrix σ 2 I D×D . Let β = U ⊤ (x − c), and β Ω = U # Ω (x Ω − c Ω ). If for some constant ℓ ∈ (0, 1), |Ω| ≥ max 8 3 coh(U )d log(2d/ε), 4 3 D (1 − ℓ) log(2D/ε) , then with probability at least 1 − 3ε, β Ω − β 2 2 ≤ 2 (1 + θ) 2 (1 − ℓ) 2 · d |Ω| · coh(U )q 2 + σ 2 (64/9)D 2 (1 − ℓ) 2 |Ω| 2 , (36) where θ = 2 D max D n=1 |[q] n | 2 q 2 log(1/ε), and q = (I − U U ⊤ )(v − c). This theorem shows that the fraction of non-zero entries, |Ω|, should be on the order of max{d log d} or {D/ log(D)} for accurate estimation of β Ω . We have a similar bound for the difference between β ⊥ and β Ω,⊥ , based on similar techniques used to derive Theorem 1. The first term in the error bound (36) is proportional to q, the approximation error related to the distance of v from U , and the second term in (36) is due to noise. Proof of this theorem can be found in Appendix A. This theorem can be viewed as an extension of [49, Theorem 1] to include noise, and also to directly bound β − β Ω instead of bounding v Ω − U Ω β Ω using v − U β.

5

.5 Choice of threshold for changepoint detection In accordance with standard changepoint detection notation, denote by E ∞ the expectation when there is no change, i.e., E H 0 , and by E k the expectation when there is a changepoint at κ = k, i.e., E H 1 ,κ=k . The performance metric for a changepoint detection algorithm is typically characterized by the expected detection delay sup k≥0 E k {T − k|T > k} and the average-run-length (ARL) E ∞ {T } [47]. Typically we use E 0 {T } as a performance metric since it is an upper bound for sup k≥0 E k {T − k|T > k}. Note that the CUSUM statistic (25) is equivalent to T = inf{t ≥ 1 : max t−w≤k<t | ˜ S t − ˜ S k | √ t − k ≥ b, } (37) where˜S t = t i=1 (e i − µ 0 )/σ 0 . Under H 0 , we have (e i − µ 0 )/σ 0 i.i.d. Gaussian distributed with zero mean and unit variance. Using the results in [50], we have the following approximation. When b → ∞, E ∞ {T } ∼ (2π) 1/2 exp{b 2 /2} b b 0 xν 2 (x)dx , (38) where ν(x) = (2/x)[Φ(x/2)−0.5] (x/2)Φ(x/2)+φ(x)/2 [48], φ(x) and Φ(x) are the pdf and cdf of the normal random variable with zero mean and unit variance. We will demonstrate in Section 6.4 that this asymptotic approximation is fairly accurate even for finite b, which allows us to choose the changepoint detection threshold to achieve a target ARL without parameter tuning.

Numerical Examples

In this section, we present several numerical examples, first based on simulated data, and then real data, to demonstrate the performance of MOUSSE in tracking a submanifold and detecting changepoints. We also verify that our theoretical approximation to ARL in Section 5.5 is quite accurate.

Tracking a static submanifold

We first study the performance of MOUSSE tracking a static submanifold. The dimension of the submanifold is D = 100 and the intrinsic dimension is d = 1. Fixing θ ∈ [−2, 2], we define v(θ) ∈ R D with its n-th element [v(θ)] n = 1/ √ 2πe −(zn−θ) 2 /(2γ 2 ) , (39) where γ = 0.6, and z n = −2 + 4n/D, n = 1, . . . , 100, corresponds to regularly spaced points between −2 and 2. This static submanifold is sampled by sampling different θ ∈ [−2, 2] and generating corresponding points on the submanifold according to (39). The observation x t is obtained from (1), where the noise variance is σ 2 = 4 × 10 −4 . We set parameter values as α = 0.95, ǫ = 0.1, µ = 0.03, and use PETRELS-FO. Figure 4 demonstrates that MOUSSE is able to track a static submanifold and reach the steady state quickly from a coarse initialization. In Figure 4 and the following numerical examples, the expected instantaneous fitting error is evaluated using N = 1200 draws from M, denoted y 1 , . . . , y N and computing the Monte Carlo estimate E{e t } ≈ 1 N N i=1 d(y i , S i ), (40) where S i denotes the minimum distance subset to y i . 10 0 10 1 10 2 10 3 2 4 6 t K t 10 0 10 1 10 2 10 3 0.1 0.2 0.3 0.4 t E{e t } Figure 4: MOUSSE tracking a static submanifold with D = 100 and d = 1.

6.2

Tracking a slowly time varying submanifold Next we track a slowly time varying submanifold using MOUSSE, in a similar setting to Section 6.1 where D = 100 and d = 1. The submanifold is also generated using (39), except that now we let γ to be time varying: γ t = 0.6 − γ 0 t t = 1, 2, . . . , s 0.6 − γ 0 (2s − t) t = s + 1, s + 2, . . . , 2s (41) where parameter γ 0 controls how fast the submanifold changes. We choose γ 0 = 2 × 10 −4 , s = 1000 with 20% missing data, and the parameters for MOUSSE are µ = 0.03, ǫ = 0.1, and α = 0.9. The result of the tracking can be found in an illustrative video in http://nislab.ee.duke.edu/MOUSSE/index.hml. Snapshots of this video at time t = 250 and t = 1150 are shown in Figure 1. In this display, the dashed line corresponds to the true submanifold, the red lines correspond to the estimated union of subspaces, and the + signs correspond to the past 500 samples, with darker colors corresponding to more recent observations. From this video, it is clear that we are effectively tracking the dynamics of the submanifold, and keeping the representation parsimonious so the number of subspaces used by our model is proportional to the curvature of the submanifold, and as the curvature increases and decreases, the number of subspaces used in our approximation similarly increases and decreases. The number of subspaces K t and fitting error as a function of time are shown in Figure 5. The red line in Figure 5 corresponds to ǫ. Note that MOUSSE is able to track the submanifold, in that it can maintain a stable number of leaf nodes in the approximation and meet the target error tolerance ǫ.

Comparison of tracking algorithms

We also compare the performance of different tracking algorithms presented in Section 3.4: GROUSE, PETRELS- GS and PETRELS-FO. We use E{e t } defined in (40) as a comparison metric. In comparing the three methods, we set the parameters for each tracking algorithm such that the algorithm has the best possible performance. The comparison is displayed in Figure 6, where the horizontal axis is the submanifold changing rate γ, the vertical axis is the percentage of missing data, and the brightness of each block corresponds to E{e t }. In Figure 6, PETRELS-FO performs better than GROUSE and PETRELS-GS, especially with a large percentage of missing data. Also note that for PETRELS-FO, the optimal parameters are fairly stable for various combinations of submanifold changing rate and percentage of missing data: the optimal parameters are α ≈ 0.9, µ ≈ 0.2, and ǫ ≈ 0.1. Considering its lower computational cost and ease of parameter tuning, we use PETRELS-FO with MOUSSE for the remaining experiments in this paper.

Changepoint detection example

To verify our theoretical approximation for ARL, we perform Monte Carlo simulation. Direct simulation of T to obtain E ∞ {T } is very time-consuming because we typically want to choose a b such that E ∞ {T } is on the order of 10000. Hence we use an indirect simulation method commonly used in changepoint detection [48]. The indirect 0 500 1000 1500 2000 4 6 8 t K t 0 500 1000 1500 2000 0.02 0.04 0.06 0.08 0.1 t !

t

Figure 5: MOUSSE tracking a slowly evolving submanifold with D = 100 and d = 1. Dashed red line depicts target error tolerance ǫ. Changing rate (× 10 −4 ) Percentage of missing 2 4 6 8 10 10% 30% 50% 70% 0 0.2 0.4 0.6 0.8 1 (a) E{et} of MOUSSE using GROUSE Changing rate (× 10 −4 ) Percentage of missing 2 4 6 8 10 10% 30% 50% 70% 0 0.2 0.4 0.6 0.8

1

(b) E{et} of MOUSSE using PETRELS- GS Changing rate (× 10 −4 ) Percentage of missing 2 4 6 8 10 10% 30% 50% 70% 0 0.2 0.4 0.6 0.8

1

(c) E{et} of MOUSSE using PETRELS- FO Figure 6: MOUSSE tracking a slowly varying submanifold using: (a) GROUSE, (b) PETRELS-GS and (c) PETRELS-FO. Horizontal axis corresponds to rate of submanifold's change and vertical axis corresponds to fraction of data missing. Brightness corresponds to E{e t }. method is based on the fact that when there is no changepoint, for large b, T is typically exponentially distributed. Hence, we have under H 0 , P{T > m} = e −m/E ∞ {T } , which means we can simulate P{T > m} for fixed m and b, and then obtain under H 0 E ∞ {T } = −m/ log P{T > m}. (42) Using this formula, we generate 10000 Monte Carlo (MC) trials, each a slowly time-varying submanifold of duration t = 500. Then we use MOUSSE to track the submanifold, and obtain a sequence of errors. We form the CUSUM statistics using this sequence of errors, and find the fraction of sequences such that max 1≤t≤500 max t−w≤k≤t 1 σ 0 |(S t − S k ) − µ 0 (t − k)| √ t − k ≥ b, which is the estimate for P{T > 500}. Then we use the above formula to obtain E ∞ {T }. TABLE 1 shows the value of b suggested by theory for different ARLs and the value of b computed using the MC procedure described above. For comparison, we also obtain the ARL by treating this as a single subspace tracking problem for which PETRELS-FO is employed. These values of b are in parentheses. To estimate the expected detection delay, we generate instances where the γ t in the model has an abrupt jump ∆ γ at time t = 200. γ t = 0.6 − γ 0 t t = 1, 2, . . . , 199 0.6 − ∆ γ − γ 0 t t = 200, 201, . . . , 400 (43) Table 1: Average run length (ARL) E ∞ {T }. In all cases the theoretical value is close to the MC estimate. Then we apply the CUSUM statistics and find the expected detection delay E 0 {T } with respect to t = 200 using 10000 Monte Carlo trials. We compare the expected detection delay of MOUSSE and the single subspace tracking method. Results corresponding to big (∆ γ = 0.05) and small (∆ γ = 0.03) magnitude of jump of γ t are given in TABLE 2, 3 respectively. Again, values in parenthesis are obtained by using single subspace tracking. The threshold b's are chosen according to the Monte Carlo results given in TABLE 1; e.g., for the cell corresponding to ARL = 1000 and 0% missing data in TABLE 2 or 3, b should be set as 4.55 for MOUSSE and 4.28 for the single subspace method. TABLE 2, 3 demonstrate that MOUSSE has much smaller expected detection delay than a single subspace method. Table 2: Detection delay when jump of γ t is ∆ γ = 0.05. Delays are significantly shorter with MOUSSE than with single subspace tracking. Table 3: Detection delay when jump of γ t is ∆ γ = 0.03. Delays are significantly shorter with MOUSSE than with single subspace tracking. A video from the Solar Data Observatory, which demonstrates an abrupt emergence of a solar flare, can be found on http://nislab.ee.duke.edu/MOUSSE/index.html. The Solar Object Locator for the original data is SOL2011-04- 30T21-45-49L061C108. Also displayed is the residuaî e t of (26) obtained using MOUSSE, which clearly shows peaks in the vicinity of the solar flare. A frame from this dataset during a solar flare is shown in Figure 7a. The frame is of size 232 × 292 resulting in 67744 dimensional online data. To reduce difficulty of parameter tuning, we scale the pixel intensities in the dataset by multiplying the data by a factor of 10 −4 to be consistent with the scale of our simulated data experiments. The parameters we use are ǫ = 0.3, µ = 0.03, and α = 0.85. The video and the snapshots in Figure 7 demonstrate that MOUSSE can not only detect the emergence of a solar flares but also localize the flare by presentingê t , and these tasks are accomplished far more effectively with MOUSSE (even with d = 1) than with a single subspace. Note that with the single subspace tracking, the residual norm e(t) is not a stationary time series prior to the flare and thus poorly suited for changepoint detection. In the original images, the background solar images has bright spots that are slowly and changing shape, which makes detection based on simple background subtraction incapable of detecting small transient flares. In contrast, with our approach, with K t around 10, the underlying manifold structure is better tracked and thus yields more obvious error e(t) when anomaly occurs. (a ) Snapshot of original SDO data at t = 227 (b) MOUSSE residual at t = 227 (c) Single subspace tracking residual at t = 227 50 100 150 200 250 300 0 1 2 3 4 5 t e t (d) et by MOUSSE 50 100 150 200 250 300 0 10 20 30 40 50 60 70 CUSUM t (e) CUSUM stats by MOUSSE 50 100 150 200 250 300 0 5 10 15 t e t (f ) et by single subspace tracking 50 100 150 200 250 300 0 20 40 60 80 CUSUM

t

(g) CUSUM stats by single subspace tracking Figure 7: Detection of solar flare at t = 227: (a) snapshot of original SDO data at t = 227; (b) MOUSSE residuaî e t , which clearly identifies an outburst of solar flare; (c) single subspace tracking residuaî e t , which gives a poor indication of the flare; (d) e(t) for MOUSSE which peaks near the flare around t = 227; (e) the CUSUM statistic (solid blue line) for MOUSSE with dashed red line indicating threshold b for ARL=10000; (f) e(t) for single subspace tracking; (g) the CUSUM statistic for single subspace tracking. Using a single subspace gives much less reliable estimates of significant changes in the statistics of the frames. Our second real data example is related to automatic identity theft detection. The basic idea is that consumers have typical spending patterns which change abruptly after identity theft. Banks would like to identify these changes as quickly as possible without triggering numerous false alarms. To test MOUSSE on this high-dimensional changepoint detection problem, we examined the E-commerce transaction history of people in a dataset used for a 2008 UCSD data mining competition at http://www.cs.purdue.edu/commugrate/data_access/all_data_sets_more.php?search_ For each person in this dataset, there is a time series of transactions. For each transaction we have a 31-dimensional real-valued feature vector and a label of whether the transaction is " good " (0) or " bad " (1). The full dataset was generated for a generic anomaly detection problem, so it generally is not appropriate for our setting. However, some of these transaction timeseries show a clear changepoint in the labels, and we applied MOUSSE to these timeseries. In particular, we use MOUSSE to track the 31-dimensional feature vector and detect a changepoint, and compare this with the " ground truth " changepoint in the label timeseries. In calculating the CUSUM statistic, we estimate the µ 0 and σ 0 of equation 25 from e 1 , . . . , e 20 . After t = 20, every time the CUSUM statistic exceeds the threshold b and an changepoint is detected, we " reset " the CUSUM to only consider e t after the most recently detected changepoint. This allows us to detect multiple changepoints in a timeseries. The effect of our procedure for one person's transaction history is displayed in Figure 8. We first see that MOUSSE accurately detects a temporally isolated outlier transaction at t = 38, after which the CUSUM is reset. After this, while MOUSSE does not generate particularly large spikes in e t , the associated CUSUM statistic shows a marked increase near t = 70 and hits the threshold at t = 72, when the labels (not used by MOUSSE) change from 0 (good) to 1 (bad). After this the CUSUM is repeatedly reset and repeatedly detects the change in the statistics of e t from the initial stationary process.

Conclusions

This paper describes a novel multiscale method for online tracking of high-dimensional data on a low-dimensional submanifold, and using the tracking residuals to perform fast and robust changepoint detection. Changepoint 20 40 60 80 100 8 10 12 K t 20 40 60 80 100 0 0.5 1 e t 20 40 60 80 100 0 10 20 CUSUM 20 40 60 80 100 0 0.5 1 label t (a) Obtained from MOUSSE t attributes 20 40 60 80 100 5 10 15 20 25 30 −0.2 0 0.2 0.4 0.6 (b ) Visualization of time-varying Attributes Figure 8: Credit card user data experiments. (a) From top to bottom: number of leaf nodes used by MOUSSE; e t ; CUSUM statistic (solid blue line) and theoretical threshold b corresponding to ARL = 10000 (dashed red line); ground truth label. Note that the CUSUM statistic has a false alarm due to an outlier at t = 38, and it starts increasing at t = 70 and frequently hits the threshold afterwards due to the changepoint at t = 70. In this case CUSUM catches both the outlier and the changepoint. (b) Demonstration of the time-varying x t (user attributes): each column corresponds to the 31-dimensional attribute vector at a given time. The white spots corespond to the outlier at time t = 38. detection is an important subset of anomaly detection problems due to the ever-increasing volume of streaming data which must be efficiently prioritized and analyzed. The multiscale structure at the heart of our method is based on a Geometric MultiResolution Analysis which facilitates low-complexity piecewise-linear approximations to a manifold. The multiscale structure allows for fast updates of the manifold estimate and flexible approximations which can adapt to the changing curvature of a dynamic submanifold. These ideas have the potential to play an important role in analyzing large volumes of streaming data, which arises in remote sensing, credit monitoring, and network traffic analysis. While the algorithm proposed in this paper has been focused on unions of subspaces, an important open question is whether similar techniques could be efficiently adopted based on sparse covariance matrix selection [51, 52]. The resulting approximation space may no longer correspond to a low-dimensional submanifold, but such structures provide good representations of high-dimensional data in many settings, and our future work includes tracking the evolution of a mixture of such structures. Issues related to non-Gaussian observation models, inverse problem settings, dynamical models, and optimal selection of the statistic used for changepoint detection (i.e., alternatives to e t , as considered in [53]) all pose additional interesting open problems. Note that U ⊤ w is zero-mean Gaussian random vector with covariance matrix σ 2 U ⊤ U = σ 2 I. Next we consider the missing data case. Recall P Ω ∈ R |Ω|×D is a projection matrix. Define w Ω = P Ω w. From (9) we have β Ω = U # Ω (v Ω − c Ω ) + U # Ω w Ω (45) Suppose in (1) we write v − c = p + q, with p ∈ S and q ∈ S ⊥ , where S ⊥ denotes the orthogonal subspace of S. Hence, p = U U ⊤ (v − c) and q = (I − U U ⊤ )(v − c). Let p Ω = P Ω p, q Ω = P Ω q. Hence, v Ω − c Ω = p Ω + q Ω . So β Ω = U ⊤ (v − c) + U # Ω q Ω + U # Ω w Ω . Hence β Ω − β 2 ≤ 2U # Ω q Ω 2 + 2U # Ω w Ω − U ⊤ w 2 = 2(U ⊤ Ω U Ω ) −1 U ⊤ Ω q Ω 2 + 2[(U ⊤ Ω U Ω ) −1 U ⊤ Ω P Ω − U ⊤ ]w 2 We will bound these two terms separately. First, note that (U ⊤ Ω U Ω ) −1 U ⊤ Ω q Ω 2 ≤ (U ⊤ Ω U Ω ) −1 2 2 U ⊤ Ω q Ω 2 (49) where A 2 denotes the spectral norm of matrix A. Using [Lemma 2] in [49], we have that with probability 1 − ε, if |Ω| ≥ 8 3 dcoh(U ) log(2d/ε), U ⊤ Ω q Ω 2 ≤ (1 + θ) 2 |Ω| D d D coh(U )q 2 , where θ = 2 max D n=1 |[q] n| 2 q 2 log(1/ε). Using [Lemma 3] in [49] we have that provided that 0 < ℓ < 1, with probability at least 1 − ε, Combine these with (49), we have that with probability 1 − 2ε, (U ⊤ Ω U Ω ) −1 U ⊤ Ω q Ω 2 ≤ (1 + θ) 2 (1 − ℓ) 2 · d |Ω| · coh(U )q 2 . (51) Next we examine the noise term. Define ˜ w = [(U ⊤ Ω U Ω ) −1 U ⊤ Ω P Ω − U ⊤ ]w, which is a zero-mean Gaussian random vector with covariance matrix Γ = σ 2 (U ⊤ Ω U Ω ) −1 − σ 2 I, where we have used the fact that P Ω P ⊤ Ω = I. Hence we bound the tail of the noise power using Markov inequality: provided that τ is sufficiently large such that the maximum eigenvalue is smaller than τ : λ max ((U In the last inequality, we have used (50). Note that D/[(1 − ℓ)|Ω|] > 1, and the upper bound in (53) is smaller than 2D if τ > 4 3 ( D (1−ℓ)|Ω| − 1) or τ > 4 3 D (1−ℓ)|Ω| . Now we set 2De −τ = ε, if ε is sufficiently small such that < 32 9 D 2 σ

2

(1−ℓ) 2 |Ω| 2 with probability 1 − ε. Finally, combining (51) and (53), we obtain the statement in Theorem 1.