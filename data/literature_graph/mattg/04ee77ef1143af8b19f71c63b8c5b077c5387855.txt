Title: Ask Me Anything: Dynamic Memory Networks for Natural Language Processing

Abstract: Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), coreference resolution (Quizbowl dataset) and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclusively on trained word vector representations and requires no string matching or manually engineered features.

Content: Introduction

Question answering (QA) is a complex natural language processing task which requires an understanding of the meaning of a text and the ability to reason over relevant facts. Most, if not all, tasks in natural language processing can be cast as a question answering problem: high level tasks like machine translation (What is the translation into French?); sequence modeling tasks like named entity recognition [1] (NER) (What are the named entity tags in this sentence?) or part of speech tagging (POS) (What are the part of speech tags?); classification problems like sentiment analysis [2] (What is the sentiment?); even multi-sentence joint classification problems like coreference resolution (Who does " their " refer to?). The dynamic memory network (DMN) is a neural network based model which can be trained in an end-to-end fashion for any QA task using raw input-question-answer triplets. Fig. 1 shows input and question test sequences followed by answers given by the model. The DMN is illustrated in Fig. 2. Generally, it can solve sequence tagging tasks, classification problems, sequence to sequence tasks, and question answering tasks that require transitive reasoning. The DMN first processes all input, question and answer texts into sequences of semantic vector representations. The question representation triggers an iterative attention process that searches the input and retrieves relevant facts. The DMN then reasons over retrieved facts and provides an answer sequence model with an appropriate summary. 1 arXiv:1506. 07285v3 [cs.CL] 29 Sep 2015 2 The Dynamic Memory Network Framework The DMN is a general modeling framework for asking questions over inputs. We will give an overview of the full model, and then go into further detail for each module, as well as present our specific instantiation of each module for natural language processing. The goal of the DMN is to first compute a vector representation of an input, given a question, and to then generate the correct answer. It contains the following modules: Input Module: This module processes raw inputs and maps them to a representation that is useful for asking questions about this input. The input may be, for instance, an image, video, or audio signal. We focus on NLP in this paper. Hence, the input may be a sentence, a long story, a movie review, a news article, or all of Wikipedia. Semantic Memory Module: Semantic memory stores general knowledge about concepts and facts. For example, it might contain information about what a hang glider is. Initialization strategies such as distributed word vectors (Glove [3], Word2Vec [4]) are popular semantic memory components that have been shown to improve performance on many NLP tasks. More complex information can be stored in the form of knowledge bases that capture relationships in the form of triplets [5] or gazetteers, which have been useful for tasks such as named entity recognition or question answering [6]. Question Module: The question module computes a representation of a question such as Where did the author first fly? This representation triggers the episodic memory module to start an iterative attention process over facts from the input sequence. Episodic Memory Module: This is the central part of the DMN. A question draws attention to specific facts from the input sequence, which are reasoned over to update this module's memory state. This process then iterates, with each iteration providing the module with newly relevant information about the input. In other words, the module has the ability to retrieve new facts which were thought to be irrelevant in previous iterations. After several passes the module then summarizes its knowledge and provides the answer module with a final representation to produce an answer. Answer Module: Given a representation from the episodic memory module, the answer module generates the model's predicted answer.

Input Module

The input module computes a useful representation of the inputs such that relevant facts can be retrieved later. Generally, the input module may be thought of as computing the intermediate steps of a function that eventually returns a final vector representation. The input module sends these intermediate values to the episodic memory module, which will complete the computation, conditioned on the question, by way of its attention mechanism. We refer to the representations that the input module provides to the episodic memory as facts to be reasoned over by the episodic memory. Figure 2: Overview of DMN modules. Communication between them is indicated by arrows and uses only vector representations. Questions trigger gates which allow vectors for certain input words or sentences to be given to the episodic memory module. The final state of the episodic memory is the input to the answer module. In natural language processing, we have a sequence of T I words w I 1 , . . . , w I T I . The input module computes the hidden states of a recurrent sequence model [7]. We use Glove [3] vectors to capture context-independent representations, and initalize the embeddings of the DMN with these values. Word embeddings are given as inputs to the recurrent network to compute hidden fact states: c t = SEQ MODEL(L[w I t ], h t−1 ), where L is the embedding matrix and w I t is the tth word of the input sequence. In particular, we use a gated recurrent network (GRU) [8] [9]. We also explored the more complex LSTM [10] but it performed similarly and is more computationally expensive. Both work much better than the standard tanh RNN and we postulate that the main strength comes from having gates that allow the model to suffer less from the vanishing gradient problem [10]. GRU Definition: Assume each time step has an input x t and a hidden state h t . We will abbreviate the below computation with h t = GRU (x t , h t−1 ): z t = σ W (z) x t + U (z) h t−1 + b (z) ; r t = σ W (r) x t + U (r) h t−1 + b (r) (1) ˜ h t = tanh W x t + r t • U h t−1 + b (h) ; h t = z t • h t−1 + (1 − z t ) @BULLET˜h t , (2) where • is an element-wise product, W (z) , W (r) , W ∈ R n H ×n I and U (z) , U (r) , U ∈ R n H ×n H . The dimensions n are hyperparameters. If we subsample the output of the recurrent network, the input module will return just the hidden states c t that correspond to end-of-sentence markers in the original story. Otherwise, the input module returns hidden states c t for all words. For notational convenience, in future modules we will refer to either of these states as c t , and say in the experiments section whether or not the subsampling was used.

Semantic Memory Module

The semantic memory consists of (i) stored word concepts and (ii) facts about them. We initialize embeddings to Glove vectors as described above. This module could include gazeteers or other forms of explicit knowledge bases, but in this work we do not use them.

Question Module

This module maps a question into a representation that can then be used for querying specific facts from the input module. We have questions that consist of sequences of T Q words w Q t . We compute a hidden state for each via q t = GRU (L[w Q t ], q t−1 ), where the GRU and embedding weights are shared with the input module. The final question vector is defined as q = q T Q . Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module

M a

r y g o t t h e m il k t h e r e . J o h n m o v e d t o t h e b e d r o o m . S a n d r a w e n t b a c k t o t h e k it c h e n . M a r y t r a v e ll e d t o t h e h a ll w a y . J o h n g o t t h e fo o t b a ll t h e r e . J o h n w e n t t o t h e h a ll w a y . J o h n p u t d o w n t h e fo o t b a ll . M a r y w e n t t o t h e g a r d e n . s1 s2 s3 s4 s5 s6 s7 s8 W h e r e is t h e fo o b a ll ? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 h a ll w a y < E O S > m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a specific question. Gate values g i t are shown above the corresponding vectors. The gates change with each search over inputs. We do not draw connections for gates that are close to zero. See Section 4.1 for details on the dataset that this example comes from.

Episodic Memory Module

The episodic memory module retrieves facts from the input module conditioned on the question. It then reasons over those facts to produce a final representation that the answer module will use to generate an answer. We refer to this representation as a memory. Importantly, we allow our module to take multiple passes over the facts, focusing attention on different facts at each pass. Each pass produces an episode, and these episodes are then summarized into the memory. Endowing our module with this episodic component allows its attention mechanism to attend more selectively to specific facts on each pass, as it can attend to other important facts at a later pass. It also allows for a type of transitive inference, since the first pass may uncover the need to retrieve additional facts. For instance, in the example in Fig. 3, we are asked Where is the football? In the first iteration, the model ought attend to sentence 7 (John put down the football.), as the question asks about the football. Only once the model sees that John is relevant can it reason the second iteration should retrieve where John was. In this example, taken from a true test question on Facebook's bAbI task, this behavior is indeed seen. Note that the second iteration has wrongly placed some weight in sentence 2, which makes some intuitive sense, as sentence 2 is another place John had been. In its general form, the episodic memory module is characterized by an attention mechanism, a function which returns an episode given the output of the attention mechanism and the facts from the input module, and a function that summarizes the episodes into a memory. In our work, we use a gating function as our attention mechanism. It takes as input, for each pass i, a candidate fact c t , a previous state m i−1 , and the question q to compute a gate: g i t = G(c t , m i−1 , q). The state is updated by way of a GRU: m i = GRU (e i , m i−1 ), where e i is the computed episode at pass i. The state may be initialized randomly, but in practice we have found that initializing it to the question vector itself helps; e.g, m 0 = q. The function G returns a single scalar and is defined as follows: z(c, m, q) = [c, m, q, c • q, c • m, |c − q|, |c − m|, c T W (b) q, c T W (b) m] (3) G(c, m, q) = σ W (2) tanh W (1) z(c, m, q) + b (1) + b (2) (4) To compute the episode for pass i, we employ a modified GRU over the sequence of T C facts c t , endowed with our gates. The episode is the final state of the GRU: h i t = g i t GRU (c t , h i t−1 ) + (1 − g i t )h i t−1 (5) e i = h i T C (6) Finally, to summarize the T P episodes e i into a memory, we use the same GRU that updates the attention mechanism's state: m i = GRU (e i , m i−1 ), and we set the memory m as m = m T P . This is equivalent to setting the memory to simply the attention mechanism's final state, but we have described it here as its own computation to highlight the potential modularity of these subcomponents. For datasets that mark which facts are important for a given question, such as Facebook's bAbI dataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classification error function. We also append a special end-of-passes representation to the facts, and stop the iterative attention process if this representation is chosen by the gate function. Otherwise, for datasets without explicit supervision, we set a maximum number of passes. The whole module is end-to-end differentiable.

Sequence Modeling

It is straightforward to apply the DMN to sequence modeling. In the sequence modeling task, we wish to label each word in the original sequence. Therefore, we desire one vector representation for each word. To this end, we run the DMN in the same way as above, once for each word. When running the DMN for word t, instead of setting the episode for pass i to the final state of our modified GRU, we take the tth: e.g, for word t, we replace Eq. 6 with e i = h i t . Note that the gates for the first pass will be the same for each word, as the question is the same. This allows for speed-up in implementation by computing these gates only once. However, gates for subsequent passes will be different, as the episodes are different. The final output of the episodic memory module is the memory m, which goes to the answer module. In the sequence modeling case, each word's unique m is sent independently to the answer module.

Answer Sequence

The answer sequence module decodes the memory into a sequence of words representing the answer. We use a GRU, and set the initial hidden state to the memory a 0 = m. The subsequent hidden states take as input the last hidden state and the previously predicted output y t−1 , as well as the question: a t = GRU ([y t−1 , q], a t−1 ), y t = sof tmax(W (a) a t ) (7) where W (a) is a standard softmax layer. The output is trained with the cross entropy error classification of the correct sequence appended with a special end-of-sequence token.. At test time, we generate words until an end-of-sequence is generated.

Training

Training is cast as a supervised classification problem to minimize cross entropy error of the answer sequence. For datasets with gate supervision, such as bAbI, we also include the cross entropy error of the gates into the overall cost. Because all modules communicate over vector representations and various types of differentiable and deep neural networks with gates, the entire DMN model can be trained via backpropagation and gradient descent.

Related Work

Given the many shoulders on which this paper is standing and the many applications to which our model is applied, it is impossible to do related fields justice. Deep Learning. There are several deep learning models that have been applied to many different tasks in NLP. For instance, recursive neural networks have been used for parsing [11] , sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks. However, because they lack the memory and question modules, a single model cannot solve as many varied tasks, nor tasks that require transitive reasoning over multiple sentences . Another commonly used model is the chain structured recurrent neural network of the kind we employ above. Recurrent neural networks have been successfully used in language modeling [14], speech recognition, and sentence generation from images [15]. Also relevant is the sequenceto-sequence model used for machine translation by Sutskever et al. [16]. This model uses two extremely large and deep LSTMs to encode a sentence in one language and then decode the sentence in another language. This sequence to sequence model is a special case of the DMN without a question and without episodic memory. Instead it maps an input sequence directly to an answer sequence. Attention. The second line of work that is very relevant to DMNs is that of attention and memory in deep learning. The work of recent months by Weston et al. on memory networks [17] focuses on adding a memory component for natural language question answering. They have an input (I) and response (R) component and their generalization (G) and output feature map (O) components have some functional overlap with our episodic memory. However, their model cannot be applied to the same variety of NLP tasks since it processes sentences independently and not via a sequence model. It requires bag of n-gram string matching features as well as a separate feature that captures whether a sentence came before another one. We compare directly to their model on the bAbI dataset [18]. Attention mechanisms are generally useful and can improve image classification [19], automatic image captioning [20] and machine translation [21] [22]. Neural Turing machines use memory to solve algorithmic problems such as list sorting [23]. NLP Applications. The DMN is a general model which we apply to several NLP problems. We compare to what, to the best of our knowledge, is the current state of the art method for each task. There are many different approaches to question answering: some build large knowledge bases (KBs) with open information extraction systems [24], some use neural networks, dependency trees and KBs [6], others only sentences [12]. A lot of other approaches exist. When QA systems do not produce the right answer, it is often unclear if it is because they do not have access to the facts, cannot reason over them or have never seen this type of question or phenomenon. Most QA dataset only have a few hundred questions and answers but require complex reasoning. They can hence not be solved by models that have to learn purely from examples. While synthetic datasets [18] have problems and can often be solved easily with manual feature engineering, they let us disentangle failure modes of models and understand necessary QA capabilities. They are useful for analyzing models that attempt to learn everything and do not rely on external features like coreference, POS, parsing, logical rules, etc. The DMN is such a model. Sentiment analysis is a very useful classification task and recently the Stanford Sentiment Treebank [2] has become a standard benchmark dataset. Kim [25] reports the previous state of the art result based on a convolutional neural network that uses multiple word vector representations. The previous best model for part of speech tagging on the Wall Street Journal section of the Penn Tree Bank [26] was Sogaard [27] who used a semisupervised nearest neighbor approach. Neuroscience. The semantic and episodic memory modules are related to cognitive neuroscience. In humans both language-specific and supramodal concept representations are seated in the semantic memory whose existence is well established [28]. Hence, word vectors and knowledge bases are stored in the DMN's semantic memory module. The episodic memory in humans stores specific experiences in their spatial and temporal context. For instance, it might contain the first memory somebody has of flying a hang glider. Eichenbaum and Cohen have argued that episodic memories represent a form of relationship (i.e., relations between spatial, sensory and temporal information) and that the hippocampus is responsible for general relational learning [29]. Interestingly, it also appears that the hippocampus is active during transitive inference [30], and disruption of the hippocampus impairs this ability [31]. The episodic memory module in the DMN is related to these findings. It retrieves specific temporal states that are in a relationship with a question. Furthermore, we found that the GRU in this module was able to do some transitive inference over the simple facts in the bAbI dataset. This module also has similarities to the Temporal Context Model [32] and its Bayesian extensions [33] which were developed to analyze human behavior in word recall experiments.

Experiments

We include experiments on question answering, part of speech tagging, sentiment analysis, as well as preliminary results on machine translation. For all datasets we used either the official train,dev,test splits or if no dev set was defined, we used 10% of the training set for development. Hyper-parameter tuning and model selection (with early stopping) is done on the development set. The DMN is trained via backpropagation and Adagrad [34]. We employ L 2 regularization, and " word-dropout " in which each word vector is set to 0 with some probability p. Word vectors are pre-trained using Glove [3].

Question Answering

The Facebook bAbI dataset is a synthetic dataset meant to test a model's ability to retrieve facts and reason over them. Each task tests a different skill that a good question answering model ought to have, such as coreference resolution, deduction, and induction. Training on the bAbI dataset Table 1: Test accuracies on the bAbI dataset. MemNN numbers taken from Weston et al. [18]. The DMN passes (accuracy > 95%) 18 tasks, whereas the MemNN passes 16. uses the following objective function: J = αE CE (Gates) + βE CE (Answers), where E CE is the standard cross-entropy cost and α and β are hyperparameters. In practice, we begin training with α set to 1 and β set to 0, and then later switch β to 1 while keeping α at 1. We subsample the facts from the input module by end-of-sentence tokens. The gate supervision aims to select one sentence per pass; thus, we also experimented with modifying Eq. 6 to a simple softmax instead of a GRU. Here, we compute the final episode vector via: e i = T t=1 softmax(g i t )c t , where softmax(g i t ) = exp(g i t ) T j=1 exp(g i j ) , and g i t here is the value of the gate before the sigmoid. This setting achieves better results, likely because the softmax is better suited to picking one sentence at a time. We list results in table 1. The DMN does worse than the MemNN on tasks 2 and 3, both tasks with long input sequences. We suspect this is due to the recurrent input sequence model having trouble modeling very long inputs. The MemNN does not suffer from this problem as it views each sentence seperately. The power of the episodic memory module is evident in tasks 7 and 8, where the DMN significantly outperforms the MemNN. Both tasks require the model to iteratively retrieve facts and store them in a representation that slowly incorporates more of the relevant information of the input sequence. Both models do poorly on tasks 17 and 19, though the MemNN does better. We suspect this is due to the MemNN using n-gram features as well as explicit sequence position features. Part-of-speech tagging is traditionally modeled as a sequence tagging problem: every word in a sentence is to be classified into its part-of-speech class (see Fig. 1). We evaluate on the standard Wall Street Journal dataset included in Penn-III [26]. We use the standard splits of sections 0-18 for training, 19-21 for development and 22-24 for test sets [27]. Since this is a word level tagging task, DMN memories are produced at the word -rather than sentence-level. We compare the DMN with the results in [27]. The DMN achieves state-of-the-art accuracy with a single model, reaching a development set accuracy of 97.5. Ensembling the top 4 development models, the DMN gets to 97.58 dev and 97.56 test accuracies, achieving a new state-of-the-art (Table 2). is a popular dataset for sentiment classification. It provides phrase-level fine-grained labels, and comes with a train/dev/test split. We present results on two formats: fine-grained root prediction, where all full sentences (root nodes) of the test set are to be classified as either very negative, negative, neutral, positive, or very positive, and binary root prediction, where all non-neutral full sentences of the test set are to be classified as either positive or negative. To train the model on the fine-grained task, we use all phrase-level labels. To train on the binary task, we use all non-neutral phrase-level labels. Table 3: Test accuracies on SST [2] . Key: MV-RNN: Socher et al., 2013. RNTN: Socher et al., 2013. DCNN: Blunsom et al., 2014. PVec: Le and Mikolov, 2014. CNN-MC: Kim, 2014. DRNN: Irsoy and Cardie, 2014. CT-LSTM: Tai et al., 2015. All results as reported in [35] For sentiment analysis, our gate function G needs only the first 3 components c, m, q of the function z as defined in Eq. 3. The DMN achieves state-of-the-art accuracy on the binary classification task, as well as near state-of-the-art on the fine-grained classification task. Our DMN was trained with GRU sequence models and no tree structure. It is easy to replace the GRU sequence model with any of the models listed above, as well as incorporate tree structure to the retrieval process. These experiments were not run, and we consider them future work. Coreference resolution is a structure prediction task which is usually solved in two steps: (i) detect all mentions and (ii) determine their coreference. Similarly to POS tagging we model mention detection as a sequence tagging problem. In particular, the task is to predict a pair of parentheses for every position corresponding to the number of mentions which start and end at a given word (see Fig. 1). Context is incorporated by using a bidirectional GRU in the input module. We present the DMN input module with a text in which a single mention is highlighted with parentheses. Each paranthesis is treated like a separate word. The question includes the original input sentence but with a different mention being highlighted. This formulation allows to ask the same question for different inputs and vice versa; significantly speeding up evaluation of all N 2 mention pairs. We evaluated our coreference system using the quizbowl datset of Guha et al. [36] which is difficult to solve using previous coref systems as it contains many nested mentions and complex entity relationships . The described mention detection obtains 83.0 precision, 75.1 recall (78.9 F1) whereas the pairwise coreference classification of gold mentions achieves 68.35 precision, 63.90 recall (66.05 F1). Moreover, as shown in Table 4, mention clusters obtained by the transitive closure of the predicted pairwise coreference graph outperform both [36] and [37] when evaluated using standard Conll metrics on the quizbowl dataset. Note that unlike these systems our approach does not use any hand-engineered features or text pre-processing such as other named entity tags or parse trees. This is, to the best of our knowledge, the first system which solves both mention detection and coreference resolution using purely deep learning methods. Metric Guha et al., 2015 Durrett and Klein, 2013 Table 4: Coreference resolution scores of gold mentions from the dataset of Guha et al., 2015 evaluated using Conll scorer v7.0.

Conclusion

We believe the DMN is a potentially general model for a variety of NLP applications. The entire model can be trained end-to-end with one, albeit complex, objective function. The model uses some ideas from neuroscience such as semantic and episodic memories known to be required for complex types of reasoning. Future work will explore additional tasks, larger multi-task models and multimodal inputs and questions.