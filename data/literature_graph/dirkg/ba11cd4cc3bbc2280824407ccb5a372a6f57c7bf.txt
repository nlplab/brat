Title: Suffix Arrays: A New Method for On-Line String Searches

Abstract: A new and conceptually simple data structure, called a suffix array, for on-line string searches is introduced in this paper. Constructing and querying suffix arrays is reduced to a sort and search paradigm that employs novel algorithms. The main advantage of suffix arrays over suffix trees is that, in practice, they use three to five times less space. From a complexity standpoint, suffix arrays permit on-line string searches of the type, ''Is W a substring of A?'' to be answered in time O(P + log N), where P is the length of W and N is the length of A, which is competitive with (and in some cases slightly better than) suffix trees. The only drawback is that in those instances where the underlying alphabet is finite and small, suffix trees can be constructed in O(N) time in the worst case, versus O(N log N) time for suffix arrays. However, we give an augmented algorithm that, regardless of the alphabet size, constructs suffix arrays in O(N) expected time, albeit with lesser space efficiency. We believe that suffix arrays will prove to be better in practice than suffix trees for many applications.

Content: Introduction

Finding all instances of a string W in a large text A is an important pattern matching problem. There are many applications in which a fixed text is queried many times. In these cases, it is worthwhile to construct a data structure to allow fast queries. The Suffix tree is a data structure that admits efficient on-line string searches. A suffix tree for a text A of length N over an alphabet can be built in O(N log | | ) time and O(N) space [Wei73, McC76]. Suffix trees permit on-line string searches of the type, ''Is W a substring of A?'' to be answered in O(P log | | ) time, where P is the length of W. We explicitly consider the dependence of the complexity of the algorithms on | |, rather than assume that it is a fixed constant, because can be quite large for many applications. Suffix trees can also be constructed in time O(N) with O(P) time for a query, but this requires O(N| | ) space, which renders this method impractical in many applications. Suffix trees have been studied and used extensively. A survey paper by Apostolico [Apo85] cites over forty references. Suffix trees have been refined from tries to minimum state finite automaton for the text and its reverse [BBE85], generalized to on-line construction [MR80, BB86], real-time construction of some features is possible [Sli80], and suffix trees have been parallelized [AIL88]. Suffix trees have been applied to fundamental string problems such as finding the longest repeated substring [Wei73], finding all squares or repetitions in a string [AP83], computing substring statistics [AP85], approximate string matching [Mye86, LV89, CL90], and string comparison [EH86]. They have also been used to address other types of problems such as text compression [RPE81], compressing assembly code [FWM84], inverted indices [Car75], and analyzing genetic sequences [CHM86]. Galil [Ga85] lists a number of open problems concerning suffix trees and on-line string searching. In this paper, we present a new data structure, called the suffix array [MM90], that is basically a sorted list of all the suffixes of A. When a suffix array is coupled with information about the longest common prefixes (lcps) of adjacent elements in the suffix array, string searches can be answered in O(P + log N) time with a simple augmentation to a classic binary search. The suffix array and associated lcp information occupy a mere 2N integers, and searches are shown to require at most P + log 2 (N 1) single-symbol comparisons. To build a suffix array (but not its lcp information) one could simply apply any string sorting algorithm such as the O(Nlog N) expected-time algorithm of Baer and Lin [BL89]. But such an approach fails to take advantage of the fact that we are sorting a collection of related suffixes. We present an algorithm for constructing a suffix array and its lcp information with 3N integers 3 and O(N log N) time in the worst case. Time could be saved by constructing a suffix tree first, and then building the array with a traversal of the tree [Ro82] and the lcp information with constant-time nearest ancestor queries [SV88] on the tree. But this will require more space. Moreover, the algorithms for direct construction are interesting in their own right. Our approach distills the nature of a suffix tree to its barest essence: A sorted array coupled with another to accelerate the search. Suffix arrays may be used in lieu of suffix trees in many (but not all) applications of this ubiquitous structure. Our search and sort approach is distinctly different and, in theory, provides superior querying time at the expense of somewhat slower construction. Galil [Ga85, Problem 9] poses the problem of designing algorithms that are not dependent on | | and our algorithms meet this criterion , i.e., O(P + log N) search time with an O(N) space structure, independent of . With a few additional and simple O(N) data structures, we show that suffix arrays can be constructed in O(N) expected time, also independent of . This claim is true under the assumption that all strings of length N are equally likely and exploits the fact that for such strings, the expected length of the longest repeated substring is O(log N/ log | | ) [KGO83]. In practice, an implementation based on a blend of the ideas in this paper compares favorably with an implementation based on suffix trees. Our suffix array structure requires only 5N bytes on a VAX, which is three to five times more space efficient than any reasonable suffix tree encoding. Search times are competitive, but suffix arrays do require three to ten times longer to build. For these reasons, we believe that suffix arrays will become the data structure of choice for the many applications where the text is very large. In fact, we recently found that the basic concept of suffix arrays (sans the lcp and a provable efficient algorithm) has been used in the Oxford English Dictionary (OED) project at the University of Waterloo [Go89]. Suffix arrays have also been used as a basis for a sublinear approximate matching algorithm [My90] and for performing all pairwise comparisons between sequences in a protein sequence database [BG91]. The paper is organized as follows. In Section 2, we present the search algorithm under the assumption that the suffix array and the lcp information have been computed. In Section 3, we show how to construct the sorted suffix array. In Section 4, we give the algorithm for computing the lcp information. In Section 5, we modify the algorithms to achieve better expected running times. We end with empirical results and comments about practice in Section 6.

Searching

Let A = a 0 a 1 . . . a N 1 be a large text of length N. Denote by A i = a i a i + 1 . . . a N 1 the suffix of A that starts at position i. The basis of our data structure is a lexicographically sorted array, Pos, of the suffixes of A; namely, Pos[k] is the start position of the kth smallest suffix in the set {A 0 , A 1 , ... A N 1 }. The sort that produces the array Pos is described in the next Section. For now we assume that Pos is given; namely, A Pos[0] < A Pos[1] < ... < A Pos[N 1] , where ''<'' denotes the lexicographical order. For a string u, let u p be the prefix consisting of the first p symbols of u if u contains more than p symbols, and u otherwise. We define the relation < p to be the lexicographical order of p-symbol prefixes; that is, u < p v iff u p < v p . We define the relations p , = p , p , > p , and p in a similar way. Note that, for any choice of p, the Pos array is also ordered according to p , because u < v implies u p v. All suffixes that have equal p-prefixes, for some p < N, must appear in consecutive positions in the Pos array, because the Pos array is sorted lexicographically. These facts are central to our search algorithm. Suppose that we wish to find all instances of a string W = w 0 w 1 . . . w P 1 of length P N in A. Let L W = min ( k : W P A Pos[k] or k = N ) and R W = max ( k : A Pos[k] P W or k = 1 ). Since Pos is in P -order, it follows that W matches a i a i + 1 . . . a i + P 1 if and only if i = Pos[k] for some k [L W , R W ]. Thus, if L W and R W can be found quickly, then the number of matches is R W L W + 1 and their left endpoints are given by Pos[L W ] , Pos[L W + 1] , ... Pos[R W ] . But Pos is in P -order, hence a simple binary search can find L W and R W using O(log N) comparisons of strings of size at most P; each such comparison requires O(P) single-symbol comparisons. Thus, the Pos array allows us to find all instances of a string in A in time O(P log N). The algorithm is given in Fig. 1. The algorithm in Fig. 1 is very simple, but its running time can be improved. We show next that the p -comparisons involved in the binary search need not be started from scratch in each iteration of the while loop. We can use information obtained from one comparison to speedup the ensuing comparisons. When this strategy is coupled with some additional precomputed information, the search is improved to P + log 2 (N 1) single-symbol comparisons in the worst case, which is a substantial improvement. if W P A Pos[0] Figure 1: An O(Plog N) search for L W . Let lcp(v, w) be the length of the longest common prefix of v and w. When we lexicographically compare v and w in a left-to-right scan that ends at the first unequal symbol we obtain lcp(v, w) as a byproduct . We can modify the binary search in Fig. 1 by maintaining two variables, l and r, such that l = lcp(A Pos[L] , W), and r = lcp(W, A Pos[R] ). Initially, l is set by the comparison of W and A Pos[0] in line 1, and r is set in the comparison against A Pos[N 1] in line 3. Thereafter, each comparison of W against A Pos[M] in line 9, permits l or r to be appropriately updated in line 10 or 12, respectively. By so maintaining l and r, h = min(l, r) single-symbol comparisons can be saved when comparing A Pos[M] to W, because A Pos[L] = l W = r A Pos[R] implies A Pos[k] = h W for all k in [L, R] including M. While this reduces the number of single-symbol comparisons needed to determine the P -order of a midpoint with respect to W, it turns out that the worst case running time is still O(P log N) (e.g., searching ac N 2 b for c P 1 b). To reduce the number of single-symbol comparisons to P + log 2 (N 1) in the worst case, we use precomputed information about the lcps of A Pos[M] with each of A Pos[L] and A Pos[R] . Consider the set of all triples (L, M, R) that can arise in the inner loop of the binary search of Fig. 1. There are exactly N 2 such triples, each with a unique midpoint M [1, N 2], and for each triple , A Pos[R M ] ). The construction of the two (N 2)-element arrays, Llcp and Rlcp, can be interwoven with the sort producing Pos and will be shown in Section 4. For now, we assume that the Llcp and Rlcp arrays have been precomputed. Consider an iteration of the search loop for triple (L, M, R). Let h = max(l, r) and let h be the difference between the value of h at the beginning and at the end of the iteration. Assuming, without loss of generality, that r l = h, there are three cases to consider 4 , based on whether Llcp[M] is greater than, equal to, or less than h. The cases are illustrated in Fig. 2(a), 2(b), and 2(c), respectively. The vertical bars denote the lcps between W and the suffixes in the Pos array (except for l and r, these lcps are not known at the time we consider M). The shaded areas illustrate Llcp[M]. For each case, we must determine whether L W is in the right half or the left half (the binary search step) and we must update the value of either l or r. It turns out that both these steps are easy to make: Case 1: Llcp[M] > l (Fig. 2 (a)) in this case, A Pos[M] = l + 1 A Pos[L] l + 1 W, and so W must be in the right half and l is unchanged. Case 2: Llcp[M] = l (Fig. 2 (b)) in this case, we know that the first l symbols of Pos[M] and W are equal; thus, we need to compare only the l + 1 st symbol, l + 2 nd symbol, and so on, until we find one, say l + j, such that W l + j Pos[M]. The l + jth symbol determines whether L W is in the right or left side. In either case, we also know the new value of r or l — it is l + j. Since l = h at the beginning of the loop, this step takes h + 1 single-symbol comparisons. Case 3: Llcp[M] < l (Fig. 2 (c)) in this case, since W matched l symbols of L and < l symbols of M, it is clear that L W is in the left side and that the new value of r is Llcp[M]. Hence, the use of the arrays Llcp and Rlcp (the Rlcp array is used when l < r) reduces the number of single-symbol comparisons to no more than h + 1 for each iteration. Summing over all iterations and observing that h P, the total number of single-symbol comparisons made in an on-line string search is at most P + log 2 (N 1) , and O(P + log N) time is taken in the worst-case. The precise search algorithm is given in Fig. 3 .

Sorting

The sorting is done in log 2 (N + 1) stages. In the first stage, the suffixes are put in buckets according to their first symbol. Then, inductively, each stage further partitions the buckets by sorting according to twice the number of symbols. For simplicity of notation, we number the stages 1, 2, 4, 8, etc., to indicate the number of affected symbols. Thus, in the H th stage, the suffixes are sorted according to the H -order. For simplicity, we pad the suffixes by adding blank symbols, such that the lengths of all of them become N + 1. This padding is not necessary, but it simplifies the discussion; the version of the algorithm detailed in Fig. 4 does not make this assumption. The first stage consists of a bucket sort according to the first (c) (b) (a) L M R l r L M R l r L M R l r Figure 2: The three cases of the O(P + log N) search. symbol of each suffix. The result of this sort is stored in the Pos array and in another array BH of Boolean values which demarcates the partitioning of the suffixes into m 1 buckets (m 1 | |); each bucket holds the suffixes with the same first symbol. The array Pos will become progressively sorted as the algorithm proceeds. Assume that after the H th stage the suffixes are partitioned into m H buckets, each holding suffixes with the same H first symbols, and that these buckets are sorted according to the H -relation. We will show how to sort the elements in each H-bucket to produce the 2H -order in O(N) time. Our sorting algorithm uses similar ideas to those in [KMR72]. Let A i and A j be two suffixes belonging to the same bucket after the H th step; that is, A i = H A j . We need to compare A i and A j according to the next H symbols. But, the next H symbols of A i (A j ) are exactly the first H symbols of A i + H (A j + H ). By the assumption, we already know the relative order, according to the H -relation, of A i + H and A j + H . It remains to see how we can use that knowledge to complete the stage efficiently. We first describe the main idea, and then show how to implement it efficiently. We start with the first bucket, which must contain the smallest suffixes according to the H -relation. Let A i be the first suffix in the first bucket (i.e., Pos[0] = i), and consider A i H (if i H < 0, then we ignore A i and take the suffix of Pos[1], and so on). Since A i starts with the smallest H-symbol string, A i H should be the first in its 2H-bucket. Thus, we move A i H to the beginning of its bucket and mark this fact. For every bucket, we need to know the number of suffixes in that bucket that have already been moved and thus placed in 2H -order. The algorithm basically scans the suffixes as they appear in the H order , and for each A i it moves A i H (if it exists) to the next available place in its H-bucket. While this basic idea is simple, its efficient implementation (in terms of both space and time) is not trivial. We describe it below. We maintain three integers arrays, Pos, Prm, and Count, and two boolean arrays, BH and B2H, all with N elements 5 . At the start of stage H, Pos[i] contains the start position of the i th smallest suffix (according to the first H symbols), Prm[i] is the inverse of Pos, namely, Prm[Pos[i]] = i, and BH[i] is 1 iff Pos[i] contains the leftmost suffix of an H-bucket (i.e., A Pos[i] H A Pos[i 1] ). Count and B2H are temporary arrays; their use will become apparent in the description of a stage of the sort. A radix sort on the first symbol of each suffix is easily tailored to produce Pos, Prm, and BH for stage 1 in O(N) time. Assume that Pos, Prm, and BH have the correct values after stage H, and consider stage 2H. We first reset Prm[i] to point to the leftmost cell of the H-bucket containing the i th suffix rather than to the suffix's precise place in the bucket. We also initialize Count[i] to 0 for all i. All operations above can be done in O(N) time. We then scan the Pos array in increasing order, one bucket at a time. Let l and r (l r) mark the left and right boundary of the H-bucket currently being scanned. Let T i (the H left extension of i) denote Pos[i] ] to 1. In effect, all the suffixes whose H + 1 st through 2H th symbols equal the unique H-prefix of the current H-bucket are moved to the top of their Hbuckets with respect to the Prm array (Pos is updated momentarily). The B2H field is used to mark those prefixes that were moved. Before the next H-bucket is considered, we make another pass through this one, find all the moved suffixes, and reset the B2H fields such that only the leftmost of them in each 2H-bucket is set to 1, and the rest are reset to 0. This way, the B2H fields correctly mark the beginning of the 2H- buckets. Thus the scan updates Prm and sets B2H so that they are consistent with the 2H -order of the suffixes. In the final step, we update the Pos array (which is the inverse of Prm), and set BH to B2H. All the steps above can clearly be done in O(N) time, and, since there are at most log 2 (N + 1) stages, the sorting requires O(N log N) time in the worst case. A pseudo-code implementation is given in Fig. 4. Average-case analysis is presented in Section 5.

5

We present a conceptually simpler but more space expensive algorithm above in order to clearly expound the idea behind the sort. In fact, two N-element integer arrays are sufficient, and since the integers are always positive we can use their sign bit for the boolean values. Thus, the space requirement is only two integers per symbol. The trick is to remove the Count array by temporarily using the Prm value of the leftmost suffix in a bucket to hold the count for the bucket. Instead of initializing Count to 0 in the first step, we turn off the BH field of every bucket so that we can tell when a Prm value is the first reference to a particular bucket. The second step again consists of a scan of the Pos array. If BH[Prm[T i ]] is off then T i is to be the first suffix in the 2H order of its H-bucket. We search the bucket for it and actually place it at the head of its bucket (as opposed to just modifying Prm to reflect where it will go in the simpler algorithm). This allows us to then use Prm[T i ] as the counter for the bucket because we can restore it later knowing that the Prm-value of the first suffix in each bucket is being so used. We thus set the BH field back on and set Prm[T i ] to 1. For later references to this bucket, which we know because BH[Prm[T i ]] is now on, we simply adjust Prm[T i ] with the count in Prm[Pos[Prm[T i ]]] and bump the count. At the end of this step the Prm fields used as counters are reset to the position of their suffix. The B2H fields could not be set in the preceding steps because the BH values were being used as counter flags. In a separate pass, the B2H values are updated to identify 2H buckets as in the simple algorithm.

#

Sorting with 3N positive integers and 2N booleans. 'Count' can be eliminated and booleans folded into sign bits, to produce a 2N integer sort. # var Pos, Prm, Count: array [0..N 1] of int; BH, B2H: array [0..N] of boolean; # First phase bucket sort, Bucket and Link overlay Pos and Prm, respectively #

for a do

Bucket [a] 1 for i 0, 1, 2, . . . , N 1 do ( Bucket[a i ] , Link[i] ) ( i, Bucket[a i ] )

c 0 for a in order do

{ i Bucket[a] while i 1 do { j Link[i] Prm[i] c if i = Bucket[a] then { BH[c] # Inductive sorting stage (with lcp info calls) # for H 1, 2, 4, 8, . . . while H < N do { for each = H bucket [l, r] do { Count[l] 0 for c [l, r] do Prm[Pos[c]] l Prm[d] e + Count[e] Count[e] Count[e] + 1 B2H[Prm[d]] true for each = H bucket [l, r] in H -order do { for d { Pos[c] H : c [l, r] } [0, N 1] do { e Prm[d] Prm[d] e + Count[e] Count[e] Count[e] + 1 B2H[Prm[d]] true } for d { Pos[c] H : c [l, r] } [0, N 1] do if B2H[Prm[d]] then { e min ( j : j > Prm[d] and (BH[ j] or not B2H[ j]) ) for f [Prm[d] + 1, e 1] do B2H[ f ] false } } for i [0, N 1] do Pos [Prm[i]] i for i [0, N 1] do if B2H[i] and not BH[i] B2H[i] } } Figure 4: The O(Nlog N) suffix sorting algorithm.

Finding Longest Common Prefixes

The O(P + log N) search algorithm requires precomputed information about the lcps between the suffixes starting at each midpoint M and its left and right boundaries L M and R M . Computing a suffix array requires 2N integers and we will see here that computing and recording the associated lcp information requires an extra N integers. We first show how to compute the lcps between suffixes that are consecutive in the sorted Pos array during the sort. We will see later how to compute all the necessary lcps. The key idea is the following . Assume that after stage H of the sort we know the lcps between suffixes in adjacent buckets (after the first stage, the lcps between suffixes in adjacent buckets are 0). At stage 2H the buckets are partitioned according to 2H symbols. Thus, the lcps between suffixes in newly adjacent buckets must be at least H and at most 2H 1. Furthermore, if A p and A q are in the same H-bucket but are in distinct 2H-buckets, then The problem is that we only have the lcps between suffixes in adjacent buckets, and A p + H and A q + H may not be in adjacent buckets. However, if A Pos[i] and A Pos[ j] where i < j have an lcp less than H and Pos is in H order, then their lcp is the minimum of the lcp's of every adjacent pair of suffixes between Pos[i] and Pos[ j] , A Pos[ j] ) = k [i, j 1] min ( lcp(A Pos[k] , A Pos[k + 1] )) Using (4.2) directly would require too much time, and maintaining the lcp of every pair of suffixes too much space. By using an O(N)-space height balanced tree structure that records the minimum pairwise lcp over a collection of intervals of the suffix array, we will be able to determine the lcp between any two suffixes in O(log N) time. We will describe this data structure, which we call an interval tree, after we firmly establish our basic approach (interval trees are similar to the Cartesian trees of Vuillemin [Vui80]). We define height(i) = lcp(A Pos[i 1] , A Pos[i] ), 1 i N 1, where Pos is the final sorted order of the suffixes. These N 1 height values are computed in an array Hgt[i]. The computation is performed inductively, together with the sort, such that Hgt[i] achieves its correct value at stage H iff height(i) < H, and it is undefined (specifically, N + 1) otherwise. Formally, if height(i) < H then Hgt[i] = height(i); otherwise Hgt[i] = N + 1. Notice that, if height(i) < H, then A Pos[i 1] and A Pos[i] must be in different Hbuckets since H-buckets contain suffixes with the same H-symbol prefix. Further observe that a Hgt value is initially N + 1, it is set to its height value during the appropriate stage of the sort, and it retains this value thereafter. Let Pos H , Hgt H , and Prm H be the values of the given arrays at the end of stage H. In stage 2H of the sort, the 2H -ordered list Pos 2H is produced by sorting the suffixes in each H-bucket of the H -ordered list Pos H . The following lemma captures the essence of how we compute : k [min(a, b) + 1, max(a, b)] ), where a = Prm 2H [ Pos 2H [i 1] + H ], and b = Prm 2H [ Pos 2H [i] + H ]. Proof: Let p = Pos 2H [i 1] and q = Pos 2H [i]. As we have observed, height(i) < 2H implies height(i) = H + lcp(A p + H , A q + H ). Next observe that Pos 2H [a] = p + H and Pos 2H [b] = q + H by the choice of a and b. Without loss of generality, assume that a < b. We now know that height(i) = H + lcp(u, v) where u = A Pos 2H [a] , v = A Pos 2H [b] , lcp(u, v) < H, and u < H v. Observe that x < H z and x H y H z imply lcp(x, z) = min(lcp(x, y) , lcp(y, z)). It follows, by induction, that if x 0 < H x n and x 0 H x 1 H . . . H x n then lcp(x 0 , x n ) = min( lcp(x k 1 , x k ) : k [1, n] ). Thus, lcp(u, v) = min( lcp(A Pos 2H [k 1] , A Pos 2H [k] ) : k [a + 1, b] ). Now lcp(u, v) < H implies that at least one term in the minimum is less than H. For those terms less than H, lcp(A Pos 2H [k 1] ,A Pos 2H [k] ) = height(k) = Hgt H [k]. This, combined with the fact that Hgt H [k] = N + 1 H for all other terms, gives the result. We are now ready to describe the algorithm. In the first stage, we set Hgt[i] to 0 if a Pos 1 [i 1] a Pos 1 [i] , and to N + 1 otherwise. This correctly establishes Hgt 1 . At the end of stage 2H > 1, we have computed Pos 2H , Prm 2H , and BH 2H (which marks the 2H-buckets). Thus, by Lemma 1, the following code correctly establishes Hgt 2H from Hgt H when placed at the end of a sorting stage. Essential to its correctness is the fact that Hgt 2H is Hgt H except for the elements whose height values are in the range [H, 2H 1]; their Hgt values are changed from N + 1 to their correct value. for i [1, N 1] such that BH[i] and Hgt[i] from N + 1 to h in our interval tree, and Min_Height(i, j) determines min ( Hgt[k] : k [i, j] ) using the interval tree. We now show how to implement each routine in time O(log N) in the worst case. Consider a balanced and full binary tree with N 1 leaves which, in left-toright order, correspond to the elements of the array Hgt. The tree has height O(log N) and N 2 interior vertices. Assume that a value Hgt[v] is also kept at each interior vertex v. We say that the tree is current if for every interior vertex v, Hgt[v] = min(Hgt[le ft(v)] , Hgt[right(v)]), where le ft(v) and right(v) are the left and right children of v. Let T be a current tree. We need to perform two operations on the tree, a query Min_Height(i, j), and a dynamic operation Set(i, h). The query operation Min_Height(i, j) computes min ( Hgt[k] : k [i, j] ). It can be answered in O(log N) time as follows. Let nca(i, j) be the nearest common ancestor of leaves i and j. The nca of leaves i and j can be found in O(log N) time by simply walking from the leaves to the root of the tree (it can be done in constant time using a more complicated data structure [SV88], but it is not necessary here). Let P be the set of vertices on the path from i to nca(i, j) excluding nca(i, j), and let Q be the similar path for leaf j. Min_Height(i, j) is the minimum of the following values: (1) Hgt[i], (2) Hgt[w] such that right(v) = w and w / P for some v P, (3) Hgt[w] such that le ft(v) = w and w / Q for some v Q, and (4) Hgt[ j]. These O(log N) vertices can be found and their minimum computed in O(log N) time. The operation Set(i, h) sets Hgt[i] to h and then makes T current again by updating the Hgt values of the interior vertices on the path from i to the root. This takes O(log N) time. Overall, the time taken to compute the height values in stage H is O(N + log N . Set H ) where Set H is the number of indices i for which height(i) [H, 2H 1]. Since Set H = N over all stages, the total additional time required to compute Hgt during the sort is O(Nlog N). The Hgt array gives the lcps of suffixes that are consecutive in the Pos array. Moreover, an interior vertex of our interval tree gives the lcp between the suffixes at its leftmost and rightmost leaves. We now show that not only are the arrays Llcp and Rlcp computable from the array Hgt but are directly available from the interval tree by appropriately choosing the shape of the tree (heretofore we only asserted that it needed to be full and balanced). Specifically, we use the tree based on the binary search of Figure 1. This implicitly-represented tree consists of 2N 3 vertices each labeled with one of the 2N 3 pairs, (L, R), that can arise at entry and exit from the while loop of the binary search. The root of the tree is labeled (0, N 1) and the remaining vertices are labeled either (L M , M) or (M, R M ) for some midpoint M [1, N 2]. From another perspective, the tree's N 2 interior vertices are (L M , R M ) for each midpoint M, and its N 1 leaves are (i 1, i) for i [1, N 1] in left to right order. For each interior vertex, le ft( (L M , R M ) ) = (L M , M) and right( (L M , R M ) ) = (M, R M ). Since the tree is full and balanced, it is appropriate for realizing Set and Min_Height if we let leaf (i 1, i) hold the value of Hgt[i]. Moreover, at the end of the sort, are directly available upon completion of the sort. 6

Linear Time Expected-case Variations

We now consider the expected time complexity of constructing and searching suffix arrays. The variations presented in this section require additional O(N) structures and so lose some of the space advantage. We present them primarily to show that linear time constructions are possible, independent of alphabet size, and because some of the ideas here are central to the implementation we found to be best in practice. We assume that all N-symbol strings are equally likely 7 . Under this input distribution, the expected length of the longest repeated substring has been shown to be 2log | | N + O(1) [KGO83]. This fact provides the central leverage for all the results that follow. Note that it immediately implies that, in the expected case, Pos will be completely sorted after O(log log N) stages, and the sorting algorithm of Section 3 thus takes O(Nlog log N) expected time. The expected sorting time can be reduced to O(N) by modifying the radix sort of the first stage as follows. Let T = log | | N and consider mapping each string of T symbols over to the integer obtained when the string is viewed as a T-digit, radix-| | number. This oft-used encoding is an isomorphism onto the range [0, | | T 1] [0, N 1] , and the -relation on the integers is identical with the T -relation on the corresponding strings. Instead of performing the initial radix sort on the first symbol of each suffix, we perform it on the integer encoding of the first T symbols of each suffix. This radix sort 6 The interval tree requires 2N 3 positive integers. However, the observation that one child of each interior vertex has the same value as its parent, permits interval trees (and thus the Llcp and Rlcp arrays) to be encoded and manipulated as N 1 signed integers. Specifically, if the number at a vertex is positive, then the vertex contains the Llcp values; if it is negative, then its positive part is the Rlcp value. The other value is that of its parent. One must store both the Llcp and Rlcp values for the root but this is only one extra integer. Note that in order to have both the Llcp and Rlcp values available at a vertex requires that we descend to it from the root. This is naturally the case for searches, and for Set and Min_height we simply traverse from root to leaf and back again at no increase in asymptotic complexity.

7

The ensuing results also hold under the more general model where each text is assumed to be the result of N independent Bernoulli trials of a | |-sided coin toss that is not necessarily uniform. still takes just O(N) time and space because the choice of T guarantees that the integer encodings are all less than N. Moreover, it sorts the suffixes according to the T -relation. Effectively, the base case of the sort has been extended from H = 1 to H = T with no loss of asymptotic efficiency. Since the expected length of the longest repeated substring is T . (2 + O(1/T)), at most 2 subsequent stages are needed to complete the sort in the expected case. Thus this slight variation gives an O(N) expected time algorithm for sorting the suffixes. Corresponding expected-case improvements for computing the lcp information, in addition to the sorted suffix array, are harder to come by. We can still achieve O(N) expected-case time as we now show. We employ an approach to computing height(i) that uses identity (4.1) recursively to obtain the desired lcps. Let the sort history of a particular sort be the tree that models the successive refinement of buckets during the sort. There is a vertex for each H-bucket except those H-buckets that are identical to the (H/2)-buckets containing them. The sort history thus has O(N) vertices, as each leaf corresponds to a suffix and each interior vertex has at least two children. Each vertex contains a pointer to its parent and each interior vertex also contains the stage number at which its bucket was split. The leaves of the tree are assumed to be arranged in an N element array, so that the singleton bucket for suffix A p can be indexed by p. It is a straightforward exercise to build the sort history in O(N) time overhead during the sort. Notice that we determine the values height(i) only after the sort is finished. Given the sort history produced by the sort, we determine the lcp of A p and A q as follows. First we find the nearest common ancestor (nca) of suffixes A p and A q in the sort history using an O(1) time nca algorithm [HT84, SV88]. The stage number H associated with this ancestor tells us that lcp( . We then recursively find the lcp of A p + H and A q + H by finding the nca of suffixes A p + H and A q + H in the history, and so on, until an nca is discovered to be the root of the history . At each successive level of the recursion, the stage number of the nca is at least halved, and so the number of levels performed is O(log L), where L is the lcp of A p and A q . Because the longest repeated substring has expected length O(log | | N), the N 1 lcp values of adjacent sorted suffixes are found in O(Nlog log N) expected time. The scheme above can be improved to O(N) expected time by strengthening the induction basis as was done for the sort. Suppose that we stop the recursion above when the stage number of an nca becomes less than T = 1 ⁄2 log | | N . Our knowledge of the expected maximum lcp length implies that, on average, only three or four levels are performed before this condition is met. Each level takes O(1) time, and we are left having to determine the lcp of two suffixes, say A p and A q , that is known to be less than T . To answer this final lcp query in constant time, we build a | | T -by-| | ) for all T -symbol strings x and y. By the choice of T there are no more than N entries in the array and they can be computed incrementally in an O(N) preprocessing step along with the integer encodings Int T (A p ) for all p. So for the final level of the recursion, In summary, we can compute the lcp between any two suffixes in O(1) expected time, and so can produce the lcp array in O(N) expected time. The technique of using integer encodings of O(log N)-symbol strings to speedup the expected preprocessing times, also provides a pragmatic speedup for searching. For any K T, let Buck[k] = min { i : Int K (A Pos[k] ) = i }. This bucket array contains | | K non-decreasing entries and can be computed from the ordered suffix array in O(N) additional time. Given a word W, we know immediately that L W and R W are between Buck[k] and Buck[k + 1] 1 for k = Int K (W). Thus in O(K) time we can limit the interval to which we apply the search algorithm proper, to one whose average size is N/| | K . Choosing K to be T or very close to T, implies that the search proper is applied to an O(1) expected-size interval and thus consumes O(P) time in expectation regardless of whether the algorithm of Figure 1 or 3 is used. While the use of bucketing does not asymptotically improve either worst-case or expected-case times, we found this speedup very important in practice.

Practice

A primary motivation for this paper was to be able to efficiently answer on-line string queries for very long genetic sequences (on the order of one million or more symbols long). In practice, it is the space overhead of the query data structure that limits the largest text that may be handled. Throughout this section we measure space in numbers of integers where typical current architectures model each integer in 4 bytes. Suffix trees are quite space expensive, requiring roughly 4 integers of overhead per text character. Utilizing an appropriate blend of the suffix array algorithms given in this paper, we developed an implementation requiring 1.25 integers of overhead per text character whose construction and search speeds are competitive with suffix trees. There are three distinct ways to implement a data structure for suffix trees, depending on how the outedges of an interior vertex are encoded. We characterize the space occupied by that part of the structure needed for searches and ignore the extra integer (for ''suffix links'') needed during the suffix tree's construction . Using a | |-element vector to model the outedges, gives a structure requiring 2N + (| | + 2) . I integers where I is the number of interior nodes in the suffix tree. Encoding each set of outedges with a binary search tree requires 2N + 5I integers. Finally, encoding each outedge set as a linked list requires 2N + 4I integers. The parameter I < N varies as a function of the text. The first four columns of Table 1 illustrates the value of I/N and the per-text-symbol space consumption of each of the three coding schemes assuming that an integer occupies 4 bytes. These results suggest that the linked scheme is the most space parsimonious. We developed a tightly coded implementation of this scheme for the timing comparisons with our suffix array software. Table 1: Empirical results for texts of length 100,000. For our practical implementation, we chose to build just a suffix array and use the radix-N initial bucket sort described in Section 5 to build it in O(N) expected time. Without the lcp array the search must take O(P log N) worst-case time. However, keeping variables l and r as suggested in arriving at the O(P + log N) search, significantly improves search speed in practice. We further accelerate the search to O(P) expected time by using a bucket table with K = log | | N/4 as described in Section 5. Our search structure thus consists of an N integer suffix array and a N/4 integer bucket array, and so consumes only 1.25 integers/5 bytes per text symbol assuming an integer is 4 bytes. As discussed in Section 3, 2N integers are required to construct the suffix array (without lcp information). So constructing an array requires a little more space than is required by the search structure, as is true for suffix trees. Given that construction is usually once only, we chose to compare the sizes of the search structures in Table 1. Table 1 summarizes a number of timing experiments on texts of length 100,000. All times are in seconds and were obtained on a VAX 8650 running UNIX. Columns 6 and 7 give the times for constructing the suffix tree and suffix array, respectively. Columns 8 and 9 give the time to perform 100,000 successful queries of length 20 for the suffix tree and array, respectively. In synopsis, suffix arrays are 3 10 times more expensive to build, 2.5 5 times more space efficient, and can be queried at speeds comparable to suffix trees.