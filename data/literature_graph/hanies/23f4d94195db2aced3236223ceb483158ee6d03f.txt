Title: Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Matrix Decomposition

Abstract: In this paper, we consider a multi-step version of the stochastic ADMM method with efficient guarantees for high-dimensional problems. We first analyze the simple setting, where the optimization problem consists of a loss function and a single regularizer (e.g. sparse optimization), and then extend to the multi-block setting with multiple regularizers and multiple variables (e.g. matrix decomposition into sparse and low rank components). For the sparse optimization problem, our method achieves the minimax rate of O(s log d/T) for s-sparse problems in d dimensions in T steps, and is thus, unimprovable by any method up to constant factors. For the matrix decomposition problem with a general loss function, we analyze the multi-step ADMM with multiple blocks. We establish O(1/T) rate and efficient scaling as the size of matrix grows. For natural noise models (e.g. independent noise), our convergence rate is minimax-optimal. Thus, we establish tight convergence guarantees for multi-block ADMM in high dimensions. Experiments show that for both sparse optimization and matrix decomposition problems, our algorithm outperforms the state-of-the-art methods.

Content: Introduction

Stochastic optimization techniques have been extensively employed for online machine learning on data which is uncertain, noisy or missing. Typically it involves performing a large number of inexpensive iterative updates, making it scalable for large-scale learning. In contrast, traditional batch-based techniques involve far more expensive operations for each update step. Stochastic optimization has been analyzed in a number of recent works. The alternating direction method of multipliers (ADMM) is a popular method for online and distributed optimization on a large scale [1], and is employed in many applications. It can be viewed as a decomposition procedure where solutions to sub-problems are found locally, and coordinated via constraints to find the global solution. Specifically, it is a form of augmented Lagrangian method which applies partial updates to the dual variables. ADMM is often applied to solve regularized problems, where the function optimization and regularization can be carried out locally, and then coordinated globally via constraints. Regularized optimization problems are especially relevant in the high dimensional regime since regularization is a natural mechanism to overcome ill-posedness and to encourage parsimony in the optimal solution, e.g., sparsity and low rank. Due to the efficiency of ADMM in solving regularized problems, we employ it in this paper. We consider a simple modification to the (inexact) stochastic ADMM method [2] by incorporating multiple steps or epochs, which can be viewed as a form of annealing. We establish that this simple modification has huge implications in achieving tight bounds on convergence rate as the dimensions of the problem instances scale. In each iteration, we employ projections on to certain norm balls of appropriate radii, and we decrease the radii in epochs over time. For instance, for the sparse optimization problem, we constrain the optimal solution at each step to be within an 1 -norm ball of the initial estimate, obtained at the beginning of each epoch. At the end of the epoch, an average is computed and passed on to the next epoch as its initial estimate. Note that the 1 projection can be solved efficiently in linear time, and can also be parallelized easily [3]. For matrix decomposition with a general loss function, the ADMM method requires multiple blocks for updating the low rank and sparse components. We apply the same principle and project the sparse and low rank estimates on to 1 and nuclear norm balls, and these projections can be computed efficiently. Theoretical implications: The above simple modifications to ADMM have huge implications for high-dimensional problems. For sparse optimization, our convergence rate is O( s log d T ), for s-sparse problems in d dimensions in T steps. Our bound has the best of both worlds: efficient high-dimensional scaling (as log d) and efficient convergence rate (as

1

T ). This also matches the minimax rate for the linear model and square loss function [4], which implies that our guarantee is unimprovable by any (batch or online) algorithm (up to constant factors). For matrix decomposition, our convergence rate is O((s + r)β 2 (p) log p/T )) + O(max{s + r, p}/p 2 ) for a p × p input matrix in T steps, where the sparse part has s non-zero entries and low rank part has rank r. For many natural noise models (e.g. independent noise, linear Bayesian networks), β 2 (p) = p, and the resulting convergence rate is minimax-optimal. Note that our bound is not only on the reconstruction error, but also on the error in recovering the sparse and low rank components. These are the first convergence guarantees for online matrix decomposition in high dimensions. Moreover, our convergence rate holds with high probability when noisy samples are input, in contrast to expected convergence rate, typically analyzed in the literature. See Table 1, 2 for comparison of this work with related frameworks. Proof of all results and implementation details can be found in the longer version [5]. Practical implications: The proposed algorithms provide significantly faster convergence in high dimension and better robustness to noise. For sparse optimization, our method has significantly better accuracy compared to the stochastic ADMM method and better performance than RADAR, based on multi-step dual averaging [6]. For matrix decomposition, we compare our method with the state-of-art inexact ALM [7] method. While both methods have similar reconstruction performance, our method has significantly better accuracy in recovering the sparse and low rank components. Related Work: ADMM: Existing online ADMM-based methods lack high-dimensional guarantees . They scale poorly with the data dimension (as O(d 2 )), and also have slow convergence for general problems (as O( 1 √ T )). Under strong convexity, the convergence rate can be improved to O(

1

T ) but only in expectation: such analyses ignore the per sample error and consider only the expected convergence rate(see Table 1). In contrast, our bounds hold with high probability. Some stochastic ADMM methods, Goldstein et al. [8], Deng [9] and Luo [10] provide faster rates for stochastic ADMM, than the rate noted in Table 1. However, they require strong conditions which are not satisfied for the optimization problems considered here, e.g., Goldstein et al. [8] require both the loss function and the regularizer to be strongly convex. Related Work: Sparse Optimization: For the sparse optimization problem, 1 regularization is employed and the underlying true parameter is assumed to be sparse. This is a well-studied problem in a number of works (for details, refer to [6]). Agarwal et al. [6] propose an efficient online method based on dual averaging, which achieves the same optimal rates as the ones derived in this paper. The main difference is that our ADMM method is capable of solving the problem for multiple random variables and multiple conditions while their method cannot incorporate these extensions. Related Work: Matrix Decomposition: To the best of our knowledge, online guarantees for highdimensional matrix decomposition have not been provided before. Wang et al. [12] propose a multiblock ADMM method for the matrix decomposition problem but only provide convergence rate analysis in expectation and it has poor high dimensional scaling (as O(p 4 ) for a p × p matrix) without further modifications. Note that they only provide convergence rate on difference between loss function and optimal loss, whereas we provide the convergence rate on individual errors of the sparse and low rank components ¯ S(T ) − S * 2 F , ¯ L(T ) − L * 2 F . See Table 2 for comparison of guarantees for matrix decomposition problem.

Notation

In the sequel, we use lower case letter for vectors and upper case letter for matrices. Moreover, X ∈ R p×p . x 1 , x 2 refer to 1 , 2 vector norms respectively. The term X * stands Method Assumptions Convergence rate ST-ADMM [2] L, convexity O(d 2 / √ T ) ST-ADMM [2] SC, E O(d 2 log T /T ) BADMM [11] convexity, E O(d 2 / √ T ) RADAR [6] LSC, LL O(s log d/T ) REASON 1 (this paper) LSC, LL O(s log d/T ) Minimax bound [4] Eigenvalue conditions O(s log d/T ) Table 1: Comparison of online sparse

optimization

methods under s sparsity level for the optimal paramter, d dimensional space, and T number of iterations. SC = Strong Convexity, LSC = Local Strong Convexity, LL = Local Lipschitz, L = Lipschitz property, E=in Expectation. The last row provides the minimax-optimal rate for any method. The results hold with high probability. Method Assumptions Convergence rate Multi-block-ADMM[12] L, SC, E O(p 4 /T ) Batch method[13] LL, LSC, DF O((s log p + rp)/T )+O(s/p 2 ) REASON 2 (this paper) LSC, LL, DF O((s + r)β 2 (p) log p/T ))+O(max{s + r, p}/p 2 ) Minimax bound[13] 2 , IN, DF O((s log p + rp)/T )+O(s/p 2 ) Table 2: Comparison of optimization methods for sparse+low rank matrix decomposition for a p × p matrix under s sparsity level and r rank matrices and T is the number of samples. Abbreviations are as in Table 1, IN = Independent noise model, DF = diffuse low rank matrix under the optimal parameter. β(p) = Ω( √ p), O(p) and its value depends the model. The last row provides the minimax-optimal rate for any method under the independent noise model. The results hold with high probability unless otherwise mentioned. For Multi-block-ADMM [12] the convergence rate is on the difference of loss function from optimal loss, for the rest of works in the table, the convergence rate is on the individual estimates of the sparse and low rank components: ¯ S(T ) − S * 2 F + ¯ L(T ) − L * 2 F . for nuclear norm of X. In addition, X 2 , X F denote spectral and Frobenius norms respectively. We use vectorized 1 , ∞ norm for matrices, i.e., X 1 = i,j |X ij |, X ∞ = max i,j |X ij |.

1 Regularized Stochastic Optimization

We consider the optimization problem θ * ∈ arg min E[f (θ, x)], θ ∈ Ω where θ * is a sparse vector. The loss function f (θ, x k ) is a function of a parameter θ ∈ R d and samples x i . In stochastic setting, we do not have access to E[f (θ, x)] nor to its subgradients. In each iteration we have access to one noisy sample. In order to impose sparsity we use regularization. Thus, we solve a sequence θ k ∈ arg min θ∈Ω f (θ, x k ) + λθ 1 , Ω ⊂ Ω, (1) where the regularization parameter λ > 0 and the constraint sets Ω change from epoch to epoch.

Epoch-based Stochastic ADMM Algorithm

We now describe the modified inexact ADMM algorithm for the sparse optimization problem in (1), and refer to it as REASON 1, see Algorithm 1. We consider an epoch length T 0 , and in each epoch i, we project the optimal solution on to an 1 ball with radius R i centered around˜θ i , which is the initial estimate of θ * at the start of the epoch. The θ-update is given by θ k+1 = arg min θ−˜θi 2 1 ≤R 2 i {{∇f (θ k ), θ − θ k − z k , θ − y k + ρ 2 θ − y k 2 2 + ρ x 2 θ − θ k 2 2 }. (2) Note that this is an inexact update since we employ the gradient ∇f (·) rather than optimize directly on the loss function f (·) which is expensive. The above program can be solved efficiently since it is a projection on to the 1 ball, whose complexity is linear in the sparsity level of the gradient, when performed serially, and O(log d) when performed in parallel using d processors [3]. For the regularizer, we introduce the variable y, and the y-update is y k+1 = arg min{λ i y k 1 − z k , θ k+1 − Algorithm 1: Input ρ, ρ x , epoch length T 0 , initial prox center˜θ 1 , initial radius R 1 , regularization parameter {λ i } k T i=1 . Define Shrink κ (a) = (a − κ) + − (−a − κ) + . for Each epoch i = 1, 2, ..., k T do Initialize θ 0 = y 0 = ˜ θ i for Each iteration k = 0, 1, ..., T 0 − 1 do θ k+1 = arg min θ−˜θi1≤Ri {{∇f (θ k ), θ − θ k − z k , θ − y k + ρ 2 θ − y k 2 2 + ρ x 2 θ − θ k 2 2 } y k+1 = Shrink λi/ρ (θ k+1 − z k ρ ), z k+1 = z k − τ (θ k+1 − y k+1 ) Return : θ(T i ) := 1 T T0−1 k=0 θ k for epoch i and˜θ i+1 = θ(T i ). Update : R 2 i+1 = R 2 i /2. y+ ρ 2 θ k+1 −y 2 2 }. This update can be simplified to the form given in REASON 1, where Shrink κ (·) is the soft-thresholding or shrinkage function [1]. Thus, each step in the update is extremely simple to implement. When an epoch is complete, we carry over the average θ(T i ) as the next epoch center and reset the other variables.

High-dimensional Guarantees

We now provide convergence guarantees for the proposed method under the following assumptions. Assumption A1: Local strong convexity (LSC): The function f : S → R satisfies an R-local form of strong convexity (LSC) if there is a non-negative constant γ = γ(R) such that for any θ 1 , θ 2 ∈ S with θ 1 1 ≤ R and θ 2 1 ≤ R, f (θ 1 ) ≥ f (θ 2 ) + ∇f (θ 2 ), θ 1 − θ 2 + γ 2 θ 2 − θ 1 2 2 . Note that the notion of strong convexity leads to faster convergence rates in general. Intuitively, strong convexity is a measure of curvature of the loss function, which relates the reduction in the loss function to closeness in the variable domain. Assuming that the function f is twice continuously differentiable, it is strongly convex, if and only if its Hessian is positive semi-definite, for all feasible θ. However, in the high-dimensional regime, where there are fewer samples than data dimension, the Hessian matrix is often singular and we do not have global strong convexity. A solution is to impose local strong convexity which allows us to provide guarantees for high dimensional problems. This notion has been exploited before in a number of works on high dimensional analysis, e.g., [14, 13, 6]. It holds for various loss functions such as square loss. Assumption A2: Sub-Gaussian stochastic gradients: Let e k (θ) := ∇f (θ, x k ) − E[∇f (θ, x k )]. There is a constant σ = σ(R) such that for all k > 0, E[exp(e k ( θ) 2 ∞ )/σ 2 ] ≤ exp(1), for all θ such that θ − θ * 1 ≤ R. Remark: The bound holds with σ = O( √ log d) whenever each component of the error vector has sub-Gaussian tails [6]. Assumption A3: Local Lipschitz condition: For each R > 0, there is a constant G = G(R) such that, |f (θ 1 )−f (θ 2 )| ≤ Gθ 1 −θ 2 1 , for all θ 1 , θ 2 ∈ S such that θ−θ * 1 ≤ R and θ 1 −θ * 1 ≤ R. The design parameters are as below where λ i is the regularization for 1 term in epoch i, ρ and ρ x are penalties in θ-update as in (2) and τ is the step size for the dual update. λ 2 i = γR i s √ T 0 log d + G 2 (ρ + ρ x ) 2 T 2 0 + σ 2 i log( 3 δ i ), ρ ∝ √ T 0 log d R i , ρ x > 0, τ ∝ √ T 0 R i . (3) Theorem 1. Under Assumptions A1 − A3, λ i as in (3) , with fixed epoch lengths T 0 = T log d/k T , where T is the total number of iterations and k T = log 2 γ 2 R 2 1 T s 2 (log d + γ s G + 12σ 2 log( 6 δ )) , and T 0 satisfies T 0 = O(log d), for any θ * with sparsity s, with probability at least 1 − δ we have ¯ θ T − θ * 2 2 = O s log d + γ s G + (log(1/δ) + log(k T /log d))σ 2 T log d k T , where ¯ θ T is the average for the last epoch for a total of T iterations. Improvement of log d factor : The above theorem covers the practical case where the epoch length T 0 is fixed. We can improve the above results using varying epoch length (which depend on the problem parameters) such that ¯ θ T − θ * 2 2 = O(s log d/T ). The details can be found in the longer version [5].This convergence rate of O(s log d/T ) matches the minimax lower bounds for sparse estimation [4]. This implies that our guarantees are unimprovable up to constant factors.

Extension to Doubly Regularized Stochastic Optimization

We consider the optimization problem M * ∈ arg min E[f (M, X)], where we want to decompose M into a sparse matrix S ∈ R p×p and a low rank matrix L ∈ R p×p . f (M, X k ) is a function of a parameter M and samples X k . X k can be a matrix (e.g. independent noise model) or a vector (e.g. Gaussian graphical model). In stochastic setting, we do not have access to E[f (M, X)] nor to its subgradients. In each iteration, we have access to one noisy sample and update our estimate based on that. We impose the desired properties with regularization. Thus, we solve a sequence M k := arg min{ f (M, X k ) + λ n S 1 + µ n L * } s.t. M = S + L, L ∞ ≤ α p . (4) We propose an online program based on multi-block ADMM algorithm. In addition to tailoring projection ideas employed for sparse case, we impose an ∞ constraint of α/p on each entry of L. This constraint is also imposed for the batch version of the problem (4) in [13], and we assume that the true matrix L * satisfies this constraint. Intuitively, the ∞ constraint controls the " spikiness " of L * . If α ≈ 1, then the entries of L are O(1/p), i.e. they are " diffuse " or " non-spiky " , and no entry is too large. When the low rank matrix L * has diffuse entries, it cannot be a sparse matrix, and thus, can be separated from the sparse S * efficiently. In fact, the ∞ constraint is a weaker form of the incoherence-type assumptions needed to guarantee identifiability [15] for sparse+low rank decomposition. For more discussions, see Section 3.2.

3.1

Epoch-based Multi-Block ADMM Algorithm We now extend the ADMM method proposed in REASON 1 to multi-block ADMM. The details are in Algorithm 2, and we refer to it as REASON 2. Recall that the matrix decomposition setting assumes that the true matrix M * = S * + L * is a combination of a sparse matrix S * and a low rank matrix L * . In REASON 2, the updates for matrices M, S, L are done independently at each step. The updates follow definition of ADMM and ideas presented in Section 2. We consider epochs of lengths T 0 . We do not need to project the update of matrix M . The update rules for S, L are result of doing an inexact proximal update by considering them as a single block, which can then be decoupled. We impose an 1 -norm projection for the sparse estimate S around the epoch initializatioñ S i . For the low rank estimate L, we impose a nuclear norm projection around the epoch initializatioñ L i . Intuitively, the nuclear norm projection, which is an 1 projection on the singular values, encourages sparsity in the spectral domain leading to low rank estimates. We also require an ∞ constraint on L. Thus, the update rule for L has two projections, i.e. infinity and nuclear norm projections. We decouple it into ADMM updates L, Y with dual variable U corresponding to this decomposition.

High-dimensional Guarantees

We now prove that REASON 2 recovers both the sparse and low rank estimates in high dimensions efficiently. We need the following assumptions, in addition to Assumptions A2, A3. Assumption A4: Spectral Bound on the Gradient Error Let E k (M, X k ) := ∇f (M, X k ) − E[∇f (M, X k )], E k 2 ≤ β(p)σ, where σ := E k ∞ . Recall from Assumption A2 that σ = O(log p), under sub-Gaussianity. Here, we require spectral bounds in addition to · ∞ bound in A2. Assumption A5: Bound on spikiness of low-rank matrix L * ∞ ≤ α p , as discussed before. Assumption A6: Local strong convexity (LSC) The function f : R d1×d2 → R n1×n2 satisfies an R-local form of strong convexity (LSC) if there is a non-negative constant γ = γ(R) such that f (B 1 ) ≥ f (B 2 ) + Tr(∇f (B 2 )(B 1 − B 2 )) + γ 2 B 2 − B 1 F , for any B 1 ≤ R and B 2 ≤ R, which is essentially the matrix version of Assumption A1. We choose algorithm parameters as below where λ i , µ i are the regularization for 1 and nuclear norm respectively, ρ, ρ x correspond to penalty terms in M -update and τ is dual update step size. λ 2 i = γ (R 2 i + ˜ R 2 i ) (s + r) √ T 0 log p+ G 2 (ρ + ρ x ) 2 T 2 0 +β 2 (p)σ 2 i log( 3 δ i )+ α 2 p 2 + β 2 (p)σ 2 T 0 log p+log 1 δ (5) µ

2

i = c µ λ 2 i , ρ ∝ T 0 log p R 2 i + ˜ R 2 i , ρ x > 0, τ ∝ T 0 R 2 i + ˜ R 2 i Theorem 2. Under Assumptions A2 − A6, parameter settings (5), let T denote total number of iterations and T 0 = T log p/k T , where k T − log (s + r) 2 γ 2 R 2 1 T log p + G s + r + β 2 (p)σ 2 [(1 + G)(log(6/δ) + log k T ) + log p] , and T 0 satisfies T 0 = O(log p), with probability at least 1 − δ we have ¯ S(T ) − S * 2 F + ¯ L(T ) − L * 2 F = O   (s + r) log p + G + β 2 (p)σ 2 (1 + G)(log 6 δ + log k T log p ) + log p T log p k T   + 1 + s + r γ 2 p α 2 p . Improvement of log p factor : The above result can be improved by a log p factor by considering varying epoch lengths (which depend on problem parameters). The resulting convergence rate is O((s + r)p log p/T + α 2 /p). The details can be found in the longer version [5]. Scaling of β(p): We have the following bounds Θ( √ p) ≤ β(p)Θ(p). This implies that the convergence rate (with varying epoch lengths) is O((s + r)p log p/T + α 2 /p), when β(p) = Θ( √ p) and when β(p) = Θ(p), it is O((s + r)p 2 log p/T + α 2 /p). The upper bound on β(p) arises trivially by converting the max-norm E k ∞ ≤ σ to the bound on the spectral norm E k 2 . In many interesting scenarios, the lower bound on β(p) is achieved, as outlined below in Section 3.2.1. Comparison with the batch result: Agarwal et al. [13] consider the batch version of the same problem (4), and provide a convergence rate of O((s log p + rp)/T + sα 2 /p 2 ). This is also the minimax lower bound under the independent noise model. With respect to the convergence rate, we match their results with respect to the scaling of s and r, and also obtain a 1/T rate. We match the scaling with respect to p (up to a log factor), when β(p) = Θ( √ p) attains the lower bound, and we discuss a few such instances below. Otherwise, we are worse by a factor of p compared to the batch version. Intuitively, this is because we require different bounds on error terms E k in the online and the batch settings. The batch setting considers an empirical estimate, hence operates on the averaged error. Whereas in the online setting we suffer from the per sample error. Efficient concentration bounds exist for the batch case [16], while for the online case, no such bounds exist in general. Hence, we conjecture that our bounds in Theorem 2 are unimprovable in the online setting. Approximation Error: Note that the optimal decomposition M * = S * + L * is not identifiable in general without the incoherence-style conditions [15, 17]. In this paper, we provide efficient guarantees without assuming such strong incoherence constraints. This implies that there is an approximation error which is incurred even in the noiseless setting due to model non-identifiability. Algorithm 2: Regularized Epoch-based Admm for Stochastic Opt. in high-dimensioN 2 (REASON 2) Input ρ, ρ x , epoch length T 0 , regularization parameters {λ i , µ i } k T i=1 , initial prox centers˜S 1 , ˜ L 1 , initial radii R 1 , ˜ R 1 . Define Shrink κ (a) shrinkage operator as in REASON 1, G M k = M k+1 − S k − L k − 1 ρ Z k . for each epoch i = 1, 2, ..., k T do Initialize S 0 = ˜ S i , L 0 = ˜ L i , M 0 = S 0 + L 0 . for each iteration k = 0, 1, ..., T 0 − 1 do M k+1 = −∇f (M k ) + Z k + ρ(S k + L k ) + ρ x M k ρ + ρ x S k+1 = min S−˜Si1≤Ri λ i S 1 + ρ 2τ k S − (S k + τ k G M k ) 2 F L k+1 = min L−˜Li * ≤ ˜ Ri µ i L * + ρ 2 L − Y k − U k /ρ 2 F Y k+1 = min Y ∞≤α/p ρ 2τ k Y − (L k + τ k G M k ) 2 F + ρ 2 L k+1 − Y − U k /ρ 2 F Z k+1 = Z k − τ (M k+1 − (S k+1 + L k+1 )) U k+1 = U k − τ (L k+1 − Y k+1 ). Set: ˜ S i+1 = 1 T0 T0−1 k=0 S k and˜L i+1 := 1 T0 T0−1 k=0 L k if R 2 i > 2(s + r + (s+r) 2 pγ 2 ) α 2 p then Update R 2 i+1 = R 2 i /2, ˜ R 2 i+1 = ˜ R i 2 /2; else STOP; Table 3: Least square regression problem, epoch size T i = 2000, Error= θ−θ * 2 θ * 2 . Agarwal et al. [13] achieve an approximation error of sα 2 /p 2 for their batch algorithm. Our online algorithm has an approximation error of max{s + r, p}α 2 /p 2 , which is decaying with p. It is not clear if this bound can be improved by any other online algorithm. We now list some statistical models under which we achieve the batch-optimal rate for sparse+low rank decomposition.

1

) Independent Noise Model: Assume we sample i.i.d. matrices X k = S * + L * + N k , where the noise N k has independent bounded sub-Gaussian entries with max i,j Var(N k (i, j)) = σ 2 . We consider the square loss function, X k − S − L , we have w.h.p. N k = O(σ √ p). We match the batch bound in [13] in this setting. Moreover, Agarwal et al. [13] provide a minimax lower bound for this model, and we match it as well. Thus, we achieve the optimal convergence rate for online matrix decomposition for this model.

2

) Linear Bayesian Network: Consider a p-dimensional vector y = Ah + n, where h ∈ R r with r ≤ p, and n ∈ R p . The variable h is hidden, and y is the observed variable. We assume that the vectors h and n are each zero-mean sub-Gaussian vectors with i.i.d entries, and are independent of Run Time T = 50 sec T = 150 sec Error M * −S−L F M * F S−S * F S * F L * −L F L * F M * −S−L F M * F S−S * F S * F L * −L F L * F REASON 2 IALM 2.20e-03 5.11e-05 0.004 0.12 0.01 0.27 5.55e-05 8.76e-09 1.50e-04 0.12 3.25e-04 0.27 Table 4: REASON 2 and inexact ALM, matrix decomposition problem. p = 2000, η 2 = 0.01 one another. Let σ 2 h and σ 2 n be the variances for the entries of h and n respectively. Without loss of generality, we assume that the columns of A are normalized, as we can always rescale A and σ h appropriately to obtain the same model. Let Σ * y,y be the true covariance matrix of y. From the independence assumptions, we have Σ * y,y = S * + L * , where S * = σ 2 n I is a diagonal matrix and L * = σ 2 h AA has rank at most r. In each step k, we obtain a sample y k from the Bayesian network. For the square loss function f , we have the error E k = y k y k − Σ * y,y . Applying [Cor. 5.50][19], we have, with w.h.p. n k n k − σ 2 n I 2 = O( √ pσ 2 n ), h k h k − σ 2 h I 2 = O( √ pσ 2 h ). We thus have with probability 1 − T e −cp , E k 2 ≤ O √ p( A 2 σ 2 h + σ 2 n ) , ∀ k ≤ T. When A 2 is bounded, we obtain the optimal bound in Theorem 2, which matches the batch bound. If the entries of A are generically drawn (e.g., from a Gaussian distribution), we have A 2 = O(1 + r/p). Moreover, such generic matrices A are also " diffuse " , and thus, the low rank matrix L * satisfies Assumption A5, with α ∼ polylog(p). Intuitively, when A is generically drawn, there are diffuse connections from hidden to observed variables, and we have efficient guarantees under this setting.

Experiments

REASON 1: For sparse optimization problem, we compare REASON 1 with RADAR and ST-ADMM under the least-squares regression setting. Samples (x t , y t ) are generated such that x t ∈ Unif[−B, B] and y t = θ * , x + n t . θ * is s-sparse with s = log d. n t ∼ N (0, η 2 ). With η 2 = 0.5 in all cases. We consider d = 20, 2000, 20000 and s = 1, 3, 5 respectively. 500 1000 1500 2000 0 0.2 0.4 0.6 0.8 1 x 10 −4 tttt t2 500 1000 1500 2000 0 5 10 15 20 25 30 rr tttt t3 500 1000 1500 2000 0 1 2 3 4 rr tttt t4 500 1000 1500 2000 0 2 4 6 8 10 tttt t1 Figure 1: Least square regression, Error= θ−θ * 2 θ * 2

vs

. iteration number, d1 = 20 and d2 = 20000. The experiments are performed on a 2.5 GHz Intel Core i5 laptop with 8 GB RAM. See Table 3 for experiment results. It should be noted that RADAR is provided with information of θ * for epoch design and recentering. In addition, both RADAR and REASON 1 have the same initial radius. Nevertheless, REASON 1 reaches better accuracy within the same run time even for small time frames. In addition, we compare relative error θ − θ * 2 /θ * 2 in REASON 1 and ST-ADMM in the first epoch. We observe that in higher dimension error fluctuations for ADMM increases noticeably (see Figure 1). Therefore, projections of REASON 1 play an important role in denoising and obtaining good accuracy. REASON 2: We compare REASON 2 with state-of-the-art inexact ALM method for matrix decomposition problem (ALM codes are downloaded from [20]). Table 4 shows that with equal time, inexact ALM reaches smaller M * −S−L F M * F error while in fact this does not provide a good decomposition . Further, REASON 2 reaches useful individual errors. Experiments with η 2 ∈ [0.01, 1] show similar results. Similar experiments on exact ALM shows worse performance than inexact ALM.

Acknowledgment

We acknowledge detailed discussions with Majid Janzamin and thank him for valuable comments on sparse and low rank recovery. The authors thank Alekh Agarwal for detailed discussions of his work and the minimax bounds. A. Anandkumar is supported in part by Microsoft Faculty Fellowship, NSF Career award CCF-1254106, NSF Award CCF-1219234, and ARO YIP Award W911NF-13-1-0084.