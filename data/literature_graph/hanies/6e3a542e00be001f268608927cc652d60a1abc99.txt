Title: High-dimensional covariance decomposition into sparse Markov and independence models

Abstract: Fitting high-dimensional data involves a delicate tradeoff between faithful representation and the use of sparse models. Too often, sparsity assumptions on the fitted model are too restrictive to provide a faithful representation of the observed data. In this paper, we present a novel framework incorporating sparsity in different domains. We decompose the observed covariance matrix into a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse independence model (with a sparse covariance matrix). Our framework incorporates sparse covariance and sparse precision estimation as special cases and thus introduces a richer class of high-dimensional models. We characterize sufficient conditions for identifiability of the two models, viz., Markov and independence models. We propose an efficient decomposition method based on a modification of the popular 1-penalized maximum-likelihood estimator (1-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples n scales as n = Ω(d 2 log p), where p is the number of variables and d is the maximum node degree in the Markov model. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation.

Content: Introduction

Covariance estimation is a classical problem in multi-variate statistics. The idea that secondorder statistics capture important and relevant relationships between a given set of variables is natural. Finding the sample covariance matrix based on observed data is straightforward and widely used (Anderson, 1984). However, the sample covariance matrix is ill-behaved in high-dimensions, where the number of dimensions p is typically much larger than the number of available samples n (p n). Here, the problem of covariance estimation is ill-posed since the number of unknown parameters is larger than the number of available samples, and the sample covariance matrix becomes singular in this regime. Various solutions have been proposed for high-dimensional covariance estimation. Intuitively , by restricting the class of covariance models to those with a limited number of free parameters, we can successfully estimate the models in high dimensions. A natural

Janzamin and Anandkumar

anism to achieve this is to impose a sparsity constraint on the covariance matrix. In other words, it is presumed that there are only a few (off-diagonal) non-zero entries in the covariance matrix, which implies that the variables under consideration approximately satisfy marginal independence, corresponding to the zero pattern of the covariance matrix (Kauermann , 1996) (and we refer to such models as independence models). Many works have studied this setting and have provided guarantees for high-dimensional estimation through simple thresholding of the sample covariance matrix and other related schemes. See Section 1.2. In many settings, however, marginal independence is too restrictive and does not hold. For instance, consider the dependence between the monthly stock returns of various companies listed on the S&P 100 index. It is quite possible that a wide range of complex (and unobserved) factors such as the economic climate, interest rates etc., affect the returns of all the companies. Thus, it is not realistic to model the stock returns of various companies through a sparse covariance model. A popular alternative sparse model, based on conditional independence relationships, has gained widespread acceptance in recent years (Lauritzen, 1996). In this case, sparsity is imposed not on the covariance matrix, but on the inverse covariance or the precision matrix. It can be shown that the zero pattern of the precision matrix corresponds to a set of conditional-independence relationships and such models are referred to as graphical or Markov models. Going back to the stock market example, a first-order approximation is to model the companies in different divisions 1 as conditionally independent given the S&P 100 index variable, which captures the overall trends of the stock returns, and thus removes much of the dependence between the companies in different divisions. High-dimensional estimation in models with sparse precision matrices has been widely studied, and guarantees for estimation have been provided under a set of sufficient conditions. See Section 1.2 for related works. However, sparse Markov models may not be always sufficient to capture all the statistical relationships among variables. Going back to the stock market example, the approximation of using the S&P index node to capture the dependence between companies of different divisions may not be enough. For instance, there can still be a large residual dependence between the companies in manufacturing and mining divisions, which cannot be accounted by the S&P index node. In this paper, we consider decomposition of the observed data into two domains, viz., Markov and independence domains. We posit that the observed data results in a sparse graphical model under structured perturbations in the form of an independence model, see Figure 1. This framework encapsulates Markov and independence models, and incorporates a richer class of models which can faithfully capture complex relationships, such as in the stock market example above, and yet retain parsimonious representation. The idea that a combination of Markov and independence models can provide good model-fitting is not by itself new and perhaps the work which is closest to ours is the work by Choi et al. (2010), where multi-resolution models with a known hierarchy of variables is considered. Their model consists of a combination of a sparse precision matrix, which captures the conditional independence across scales, and a sparse covariance matrix, which captures the residual in-scale correlations. Heuristics for learning and inference are provided in Choi et al. (2010). However, the approach in Choi et al. (2010) has several deficiencies, including High-Dimensional Covariance Decomposition High-Dimensional Covariance Decomposition 1 Σ * J * M −1 Σ * R S R S M Figure 1: Representation of the covariance decomposition problem, where perturbing the observed covariance matrix with a structured noise model results in a sparse graphical model. The case where the noise model has sparse marginal dependencies is considered. oretical guarantees, assumption of a known sparsity support for the Markov model, use of expectation maximization (EM) which has no guarantees of reaching the global optimum, non-identifiability due to the presence of both latent variables and residual correlations, and so on. In contrast, we develop efficient convex optimization methods for decomposition , which are easily implementable and also provide theoretical guarantees for successful recovery. In summary, in this paper, we provide an in-depth study of efficient methods and guarantees for joint estimation of a combination of Markov and independence models. Our model reduces to sparse covariance and sparse inverse covariance estimation for certain choices of tuning parameter. Therefore, we incorporate a range of models from sparse covariance to sparse inverse covariance.

Summary of Contributions

We consider joint estimation of Markov and independence models, given observed data in a high dimensional setting. Our contributions in this paper are three fold. First, we derive a set of sufficient restrictions, under which there is a unique decomposition into the two domains, viz., the Markov and the independence domains, thereby leading to an identifiable model. Second, we propose novel and efficient estimators for obtaining the decomposition, under both exact and sample statistics. Third, we provide strong theoretical guarantees for high-dimensional learning, both in terms of norm guarantees and sparsistency in each domain, viz., the Markov and the independence domain. Our learning method is based on convex optimization. We adapt the popular 1 penalized maximum likelihood estimator (MLE), proposed originally for sparse Markov model selection and has efficient implementation in the form of graphical lasso (Friedman et al., 2007). This method involves an 1 penalty on the precision matrix, which is a convex relaxation of the 0 penalty, in order to encourage sparsity in the precision matrix. The Lagrangian dual of this program is a maximum entropy solution which approximately fits the given sample covariance matrix. We modify this program to our setting as follows: we incorporate an additional 1 penalty term involving the residual covariance matrix (corresponding to the independence model) in the max-entropy program. This term can be viewed as encouraging sparsity in the independence domain, while fitting a maximum entropy Markov model to the rest of the sample correlations. We characterize the optimal 3 Figure 1: Representation of the covariance decomposition problem, where perturbing the observed covariance matrix with a structured noise model results in a sparse graphical model. The case where the noise model has sparse marginal dependencies is considered. lack of theoretical guarantees, assumption of a known sparsity support for the Markov model, use of expectation maximization (EM) which has no guarantees of reaching the global optimum, non-identifiability due to the presence of both latent variables and residual correlations, and so on. In contrast, we develop efficient convex optimization methods for decomposition, which are easily implementable and also provide theoretical guarantees for successful recovery. In summary, in this paper, we provide an in-depth study of efficient methods and guarantees for joint estimation of a combination of Markov and independence models. Our model reduces to sparse covariance and sparse inverse covariance estimation for certain choices of tuning parameter. Therefore, we incorporate a range of models from sparse covariance to sparse inverse covariance.

Summary of Contributions

We consider joint estimation of Markov and independence models, given observed data in a high dimensional setting. Our contributions in this paper are three fold. First, we derive a set of sufficient restrictions, under which there is a unique decomposition into the two domains, viz., the Markov and the independence domains, thereby leading to an identifiable model. Second, we propose novel and efficient estimators for obtaining the decomposition, under both exact and sample statistics. Third, we provide strong theoretical guarantees for high-dimensional learning, both in terms of norm guarantees and sparsistency in each domain, viz., the Markov and the independence domain. Our learning method is based on convex optimization. We adapt the popular 1 penalized maximum likelihood estimator (MLE), proposed originally for sparse Markov model selection and has efficient implementation in the form of graphical lasso (Friedman et al., 2007). This method involves an 1 penalty on the precision matrix, which is a convex relaxation of the 0 penalty, in order to encourage sparsity in the precision matrix. The Lagrangian dual of this program is a maximum entropy solution which approximately fits the given sample covariance matrix. We modify this program to our setting as follows: we incorporate an additional 1 penalty term involving the residual covariance matrix (corresponding to the independence model) in the max-entropy program. This term can be viewed as encouraging sparsity in the independence domain, while fitting a maximum entropy Markov model to the rest of the sample correlations. We characterize the optimal solution of the above program, and also provide intuitions on the class of Markov and independence model combinations which can be incorporated under this framework. As a byproduct of this analysis, we obtain a set of conditions for identifiability of the two model components. We provide strong theoretical guarantees for our proposed method under a set of sufficient conditions. We establish that it is possible to obtain sparsistency and norm guarantees in both the Markov and the independence domains. We establish that the number of samples n is required to scale as n = Ω(d 2 log p) for consistency, where p is the number of variables, and d is the maximum degree in the Markov graph. The set of sufficient conditions for successful recovery are based on the so-called notion of mutual incoherence, which controls the dependence between different sets of variables (Ravikumar et al., 2011). In Section 7, the synthetic experiments are run on a model which does not necessarily satisfy sufficient mutual incoherence conditions; But we observe that our method has good numerical estimation performance even when the above incoherence conditions are not fully satisfied. We establish that our estimation reduces to sparse covariance and sparse inverse covariance estimation for certain choices of tuning parameter. On one end, it reduces to the 1 penalized MLE for sparse precision estimation (Ravikumar et al., 2011). On the other extreme, it reduces to (soft) threshold estimator for sparse covariance estimator, on lines of Bickel and Levina (2008). Moreover, our conditions for successful recovery are similar to those previously characterized for consistent estimation of sparse covariance/precision matrix. Our experiments validate our theoretical results on the sample complexity and demonstrate that our method is able to learn a richer class of models, compared to sparse graphical model selection, while requiring similar number of samples. In particular, our method is able to provide better estimates for the overall precision matrix, which is dense in general, while the performance of 1 -based optimization is worse since it attempts to approximate the dense matrix via a sparse estimate. Additionally, we demonstrate that our estimated models have better accuracy under simple distributed inference algorithms such as loopy belief propagation (LBP). This is because the Markov components of the estimated models tend to be more walk summable (Malioutov et al., 2006), since some of the correlations can be " transferred " to the residual matrix. Thus, in addition to learning a richer model class, incorporating sparsity in both covariance and precision domains, we also learn models amenable to efficient inference. We also apply our method to real data sets. We see the resulting models are fairly interpretable for the real data sets. For instance, for stock returns data set, we observe in both Markov and residual graphs that there exist edges among companies in the same division or industry, e.g., in the residual graph, nodes " HD " , " WMT " , " TGT " and " MCD " , all belonging to division Retail Trade form a partition. Also for foreign exchange rate data set, we observe that the statistical dependencies of foreign exchange rates are correlated with the geographical locations of countries, e.g., it is observed in the learned model that the exchange rates of Asian countries are more correlated. High-Dimensional Covariance Decomposition

Related Works

There have been numerous works on high-dimensional covariance selection and estimation, and we describe them below. In all the settings below based on sparsity of the covariance matrix in some basis, the notion of consistent estimation of the sparse support is known as sparsistency. Sparse Graphical Models: Estimation of covariance matrices by exploiting the sparsity pattern in the inverse covariance or the precision matrix has a long history. The sparsity pattern of the precision matrix corresponds to a Markov graph of a graphical model which characterizes the set of conditional independence relationships between the variables. Chow and Liu established that the maximum likelihood estimate (MLE) for tree graphical models reduces to a maximum weighted spanning tree algorithm where the edge weights correspond to empirical mutual information. The seminal work by Dempster (1972) on covariance selection over chordal graphs analyzed the convex program corresponding to the Gaussian MLE and its dual, when the graph structure is known. In the high-dimensional regime, penalized likelihood methods have been used in a number of works to achieve parsimony in covariance selection. Penalized MLE based on 1 penalty has been used in Huang et al. (2006); Meinshausen and Bühlmann (2006); d&apos;Aspremont et al. (2008); Banerjee et al. (2008); Rothman et al. (2008); Ravikumar et al. (2011), among numerous other works, where sparsistency and norm guarantees for recovery in high dimensions are provided. Graphical lasso (Friedman et al., 2007) is an efficient and popular implementation for the 1 -MLE. There have also been recent extensions to group sparsity structures(Yuan and Lin, 2006; Zhao et al., 2009), scenarios with missing samples (Loh and Wainwright, 2011) , semi-parametric settings based on non-paranormals (Liu et al., 2009), and to the non-parametric setting (Kolar et al., 2010). In addition to the convex methods, there have also been a number of non-convex methods for Gaussian graphical model selection (Spirtes and Meek, 1995; Kalisch and Bühlmann, 2007; Zhang, 2009; Anandkumar et al., 2011; Zhang, 2008). While we base much of our consistency analysis on Ravikumar et al. (2011), we also need to develop novel techniques to handle the delicate issue of errors in the two domains, viz., Markov and independence domains. Sparse Covariance Matrices: In contrast to the above formulation, alternatively we can impose sparsity on the covariance matrix. Note that the zero pattern in the covariance matrix corresponds to marginal independence relationships (Cox and Wermuth, 1993; Kauermann, 1996; Banerjee and Richardson, 2003). High-dimensional estimation of sparse covariance models has been extensively studied in El Karoui (2008); Bickel and Levina (2008); Cai et al. (2010), among others. Wagaman and Levina (2009) consider blockdiagonal and banded covariance matrices and propose an Isomap method for discovering meaningful orderings of variables. The work in Lam and Fan (2009) provides unified results for sparsistency under different sparsity assumptions, viz., sparsity in precision matrices, covariance matrices and models with sparse Cholesky decomposition. The above works provide strong guarantees for covariance selection and estimation under various sparsity assumptions. However, they cannot handle matrices which are combinations of different sparse representations, but are otherwise dense when restricted to any single representation. Decomposable Regularizers: Recent works have considered model decomposition based on observed samples into desired parts through convex relaxation approaches. Typically, each part is represented as an algebraic variety, which are based on semi-algebraic sets, and conditions for recovery of each component are characterized. For instance, decomposition of the inverse covariance matrix into sparse and low-rank varieties is considered in Chandrasekaran et al. (2009, 2010a); Candès et al. (2009) and is relevant for latent Gaussian graphical model. The work in Silva et al. (2011) considers finding a sparse-approximation using a small number of positive semi-definite (PSD) matrices, where the " basis " or the set of PSD matrices is specified a priori. In Negahban et al. (2010), a unified framework is provided for high-dimensional analysis of the so-called M -estimators, which optimize the sum of a convex loss function with decomposable regularizers. A general framework for decomposition into a specified set of algebraic varieties was studied in Chandrasekaran et al. (2010b). The above formulations, however, cannot incorporate our scenario, which consists of a combination of sparse Markov and independence graphs. This is because, although the constraints on the inverse covariance matrix (Markov graph) and the covariance matrix (independence graph) can each be specified in a straightforward manner, their combined constraints on the resulting covariance matrix is not easy to incorporate into a learning method. In particular, we do not have a decomposable regularizer for this setting. Multi-Resolution Models: Perhaps the work which is closest to ours is the work by Choi et al. (2010), where multi-resolution models with a known hierarchy of variables is considered . The model consists of a combination of a sparse precision matrix, which captures the conditional independence across scales, and a sparse covariance matrix, which captures the residual in-scale correlations. Heuristics for learning and inference are provided. However, the work has three main deficiencies: the sparsity support is assumed to be known, the proposed heuristics have no theoretical guarantees for success and the models considered are in general not identifiable, due to the presence of both latent variables and residual correlations.

Preliminaries and Problem Statement

Notation: For any vector v ∈ R p and a real number a ∈ [1, ∞), the notation v a refers to the a norm of vector v given by v a := p i=1 |v i | a 1 a . For any matrix U ∈ R p×p , the induced or the operator norm is given by |||U ||| a,b := max za=1 U z b for parameters a, b ∈ [1, ∞). Specifically, we use the ∞ operator norm which is equivalent to |||U ||| ∞ = max i=1,...,p p j=1 |U ij |. We also have |||U ||| 1 = |||U T ||| ∞ . Another induced norm is the spectral norm |||U ||| 2 (or |||U |||) which is equivalent to the maximum singular value of U . We also use the ∞ element-wise norm notation U ∞ to refer to the maximum absolute value of the entries of U . Note that it is not a matrix norm but a norm on the vectorized form of the matrix. The trace inner product of two matrices is denoted by U, V := Tr(U T V ) = i,j U ij V ij . Finally, we use the usual notation for asymptotics: f (n) = Ω(g(n)) if f (n) ≥ cg(n) for some constant c > 0 and f (n) = O(g(n)) if f (n) ≤ c g(n) for some constant c < ∞. High-Dimensional Covariance Decomposition

Gaussian Graphical Models

A Gaussian graphical model is a family of jointly Gaussian distributions which factor in accordance to a given graph. Given a graph G = (V, E), with V = {1, . . . , p}, consider a vector of Gaussian random variables X = [X 1 , X 2 , . . . , X p ], where each node i ∈ V is associated with a scalar Gaussian random variable X i . A Gaussian graphical model Markov on G has a probability density function (pdf) that may be parameterized as f X (x) ∝ exp − 1 2 x T Jx + h T x , (1) where J is a positive-definite symmetric matrix whose sparsity pattern corresponds to that of the graph G. More precisely, J(i, j) = 0 ⇐⇒ (i, j) / ∈ G. The matrix J is known as the potential or concentration matrix, the non-zero entries J(i, j) as the edge potentials, and the vector h as the potential vector. The form of parameterization in (1) is known as the information form and is related to the standard mean-covariance parameterization of the Gaussian distribution as µ = J −1 h, Σ = J −1 , where µ := E[X] is the mean vector and Σ := E[(X − µ)(X − µ) T ] is the covariance matrix. We say that a jointly Gaussian random vector X with joint pdf f (x) satisfies local Markov property with respect to a graph G if f (x i |x N (i) ) = f (x i |x V \i ) holds for all nodes i ∈ V , where N (i) denotes the set of neighbors of node i ∈ V and, V \ i denotes the set of all nodes excluding i. More generally, we say that X satisfies the global Markov property, if for all disjoint sets A, B ⊂ V , we have f (x A , x B |x S ) = f (x A |x S )f (x B |x S ). where set S is a separator 2 of A and B. The local and global Markov properties are equivalent for non-degenerate Gaussian distributions (Lauritzen, 1996). On lines of the above description of graphical models, consider the class of Gaussian models 3 N (µ, Σ Gc ), where the covariance matrix is supported on a graph G c (henceforth referred to as the conjugate graph), i.e., Σ Gc (i, j) = 0 ≡ (i, j) / ∈ G c . Recall that uncorrelated Gaussian variables are independent, and thus, X i ⊥ ⊥ X j ≡ (i, j) / ∈ G c .

2

. A set S ⊂ V is a separator for sets A and B if the removal of nodes in S partitions A and B into distinct components. 3. In the sequel, we denote the Markov graph, corresponding the support of the information matrix, as G and the conjugate graph, corresponding to the support of the covariance matrix, as Gc.

Janzamin and Anandkumar

Equivalence between pairwise independence and global Markov properties were studied in Cox and Wermuth (1993); Kauermann (1996); Banerjee and Richardson (2003). In this paper, we posit that the observed model results in a sparse graphical model under structure perturbations in the form of an independence model: Σ * + Σ * R = J * M −1 , Supp(J * M ) = G M , Supp(Σ * R ) = G R , (2) where Supp(·) denotes the set of non-zero (off-diagonal) entries, G M denotes the Markov graph and G R , the independence graph.

Problem Statement

We now give a detailed description of our problem statement, which consists of the covariance decomposition problem (given exact statistics) and covariance estimation problem (given a set of samples).

Covariance Decomposition Problem

A fundamental question to be addressed is the identifiability of the model parameters. Definition 1 (Identifiability) A parametric model {P θ : θ ∈ Θ} is identifiable with respect to a measure µ if there do not exist two distinct parameters θ 1 = θ 2 such that P θ 1 = P θ 2 almost everywhere with respect to µ. Thus, if a model is not identifiable, there is no hope of estimating the model parameters from observed data. A Gaussian graphical model (with no hidden variables) belongs to the family of standard exponential distributions (Wainwright and Jordan, 2008, Ch. 3). Under non-degeneracy conditions, it is also in the minimal form, and as such is identifiable (Brown, 1986). In our setting in (2), however, identifiability is not straightforward to address, and forms an important component of the covariance decomposition problem, described below. Decomposition Problem: Given the covariance matrix Σ * = J * M −1 − Σ * R as in (2), where J * M is an unknown concentration matrix and Σ * R is an unknown residual covariance matrix, how and under what conditions can we uniquely recover J * M and Σ * R from Σ * ? In other words, we want to address whether the matrices J * M and Σ * R are identifiable, given Σ * , and if so, how can we design efficient methods to recover them. If we do not impose any additional restrictions, there exists an equivalence class of models which form solutions to the decomposition problem. For instance, we can model Σ * entirely through an independence model (Σ * = Σ * R ), or through a Markov model (Σ * = J * M −1 ). However, in most scenarios, these extreme cases are not desirable, since they result in dense models, while we are interested in sparse representations with a parsimonious use of edges in both the graphs, viz., the Markov and the independence graphs. In Section 3.1, we provide a sufficient set of structural and parametric conditions to guarantee identifiability of the Markov and the independence components, and in Section 3.2, we propose an optimization program to obtain them. In the above decomposition problem, we assume that the exact covariance matrix Σ * is known. However, in practice, we only have access to samples, and we describe this setting below. Denote Σ n as the sample covariance matrix 4 Σ n := 1 n n k=1 x (k) x T (k) , (3) where x (k) , k = 1, ..., n are n i.i.d. observations of a zero mean Gaussian random vector X ∼ N (0, Σ * ), where X := (X 1 , ..., X p ). Now the estimation problem is described below. Estimation Problem: Assume that there exists a unique decomposition Σ * = J * M −1 −Σ * R where J * M is an unknown concentration matrix with bounded entries and Σ * R is an unknown sparse residual covariance matrix given a set of constraints. Given the sample covariance matrix Σ n , our goal is to find estimates of J * M and Σ * R with provable guarantees. In the sequel, we relate the exact and the sample versions of the decomposition problem. In Section 4, we propose a modified optimization program to obtain efficient estimates of the Markov and independence components. Under a set of sufficient conditions, we provide guarantees in terms of sparsistency, sign consistency, and norm guarantees, defined below. Definition 2 (Estimation Guarantees) We say that an estimate ( J M , Σ R ) to the decomposition problem in (2), given a sample covariance matrix Σ n , is sparsistent or model consistent, if the supports of J M and Σ R coincide with the supports of J * M and Σ * R respectively . It is said to be sign consistent, if additionally, the respective signs coincide. The norm guarantees on the estimates is in terms of bounds on J M − J * M and Σ R −Σ * R , under some norm ··.

Analysis under Exact Statistics

In this section, we provide the results under exact statistics.

Conditions for Unique Decomposition

We first provide a set of sufficient conditions under which we can guarantee that the decomposition of Σ * in (2) into concentration matrix J * M and residual matrix Σ * R is unique. 5 We impose the following set of constraints on the two matrices: (A.0) Σ * and J * M are positive definite matrices, i.e., Σ * 0, J * M 0. (A.1) Off-diagonal entries of J * M are bounded from above, i.e., J * M ∞,off ≤ λ * , for some λ * > 0.

4.

Without loss of generality, we limit our analysis to zero-mean Gaussian models. The results can be easily generalized to models with non-zero means.

5

. We drop the positive definite constraint on the residual matrix Σ * R thereby allowing for a richer class of covariance decomposition. In Section 5.3, we modify the conditions and the learning method to incorporate positive definite residual matrices Σ * R .

Janzamin and Anandkumar

(A.2) Diagonal entries of Σ * R are zero: Σ * R ii = 0, and the support of its off-diagonal entries satisfies Σ * R ij = 0 ⇐⇒ | J * M ij | = λ * , ∀ i = j. (A.3) For any i, j, we have sign Σ * R ij . sign J * M ij ≥ 0, i.e, the signs are the same. Indeed, the above constraints restrict the class of models for which we can provide guarantees. However, in many scenarios, the above assumptions may be reasonable, and we now provide some justifications. (A.0) is a natural assumption to impose since we are interested in valid Σ * and J * M matrices. Condition (A.1) corresponds to bounded offdiagonal entries of J * M . Intuitively, this limits the extent of " dependence " between the variables in the Markov model, and can lead to models where inference can be performed with good accuracy using simple algorithms such as belief propagation. Condition (A.2) limits the support of the residual matrix Σ * R : the residual covariances are captured at those locations (edges) where the concentration entries (J * M ) i,j are " clipped " (i.e., the bound λ * is achieved). Intuitively, the Markov matrix J * M is unable to capture all the correlations between the node pairs due to clipping, and the residual matrix Σ * R captures the remaining correlations at the clipped locations. Condition (A.3) additionally characterizes the signs of the entries of Σ * R . For the special case, when the Markov model is attractive, i.e., (J * M ) i,j ≤ 0 for i = j, the residual entries (Σ * R ) i,j are also all negative. This implies that the model corresponding to Σ * is also attractive, since it only consists of positive correlations. By default, we set the diagonal entries of the residual matrix to zero in (A.2) and thus, assume that the Markov matrix captures all the variances in the model. In Section 4.2.1, we provide a simple example of a Markov chain and a residual covariance model satisfying the above conditions. It is also worth mentioning that the number of model parameters satisfying above conditions is equivalent to the number of parameters in the special case of sparse inverse covariance estimation when λ → ∞ (Ravikumar et al., 2011). It is assumed in assumption (A.2) that the residual matrix Σ * R takes nonzero value when the corresponding entry in the Markov matrix J * M takes its maximum absolute value λ * . This assumption in conjunction with the sign assumption in (A.3), exactly determines the Markov entry J M ij when the corresponding residual entry Σ R ij = 0. So, for each (i, j) pair, only one of the entries J M ij and Σ R ij are unknown which results that the proposed model in this paper does not introduce additional parameters comparing to the sparse inverse covariance estimation, which is interesting. According to the above discussion, we observe that the overall covariance and inverse covariance matrices Σ * and J * = Σ * −1 are dense, but represented with small number of parameters. It is interesting that we are able to represent models with dense patterns, but it is important to notice that the sparse representation leads to some restrictions on the model. In the sequel, we propose an efficient method to recover the respective matrices J * M and Σ * R under conditions (A.0)-(A.3) and then establish the uniqueness of the decomposition. Finally, note that we do not impose any sparsity constraints on the concentration matrix J * M , and in fact, our method and guarantees allow for dense matrices J * M , when the exact High-Dimensional Covariance Decomposition covariance matrix Σ * is available. However, when only samples are available, we limit ourselves to sparse J * M and provide learning guarantees in the high-dimensional regime, where the number of samples can be much smaller than the number of variables.

Formulation of the Optimization Program

We now propose a method based on convex optimization for obtaining (J * M , Σ * R ) given the covariance matrix Σ * in (2). Consider the following program Σ M , Σ R := arg max Σ M 0,Σ R log det Σ M − λΣ R 1,off (4) s. t. Σ M − Σ R = Σ * , (Σ R ) d = 0, where ·· 1,off denotes the 1 norm of the off-diagonal entries, which is the sum of the absolute values of the off-diagonal entries, and (·) d denotes the diagonal entries. Intuitively, the parameter λ imposes a penalty on large residual covariances, and under favorable conditions, can encourage sparsity in the residual matrix. The program in (4) can be recast Σ M , Σ R := arg max Σ M 0,Σ R log det Σ M (5) s. t. Σ M − Σ R = Σ * , (Σ R ) d = 0, Σ R 1,off ≤ C(λ), for some constant C(λ) depending on λ. The objective function in the above program corresponds to the entropy of the Markov model (modulo a scaling and a shift factor) (Cover and Thomas, 2006), and thus, intuitively, the above program looks for the optimal Markov model with maximum entropy subject to an 1 constraint on the residual matrix. We declare the optimal solution Σ R in (4) as the estimate of the residual matrix Σ * R , and J M := Σ −1 M as the estimate of the Markov concentration matrix J * M . The justification behind these estimates is based on the fact that the Lagrangian dual of the program in (4) is (see Appendix A) J M := arg min J M 0 Σ * , J M − log det J M (6) s. t. J M ∞,off ≤ λ, where ·· ∞,off denotes the ∞ element-wise norm of the off-diagonal entries, which is the maximum absolute value of the off-diagonal entries. Further, we show in Appendix A that the following relations exist between the optimal primal 6 solution J M and the optimal dual solution Σ M , Σ R : J M = Σ −1 M , and thus, J −1 M − Σ R = Σ * is a valid decomposition of the covariance matrix Σ * . Remark 3 Notice that when the ∞ constraint is removed in the primal program in (6), which is equivalent to letting λ → ∞, the program corresponds to the maximum likelihood estimate, and the optimal solution in this case is J M = Σ * −1 . Similarly, in the dual program in (4), when λ → ∞, the optimal solution corresponds to Σ M = Σ * and Σ R = 0. At the other extreme, when λ → 0, J M is a diagonal matrix, and the residual matrix Σ R is in general, a full matrix (except for the diagonal entries). Thus, the parameter λ allows us to carefully tune the contributions of the Markov and residual components, and we notice in our experiments in Section 7 that λ plays a crucial role in obtaining efficient decomposition into Markov and residual components.

Guarantees and Main Results

We now establish that the optimal solutions of the proposed optimization programs in (4) and (6) lead to a unique decomposition of the given covariance matrix Σ * under conditions ), given a covariance matrix Σ * , if we set the parameter λ = J * M ∞,off in the optimization program in (4), then the optimal solutions of primal-dual optimization programs (6) and (4) are given by J M , Σ R = J * M , Σ * R , and the decomposition is unique. See the proof in Appendix C. Thus, we establish that the proposed optimization programs in (4) and (6) uniquely recover the Markov concentration matrix J * M and the residual covariance matrix Σ * R given Σ * under conditions (A.0)–(A.3).

Sample Analysis of the Algorithm

In this section, we provide the results under sample statistics where some i.i.d. samples of random variables are only available.

Optimization Program

We have so far provided guarantees on unique decomposition given the exact covariance matrix Σ * . We now consider the case, when n i.i.d. samples are available from N (0, Σ * ), which allows us to estimate the sample covariance matrix Σ n , as in (3). We now modify the dual program in (4), considered in the previous section, to incorporate the sample covariance matrix Σ n as follows Σ M , Σ R := arg max Σ M ,Σ R log det Σ M − λΣ R 1,off (7) s. t. Σ n − Σ M + Σ R ∞,off ≤ γ, Σ M d = Σ n d , Σ R d = 0, Σ M 0, Σ M − Σ R 0. Note that, in addition to substituting Σ * by Σ n , there are two more modifications in the above program comparing to the exact case in (4). First, the positive-definiteness constraint on the overall covariance matrix Σ = Σ M − Σ R is added to make sure that the overall covariance matrix estimation is valid. This constraint is not required in the exact case since we have the constraint Σ = Σ * in that case which ensures the positive-definiteness of High-Dimensional Covariance Decomposition overall covariance matrix according to assumption (A.0) that Σ * 0. Second, the equality constraint Σ M − Σ R = Σ * is relaxed on the off-diagonal entries by introducing the new parameter γ which allows some deviation. More discussion including the Lagrangian primal form of the above optimization program and the effect of new parameter γ is provided in section 6.

Assumptions under Sample Statistics

We now provide conditions under which we can provide guarantees for estimating the Markov model J * M and the residual model Σ * R , given the sample covariance Σ n in high dimensions. These are conditions in addition to conditions (A.0)–(A.3) in Section 3.1. The additional assumptions for successful recovery in high dimensions are based on the Hessian of the objective function in the optimization program in (19), with respect to the variable J M , evaluated at the true Markov model J * M . The Hessian of this function is given by Boyd and Vandenberghe ( 2004) Γ * = J * M −1 ⊗ J * M −1 = Σ * M ⊗ Σ * M , (8) where ⊗ denotes the Kronecker matrix product (Horn and Johnson, 1985). Thus Γ * is a p 2 × p 2 matrix indexed by the node pairs. Based on the results for exponential families (Brown, 1986), Γ * (i,j),(k,l) = Cov{X i X j , X k X l }, and hence it can be interpreted as an edge-based alternative to the usual covariance matrix Σ * M . Define K M as the ∞ operator norm of the covariance matrix of the Markov model K M := |||Σ * M ||| ∞ . We now denote the supports of the Markov and residual models. Denote E M := {(i, j) ∈ V × V |i = j, J * M ij = 0} as the edge set of Markov matrix J * M . Define S M := E M ∪ {(i, i)|i = 1, ..., p}, (9) S R := {(i, j) ∈ V × V | Σ * R ij = 0}. (10) Thus, the set S M includes diagonal entries and also all edges of Markov graph corresponding to J * M . Also, recall from (A.2) in Section 3.1 that the diagonal entries of Σ * R are set to zero, and that the support set S R is contained in S M , i.e., S R ⊂ S M . Let S c M and S c R denote the respective complement sets. Define S := S M ∩ S c R , (11) so that {S R , S, S c M } forms a partition of {(1, ..., p) × (1, ..., p)}. This partitioning plays a crucial role in being able to provide learning guarantees. Define the maximum node degree for Markov model J * M as d := max j=1,...,p |{i : (i, j) ∈ S M }|. Finally, for any two subsets T and T of V × V , Γ * T T denotes the submatrix of Γ * indexed by T as rows and T as columns. We now impose various constraints on the submatrices of the Hessian in (8), limited to each of the sets {S R , S, S c M }. (A.4) Mutual Incoherence: These conditions impose mutual incoherence among three partitions of Γ * indexed by S R , S c M and S. For some α ∈ (0, 1], we have max{|||Γ * S c M S Γ * SS −1 Γ * SS R − Γ * S c M S R ||| ∞ , |||Γ * S c M S Γ * SS −1 ||| ∞ } ≤ (1 − α), (12) K SS R := ||| Γ * SS −1 Γ * SS R ||| ∞ < 1 4 . (13) (A.5 ) Covariance Control: For the same α specified above, we have the bound: The minimum eigenvalue of overall covariance matrix Σ * satisfies the lower bound λ min (Σ * ) ≥ C 6 d log(4p τ ) n + C 7 d 2 log(4p τ ) n for some C 6 , C 7 > 0 and τ > 2. In (A.4), the condition in (12) bounds the effect of the non-edges of the Markov model, indexed by S c M , to its edges, indexed by S R and S. Note that we distinguish between the common edges of the Markov model with the residual model (S R ) and the remaining edges of the Markov model (S). The second condition in (13) controls the influence of the edgebased terms which are shared with the residual matrix, indexed by S R , to other edges of the Markov model, indexed by S = S M ∩ S c R . Condition (A.5) imposes ∞ bounds on the rows of (Γ * SS ) −1 . Note that for sufficiently large m, the bound in (14) tends to α 4(1−α) . Also note that the conditions (A.4) and (A.5) are only imposed on the Markov model J * M and there are no additional constraints on the residual matrix Σ * R (other than the conditions previously introduced in Section 3.1). In condition (A.6), it is assumed that the minimum eigenvalue of overall covariance matrix Σ * is sufficiently far from zero to make sure that its estimation Σ is positive definite and therefore a valid covariance matrix. In this section, we propose a simple model satisfying assumptions (A.0)–(A.5). Consider a Markov chain with concentration matrix J * M over 4 nodes, as shown in Figure 2. The diagonal entries in the corresponding covariance matrix Σ * M = J * M −1 are set to unity, and the correlations between the neighbors in J * M are set uniformly to some value ρ ∈ (−1, 1), i.e., Σ * M ij = ρ for (i, j) ∈ E M . Due to the Markov property, the correlations between other node pairs are given by Σ * M 13 = Σ * M 24 = ρ 2 and Σ * M 14 = ρ 3 . For the residual covariance matrix Σ * R , we consider one edge between nodes 1 and 2, i.e., S R = {(1, 2), (2, 1)}. It is easy to see that conditions (A.0)–(A.2) are satisfied. Recall that S c M = {(i, j) : (i, j) / ∈ E M } and the remaining node pairs belongs to set S := S M \ S R . Through some straightforward calculations, we can show that for any |ρ| < 0.07, the mutual incoherence conditions in (A.4) and (A.5) are satisfied for α = 0.855 and m ≥ 83. Note that the value of nonzero entries of Σ * R are not involved or restricted by these assumptions. However, they do need to satisfy the sign condition in (A.3). Thus, we have non-trivial models High-Dimensional Covariance Decomposition High-Dimensional Covariance Decomposition 1 2 3 4 Figure 2: Example of a Markov chain and a residual covariance matrix, where a residual edge is present between nodes 1 and 2. satisfying the set of sufficient conditions for successful high-dimensional estimation. 7 In Section 7, the synthetic experiments are run on a model which does not necessarily satisfy mutual incoherence conditions (A.4) and (A.5); But we observe that our method has good numerical estimation performance even when the above incoherence conditions are not fully satisfied.

Guarantees and Main Results

We are now ready to provide the main result of this paper.

Theorem 5

Consider a Gaussian distribution with covariance matrix Σ * = J * M −1 − Σ * R satisfying conditions (A.0)-(A.6). Given a sample covariance matrix Σ n using n i.i.d. samples from the Gaussian model, let J M , Σ R denote the optimal solutions of the primal-dual pair (19) and (7), with parameters γ = C 1 log p/n and λ = λ * + C 2 log p/n for some constants C 1 , C 2 > 0, where λ * := J * M ∞,off . Suppose that Σ * R min := min (i,j)∈S R | Σ * R ij | scales as Σ * R min = Ω log p/n and the sample size n is lower bounded as n = Ω d 2 log p , (15) then with probability greater than 1 − 1/p c → 1 (for some c > 0), we have: a) The estimates J M 0 and Σ R satisfy ∞ bounds J M − J * M ∞ = O log p n , Σ R − Σ * R ∞ = O log p n . b) The estimate Σ R is sparsistent and sign consistent with Σ * R . c) If in addition, J * M min := min (i,j)∈S M | J * M ij | scales as J * M min = Ω log p/n , then the estimate J M is sparsistent and sign consistent with J * M . 7. Similarly, for the case when the correlations corresponding to Markov edges are distinct as Σ * M 12 = ρ1, Σ * M 23 = ρ2, and Σ * M 34 = ρ3, we can argue the same conditions. For compatibility with Figure 2, assume that ρ1 is the maximum among these three parameters, and therefore, the residual edge is between nodes 1 and 2. This is because the maximum of off-diagonal entries of J * M also happens in entry (1, 2). Then, the same condition |ρ1| < 0.07 is sufficient for satisfying conditions (A.0)–(A.5).

15

Figure 2: Example of a Markov chain and a residual covariance matrix, where a residual edge is present between nodes 1 and 2. satisfying the set of sufficient conditions for successful high-dimensional estimation. 7 In Section 7, the synthetic experiments are run on a model which does not necessarily satisfy mutual incoherence conditions (A.4) and (A.5); But we observe that our method has good numerical estimation performance even when the above incoherence conditions are not fully satisfied.

Guarantees and Main Results

We are now ready to provide the main result of this paper. Σ n using n i.i.d. samples from the Gaussian model, let J M , Σ R denote the optimal solutions of the primal-dual pair (19) and (7), with parameters γ = C 1 log p/n and λ = λ * + C 2 log p/n for some constants C 1 , C 2 > 0, where λ * := J * M ∞,off . Suppose that Σ * R min := min (i,j)∈S R | Σ * R ij | scales as Σ * R min = Ω log p/n and the sample size n is lower bounded as n = Ω d 2 log p , (15) then with probability greater than 1 − 1/p c → 1 (for some c > 0), we have: a) The estimates J M 0 and Σ R satisfy ∞ bounds J M − J * M ∞ = O log p n , Σ R − Σ * R ∞ = O log p n . b) The estimate Σ R is sparsistent and sign consistent with Σ * R . c) If in addition, J * M min := min (i,j)∈S M | J * M ij | scales as J * M min = Ω log p/n , then the estimate J M is sparsistent and sign consistent with J * M . 7. Similarly, for the case when the correlations corresponding to Markov edges are distinct as Σ * M 12 = ρ1, Σ * M 23 = ρ2, and Σ * M 34 = ρ3, we can argue the same conditions. For compatibility with Figure 2, assume that ρ1 is the maximum among these three parameters, and therefore, the residual edge is between nodes 1 and 2. This is because the maximum of off-diagonal entries of J

Janzamin and Anandkumar

Proof See Appendix D. Remark 6 Here, we provide a few more observations and extensions as follows. 1. Non-asymptotic sample complexity and error bounds: In the above theorem, we establish that the number of samples is required to scale as n = Ω(d 2 log p). In fact, our results are non-asymptotic, and the exact constants are provided in inequality (31). The non-asymptotic form of error bounds are also provided in (34) and (40).

2

. Extension to sub-Gaussian and other distributions: In the above theorem, we considered Gaussian distribution. Similar to high dimensional covariance estimation in Ravikumar et al. (2011), the result in the theorem can be easily extended to sub- Gaussian and other distributions with known tail conditions.

3.

Comparison between direct estimation of Σ * and the above decomposition: The overall matrix Σ * (and J * ) is a full matrix in general. Thus, if we want to estimate it directly, we need n = Ω p 2 log p samples since the maximum node degree is Θ(p). Therefore, we can not estimate it directly in high dimensional regime and it demonstrates the importance of such sparse covariance + inverse covariance models for estimation. We discussed Remark 3 that the parameter λ allows us to carefully tune the contributions of the Markov and residual components. When λ → ∞, the program corresponds to 1 penalized maximum likelihood estimator which is well-studied in Ravikumar et al. (2011); Rothman et al. (2008). In this case, Σ R = 0 and all the dependencies among random variables are captured by the sparse graphical model represented by J M . On the other extreme, when λ * = 0 and thus λ = C 2 log p/n → 0, with increasing the number of samples n, the off-diagonal entries in J M are bounded too tight by λ (refer to the primal program in (19)) and therefore the residual covariance matrix Σ R captures most of the dependencies among random variables. In this case, we have the covariance estimation Σ = Σ M − Σ R , where the diagonal entries are included in Σ M and the off-diagonal entries are mostly included in − Σ R . In order to explain the results for these cases in a more concrete way, we explicitly mention the results for both sparse inverse covariance estimation (λ → ∞) and sparse covariance estimation (λ ≈ 0) methods in the following subsections. Note that both of these are special cases of the general result expressed in Theorem 5. Thus, in Theorem 5, we generalize these extreme cases to models with a linear combination of sparse covariance and sparse inverse covariance matrices.

Discussions and Extension

In this section, we first provide a detailed discussion of special cases sparse covariance and sparse inverse covariance estimation. Then, the extension of results to the structured noise model is mentioned. In this section, we mention the result for sparse inverse covariance estimation in high dimensional regime. This result is provided by Ravikumar et al. (2011) and is a special case of Theorem 5 when the parameter λ goes to infinity. Before proposing the explicit result in Corollary 7, we state how the required conditions in Theorem 5 reduces to the conditions in Ravikumar et al. (2011). Since the support of residual matrix Σ * R is a zero matrix in this special case, the mutual incoherence conditions in (A.4) reduce exactly to the same mutual incoherence condition in Ravikumar et al. (2011) as |||Γ * S c S Γ * SS −1 ||| ∞ ≤ (1 − α) for some α ∈ (0, 1], (16) where S = S M is the support of Markov matrix J * = J * M as defined in (9). Also note that the covariance control condition (A.5) is not required any more. Furthermore, the sample complexity and convergence rate of J * M estimation in Theorem 5 exactly reduce to the results in Ravikumar et al. (2011) as (for q = 8, l = 3) n > n f p τ ; 1/ max v * ,2ld 1 + q α K SS K M max 1, 2 l − 1 1 + q α K SS K 2 M , (17) J − J * ∞ ≤ 2K SS 1 + q α δ f (p τ ; n), (18) where the result is valid for any q ≥ 8 and l > 1. Corollary 7 (Sparse Inverse Covariance Estimation (Ravikumar et al., 2011)) Consider a Gaussian distribution with covariance matrix Σ * = J * −1 satisfying mutual incoherence condition (16). Given a sample covariance matrix Σ n using n i.i.d. samples from the Gaussian model, let J denote the optimal solution of the primal-dual pair (19) and (7), with parameters γ = C 1 log p/n and λ → ∞ (removing ∞ constraints in the primal program (19)) for some constant C 1 > 0. Suppose that the sample size n is lower bounded as n = Ω d 2 log p , then with probability greater than 1 − 1/p c → 1 (for some c > 0), we have: a) The estimate J 0 satisfies ∞ bound J − J * ∞ = O log p n . b) If in addition J * min := min (i,j)∈S M | J * ij | scales as J * min = Ω log p/n , the estimate J is sparsistent and sign consistent with J * . Remark 8 (Comparison of general result in Theorem 5 and sparse inverse covariance estimation in Corollary 7) Considering the results in Theorem 5, sample complexity and convergence rate of estimated models are exactly the same as results in Ravikumar et al. (2011) with only some minor differences in coefficients. Compare (31) with (17) for sample complexity and (34) with (18) for convergence rate of estimated Markov matrix J M . But regarding the mutual incoherence conditions, we observe that the conditions for the special case sparse inverse covariance estimation in (16) are less restrictive than the conditions for the general case in (12)-(13). Since the sparse inverse covariance estimation (Ravikumar et al., 2011) is a special case of the general model in this paper, this additional limitation on models is inevitable, i.e., it is natural that we need some more incoherence conditions in order to be able to recover both the Markov and residual models in the general case.

Sparse Covariance Estimation

High-dimensional estimation of sparse covariance models has been studied in Bickel and Levina (2008). They propose an estimation of a class of sparse covariance matrices by " hard thresholding " . They also prove spectral norm guarantees on the error between the estimated and exact covariance matrices. We also recover similar results in the other extreme case of proposed program (7) when λ ≈ 0. The program reduces to the sparse covariance estimator as discussed earlier. In order to see that again, let us investigate the dual program restated as follows Σ M , Σ R := arg max Σ M ,Σ R log det Σ M − λΣ R 1,off s. t. Σ n − Σ M + Σ R ∞,off ≤ γ, Σ M d = Σ n d , Σ R d = 0, Σ M 0, Σ M − Σ R 0. When the parameter λ ≈ 0, the variable Σ R is very slightly penalized in the objective function. Therefore, most of the statistical dependencies are captured by Σ R and thus, off-diagonal entries of Σ M take very small values. Furthermore, according to the property of optimization program that the support of Σ R is contained within the support of J M , sparsity on Σ R is encouraged by the effect of parameter γ. It is also observed that we are approximately performing " soft thresholding " in program (7) (when λ ≈ 0) comparing to " hard thresholding " in Bickel and Levina (2008). Consider the case λ = 0, where the Markov part Σ M is a diagonal matrix. Therefore, the Σ n − Σ M + Σ R ∞,off ≤ γ constraint in the dual program (7) reduces to Σ n + Σ R ∞,off ≤ γ where it is seen that the negative soft thresholding is performed on matrix Σ n with threshold parameter γ, given by S γ (x) = sign(−x)(|x| − γ) + . Notice that we need to have λ ≈ 0 for recovering the sparse covariance matrix given empirical covariances and in this case, we can view the estimator as approximately performing soft thresholding. Finally, we propose the corollary for this special case. Before that, we need some additional definitions for a general covariance matrix Σ * . Similar to definition (10), the support of a covariance matrix Σ * is defined as S Σ := {(i, j) ∈ V × V |Σ * ij = 0}.

High-Dimensional Covariance Decomposition

The maximum node degree for a covariance matrix Σ * is also defined as d Σ := max j=1,...,p |{i : (i, j) ∈ S Σ }|. Corollary 9 (Sparse Covariance Estimation) Consider a Gaussian distribution with covariance matrix Σ * satisfying eigenvalue control condition (A.6). Given a sample covariance matrix Σ n using n i.i.d. samples from the Gaussian model, let Σ M , Σ R denote the optimal solutions of the primal-dual pair (19) and (7), with parameters γ = C 1 log p/n and λ = C 2 log p/n for some constants C 1 , C 2 > 0. The estimated covariance matrix Σ is defined as Σ off := − Σ R and Σ d := Σ M d . Suppose that Σ * off min := min (i,j)∈S Σ ,i =j | Σ * ij | scales as Σ * off min = Ω log p/n and the sample size n is lower bounded as n = Ω d 2 Σ log p , then with probability greater than 1 − 1/p c → 1 (for some c > 0), we have: a) The estimate Σ satisfies ∞ bound Σ − Σ * ∞,off = O log p n . b) The estimate Σ off is sparsistent and sign consistent with Σ * off . In the discussion up to now, we considered general residual matrices Σ * R , not necessarily positive definite, thereby allowing for a rich class of covariance decomposition models. In this section, we modify the conditions and the learning method to incorporate positivedefinite residual matrices Σ * R . We regularize the diagonal entries in an appropriate way to ensure that both J * M and Σ * R are positive definite. Thus, the identifiability assumptions (A.0)-(A.3) are modified as follows: (A.0') Σ * , Σ * R and J * M are positive definite matrices, i.e., Σ * 0, Σ * R 0, J * M 0. (A.1') J * M is normalized such that J * M d = λ * 1 for some λ * 1 > 0 and off-diagonal entries of J * M are bounded from above, i.e., J * M ∞,off ≤ λ * 2 , for some λ * 2 > 0. (A.2') The off-diagonal entries of Σ * R satisfy Σ * R ij = 0 ⇐⇒ | J * M ij | = λ * 2 , ∀ i = j. (A.3') For any i, j, we have sign Σ * R ij . sign J * M ij ≥ 0, i.e, the signs are the same. It is seen in (A.1') that we put additional restrictions on diagonal entries of the Markov matrix J * M in order to have nonzero diagonal entries for the residual matrix Σ * R . Similar to the general form of dual program introduced in (23), we propose the following optimization program to estimate the Markov and residual components in the structured noise model: Σ M , Σ R := arg max Σ M ,Σ R 0 log det Σ M − λ 1 Σ R 1,on − λ 2 Σ R 1,off s. t. Σ n + Σ R − Σ M ∞,off ≤ γ, Σ n d + Σ R d = Σ M d . The decomposition result under exact statistics can be similarly proven by setting parameter γ = 0 when the identifiability assumptions (A.0')-(A.3') are satisfied. Furthermore, under additional estimation assumptions (A.4)-(A.6), the sample statistics guarantees in Theorem 5 can be also extended to the solutions of above program.

Proof Outline

In this section, the Lagrangian primal form for the proposed dual program (7) is provided first and then the proof outlne is presented. For now, we drop the positive-definiteness constraint Σ M − Σ R 0 in the proposed dual program (7). We finally show that this constraint is satisfied for the proposed estimation under specified conditions and thus this constraint can be dropped. In the subsequent discussion, we drop this constraint. It is shown in Appendix A that the primal form for this reduced dual program is J M := arg min J M 0 Σ n , J M − log det J M + γJ M 1,off (19) s. t. J M ∞,off ≤ λ, We further establish that Σ M = J −1 M is valid between the dual variable Σ M and primal variable J M and thus, Σ n − J −1 M + Σ R ∞,off ≤ γ. (20) Comparing the above with the exact decomposition Σ * = J * M −1 − Σ * R in (2), we note that for the sample version, we do not exactly fit the Markov and the residual models with the sample covariance matrix Σ n , but allow for some divergence, depending on γ. Similarly, the primal program (19) has an additional 1 penalty term on J M , which is absent in (6). Having a non-zero γ in the primal program enables us to impose a sparsity constraint on J M , which in turn, enables us to estimate the matrices in the high dimensional regime (p n), under a set of conditions of sufficient conditions given in section 4.2. We now provide a high-level description of the proof for Theorem 5. The detailed proof is given in Appendix D. The proof is based on the primal-dual witness method, which has been previously employed in Ravikumar et al. (2011) and other works. However, we require significant modifications of this approach in order to handle the more complex setting of covariance decomposition. In the primal-dual witness method, we define a modified version of the original optimization program (19). Note that the key idea in constructing the modified version is to be Figure 3: The sets S R , S and S

c

M form a partition of {(1, ..., p) × (1, ..., p)}, where p is the number of nodes, S R is the support of the residual covariance matrix Σ * R and S M is the support of the precision matrix J * M of the Markov model and S c M is its complement. able to analyze it and prove guarantees for it in a less complicated way comparing to the original version. Let us denote the solutions of the modified program by J M , Σ R pair. In general, the optimal solutions of the two programs, original and modified one, are different. However, under conditions (A.0)–(A.5), we establish that their optimal solutions coincide. See Appendix D for details. Through this equivalence, we thus establish that the optimal solution J M , Σ R of the original program in (19) inherits all the properties of the optimal solution J M , Σ R of the modified program, i.e., the solutions of the modified program act as witness for the original program. In the following, we define the modified optimization program and its properties. The primal-dual witness method steps which guarantee the equivalence between solutions of the original and the modified program are mentioned in Appendix D. We modify the sample version of our optimization program in (19) as follows: J M := arg min J M 0 Σ n , J M − log det J M + γJ M 1,off (21) s. t. J M S c M = 0, J M S R = λ sign J * M S R . Note that since we do not a priori know the supports of the original matrices J * M and Σ * R , the above program cannot be implemented in practice, but is only a device useful for proving consistency results. We observe that the objective function in the modified program above is the same as the original program in (19), and only the constraints on the precision matrix are different in the two programs. In the above program in (21), constraints on the entries of the precision matrix when limited to sets S R and S c M are more restrictive, while those in set S := S M \ S R are more relaxed (i.e., the ∞ constraints present in (19) are removed above), compared to the original program in (19). Recall that S M denotes the support of the Markov model, while S R ⊆ S M denotes the support of the residual or the independence model. See Figure 3. We now discuss the properties of the optimal solution J M , Σ R of the modified program in (21). Since the precision matrix entries on S c M are set to zero in (21), we have that Supp( J M ) ⊆ Supp(J * M ). Denoting Σ R as the residual covariance matrix corresponding to the modified program (21), we can similarly characterize it in the following form derived from duality: Σ R ij = 0 for (i, j) ∈ S β ij for (i, j) ∈ S R , S c M , (22) 21 Figure 3: The sets S R , S and S

c

M form a partition of {(1, ..., p) × (1, ..., p)}, where p is the number of nodes, S R is the support of the residual covariance matrix Σ * R and S M is the support of the precision matrix J * M of the Markov model and S c M is its complement. able to analyze it and prove guarantees for it in a less complicated way comparing to the original version. Let us denote the solutions of the modified program by J M , Σ R pair. In general, the optimal solutions of the two programs, original and modified one, are different. However, under conditions (A.0)–(A.5), we establish that their optimal solutions coincide. See Appendix D for details. Through this equivalence, we thus establish that the optimal solution J M , Σ R of the original program in (19) inherits all the properties of the optimal solution J M , Σ R of the modified program, i.e., the solutions of the modified program act as witness for the original program. In the following, we define the modified optimization program and its properties. The primal-dual witness method steps which guarantee the equivalence between solutions of the original and the modified program are mentioned in Appendix D. We modify the sample version of our optimization program in (19) as follows: J M := arg min J M 0 Σ n , J M − log det J M + γJ M 1,off (21) s. t. J M S c M = 0, J M S R = λ sign J * M S R . Note that since we do not a priori know the supports of the original matrices J * M and Σ * R , the above program cannot be implemented in practice, but is only a device useful for proving consistency results. We observe that the objective function in the modified program above is the same as the original program in (19), and only the constraints on the precision matrix are different in the two programs. In the above program in (21), constraints on the entries of the precision matrix when limited to sets S R and S c M are more restrictive, while those in set S := S M \ S R are more relaxed (i.e., the ∞ constraints present in (19) are removed above), compared to the original program in (19). Recall that S M denotes the support of the Markov model, while S R ⊆ S M denotes the support of the residual or the independence model. See Figure 3. We now discuss the properties of the optimal solution J M , Σ R of the modified program in (21). Since the precision matrix entries on S c M are set to zero in (21), we have that Supp( J M ) ⊆ Supp(J * M ). Denoting Σ R as the residual covariance matrix corresponding to the modified program (21), we can similarly characterize it in the following form derived from duality: Σ R ij = 0 for (i, j) ∈ S β ij for (i, j) ∈ S R , S c M , (22) where β ij are the Lagrangian multipliers corresponding to the equality constraints in the modified program (21). Define estimation errors ∆ J := J M −J * M and ∆ R := Σ R −Σ * R for the modified program in (21). It is easy to see that ∆ J S R = λ δ , ∆ J S c M = 0, ∆ R S = 0, where λ δ := λ − λ * > 0. This implies that in any of the three sets S, S R or S c M , only one of the two estimation errors ∆ J or ∆ R can be non-zero (or is at most λ δ ). This property is crucial to be able to decouple the perturbations in the Markov and the independence domains, and thereby gives bounds on the individual perturbations. It is not clear if there is an alternative partitioning of the variables (here the partition is S, S R and S c M ) which allows us to decouple the estimation errors for J M and Σ R . Through this decoupling, we are able to provide bounds on estimation errors ∆ J and ∆ R and thus, Theorem 5 is established.

Experiments

In this section, we provide synthetic and real experimental results for the proposed algorithm . We term our proposed optimization program as 1 + ∞ method and compare it with the well-known 1 method which is a special case of the proposed algorithm when λ = ∞. The primal optimization program (19) is implemented via the ADMM (Alternating Direction Method of Multipliers) technique proposed in Mohan (2013). We also compare the performance of belief propagation on the proposed model.

Synthetic Data

We build a Markov + residual synthetic model in the following way. We choose 0.2 fraction of Markov edges randomly to introduce residual edges. The underlying graph for the Markov part is a q × q 2-D grid structure (4-nearest neighbor grid). Therefore, the number of nodes is p = q 2 . Because of assumption (A.2), we randomly set 0.2 fraction of nonzero Markov off-diagonal entries to {−0.2, 0.2}, and the rest of nonzero off-diagonal entries in J * M (corresponding to the grid edges) are randomly chosen from set ±[0.15, 0.2] , i.e., J * M ij ∈ [−0.2, −0.15] ∪ [0.15, 0.2] , for all (i, j) ∈ E M . Note that 0.2 fraction of edges take the maximum absolute value which is needed by assumption (A.2). Then we ensure that J * M is positive definite by adding some uniform diagonal weighting. The nonzero entries of Σ * R are chosen from ±[0.15, 0.2] such that the sign of residual entry is the same as the sign of overlapping Markov entry (assumption (A.3)). We also generate a random mean in the interval [0] [1] for each variable. Note that this generated synthetic model does not necessarily satisfy mutual incoherence conditions (A.4) and (A.5); But we observe in the following that our method has good numerical estimation performance even when the incoherence conditions are not fully satisfied. Before we provide experiment results, it is worth mentioning that the realization of above model is an example that both Markov and residual matrices J * M and Σ * R are sparse, while the overall covariance matrix Σ * = J * M −1 − Σ * R and concentration matrix J * = Σ * −1 are both dense matrices. Table 1: Regularization parameters used for grid-structured Markov graph simulations in Figure 4 . Note that γ = c γ log p/n.

Effect of graph size p

We apply our method ( 1 + ∞ method) to random realizations of the above described model Σ * = J * M −1 − Σ * R with different sizes p ∈ {25, 64, 100, 400, 900}. Normalized Dist J M , J * M , the edit distance between the estimated and exact Markov components J M and J * M , and normalized Dist Σ R , Σ * R , the edit distance between the estimated and exact residual com- ponents Σ R and Σ * R as a function of number of samples are plotted in Figure 4 for different sizes p. In Figure 4 .a, normalized Dist J M , J * M is plotted and in Figure 4 .b, the same is plotted with rescaled horizontal axis n/ log p. We observe that by increasing the number of samples , the edit distance decreases, and by increasing the size of problem, it becomes harder to recover the components which are intuitive. More importantly, we observe in the rescaled graph that the plots for different sizes p make a lineup which is consistent with the theoretical results saying that 8 n = O(d 2 log p) is sufficient for correct recovery. Similarly, in Figure 4.c, normalized 9 Dist Σ R , Σ * R is plotted and in Figure 4.d, the same is plotted with rescaled horizontal axis n/ log p. We similarly have the initial observations that by increasing the number of samples, the edit distance decreases, and by increasing the size of problem, it becomes harder to recover the components. The theoretical sample complexity n = O(d 2 log p) is also validated in Figure 4.d. The value of regularization parameters used for this simulation are provided in Table 1. Since in the synthetic experiments, we know the value of λ * := J * M ∞,off , parameter λ is set to λ * = 0.2. It is observed that the recovery of sparsity pattern of the Markov component J * M is fairly robust to the choice of this parameter. For choosing parameter γ, the experiment is run for several values of γ to see which one gives the best recovery result. The effect of parameter γ is discussed in detail in the next subsection.

Effect of regularization parameter γ

We apply our method ( 1 + ∞ method) to random realizations of the above described grid-structured synthetic model Σ * = J * M −1 − Σ * R with fixed size p = 64. Here, we fix the 8. Note that in the grid graph, d = 4 is fixed for different sizes p. 9. The normalized distance for recovering residual component is greater than 1 for small n. Since we normalize the distance with the number of edges in the exact model, this may happen. Janzamin and Anandkumar Janzamin and Anandkumar 0 500 1000 1500 2000 2500 3000 0 0.2 0.4 0.6 0.8 1 p = 25 p = 64 p = 100 p = 400 p = 900

n Grid graph

Norm. Dist ( J M , J * M ) (a) 0 200 400 600 800 0 0.2 0.4 0.6 0.8 1 p = 25 p = 64 p = 100 p = 400 p = 900 n/ log p Grid graph Norm. Dist ( J M , J * M ) (b) 0 500 1000 1500 2000 2500 3000 0 0.5 1 p = 25 p = 64 p = 100 p = 400 p = 900 n Grid graph Norm. Dist ( Σ R , Σ * R ) (c) 0 200 400 600 800 0 0.5 1 p = 25 p = 64 p = 100 p = 400 p = 900 n/ log p Grid graph Norm. Dist ( Σ R , Σ * R ) (d) Figure 4: Simulation results for grid-structured Markov graph with different size p. (a-b) Normalized edit distance between the estimated Markov component J M and the exact Markov component J * M . In panel (b), the horizontal axis is rescaled as n/ log p. (c-d) Normalized edit distance between the estimated residual component Σ R and the exact residual component Σ * R . In panel (d), the horizontal axis is rescaled as n/ log p. Each point in the figures is derived from averaging 10 trials. regularization parameter 10 λ = 0.2 and change the regularization parameter γ = c γ log p/n where c γ ∈ {1, 1.3, 2.08, 2.5, 3}. The edit distance between the estimated and exact Markov components J M and J * M , and the edit distance between the estimated and exact residual components Σ R and Σ * R are plotted in Figure 5. We observe the pattern that for c γ less than some optimal value c * γ , the Markov component is not recovered, and for values greater than the optimal value, the components are recovered with different statistical efficiency, where by increasing c γ , the statistical rate of Markov component recovery becomes worse. For the simulations of previous subsection provided in Figure 4, we choose some regularization parameter close to c * γ . For example, we choose c γ = 2.08 for p = 64 as suggested by Figure 5. 7.1.3 Comparing 1 + ∞ and 1

methods

We apply 1 + ∞ and 1 methods to a random realization of the above described gridstructured synthetic model 11 Σ * = J * M −1 − Σ * R with size p = 64. The edit distance between the estimated and exact Markov components J M and J * M is plotted in Figure 6.a. We observe that the behaviour of 1 + ∞ method is very close to 1 method which suggests that 10. λ is set to the maximum absolute value of off-diagonal entries of Markov matrix J * M . 11. Here, we choose the nonzero off-diagonal entries of J * M randomly from {−0.2, 0.2}. 24 (a) Janzamin and Anandkumar 0 500 1000 1500 2000 2500 3000 0 0.2 0.4 0.6 0.8 1 p = 25 p = 64 p = 100 p = 400 p = 900

n Grid graph

Norm. Dist ( J M , J * M ) (a) 0 200 400 600 800 0 0.2 0.4 0.6 0.8 1 p = 25 p = 64 p = 100 p = 400 p = 900 n/ log p Grid graph Norm. Dist ( J M , J * M ) (b) 0 500 1000 1500 2000 2500 3000 0 0.5 1 p = 25 p = 64 p = 100 p = 400 p = 900 n Grid graph Norm. Dist ( Σ R , Σ * R ) (c) 0 200 400 600 800 0 0.5 1 p = 25 p = 64 p = 100 p = 400 p = 900 n/ log p Grid graph Norm. Dist ( Σ R , Σ * R ) (d) Figure 4: Simulation results for grid-structured Markov graph with different size p. (a-b) Normalized edit distance between the estimated Markov component J M and the exact Markov component J * M . In panel (b), the horizontal axis is rescaled as n/ log p. (c-d) Normalized edit distance between the estimated residual component Σ R and the exact residual component Σ * R . In panel (d), the horizontal axis is rescaled as n/ log p. Each point in the figures is derived from averaging 10 trials. regularization parameter 10 λ = 0.2 and change the regularization parameter γ = c γ log p/n where c γ ∈ {1, 1.3, 2.08, 2.5, 3}. The edit distance between the estimated and exact Markov components J M and J * M , and the edit distance between the estimated and exact residual components Σ R and Σ * R are plotted in Figure 5. We observe the pattern that for c γ less than some optimal value c * γ , the Markov component is not recovered, and for values greater than the optimal value, the components are recovered with different statistical efficiency, where by increasing c γ , the statistical rate of Markov component recovery becomes worse. For the simulations of previous subsection provided in Figure 4, we choose some regularization parameter close to c * γ . For example, we choose c γ = 2.08 for p = 64 as suggested by Figure 5. 7.1.3 Comparing 1 + ∞ and 1

methods

We apply 1 + ∞ and 1 methods to a random realization of the above described gridstructured synthetic model 11 Σ * = J * M −1 − Σ * R with size p = 64. The edit distance between the estimated and exact Markov components J M and J * M is plotted in Figure 6.a. We observe that the behaviour of 1 + ∞ method is very close to 1 method which suggests that 10. λ is set to the maximum absolute value of off-diagonal entries of Markov matrix J * M . 11. Here, we choose the nonzero off-diagonal entries of J * M randomly from {−0.2, 0.2}. 24 (b) Janzamin and Anandkumar 0 500 1000 1500 2000 2500 3000 0 0.2 0.4 0.6 0.8 1 p = 25 p = 64 p = 100 p = 400 p = 900

n Grid graph

Norm. Dist ( J M , J * M ) (a) 0 200 400 600 800 0 0.2 0.4 0.6 0.8 1 p = 25 p = 64 p = 100 p = 400 p = 900 n/ log p Grid graph Norm. Dist ( J M , J * M ) (b) 0 500 1000 1500 2000 2500 3000 0 0.5 1 p = 25 p = 64 p = 100 p = 400 p = 900 n Grid graph Norm. Dist ( Σ R , Σ * R ) (c) 0 200 400 600 800 0 0.5 1 p = 25 p = 64 p = 100 p = 400 p = 900 n/ log p Grid graph Norm. Dist ( Σ R , Σ * R ) (d) Figure 4: Simulation results for grid-structured Markov graph with different size p. (a-b) Normalized edit distance between the estimated Markov component J M and the exact Markov component J * M . In panel (b), the horizontal axis is rescaled as n/ log p. (c-d) Normalized edit distance between the estimated residual component Σ R and the exact residual component Σ * R . In panel (d), the horizontal axis is rescaled as n/ log p. Each point in the figures is derived from averaging 10 trials. regularization parameter 10 λ = 0.2 and change the regularization parameter γ = c γ log p/n where c γ ∈ {1, 1.3, 2.08, 2.5, 3}. The edit distance between the estimated and exact Markov components J M and J * M , and the edit distance between the estimated and exact residual components Σ R and Σ * R are plotted in Figure 5. We observe the pattern that for c γ less than some optimal value c * γ , the Markov component is not recovered, and for values greater than the optimal value, the components are recovered with different statistical efficiency, where by increasing c γ , the statistical rate of Markov component recovery becomes worse. For the simulations of previous subsection provided in Figure 4, we choose some regularization parameter close to c * γ . For example, we choose c γ = 2.08 for p = 64 as suggested by Figure 5. 7.1.3 Comparing 1 + ∞ and 1

methods

We apply 1 + ∞ and 1 methods to a random realization of the above described gridstructured synthetic model 11 Σ * = J * M −1 − Σ * R with size p = 64. The edit distance between the estimated and exact Markov components J M and J * M is plotted in Figure 6.a. We observe that the behaviour of 1 + ∞ method is very close to 1 method which suggests that 10. λ is set to the maximum absolute value of off-diagonal entries of Markov matrix J * M . 11. Here, we choose the nonzero off-diagonal entries of J * M randomly from {−0.2, 0.2}. 24 (c) Janzamin and Anandkumar 0 500 1000 1500 2000 2500 3000 0 0.2 0.4 0.6 0.8 1 p = 25 p = 64 p = 100 p = 400 p = 900

n Grid graph

Norm. Dist ( J M , J * M ) (a) 0 200 400 600 800 0 0.2 0.4 0.6 0.8 1 p = 25 p = 64 p = 100 p = 400 p = 900 n/ log p Grid graph Norm. Dist ( J M , J * M ) (b) 0 500 1000 1500 2000 2500 3000 0 0.5 1 p = 25 p = 64 p = 100 p = 400 p = 900 n Grid graph Norm. Dist ( Σ R , Σ * R ) (c) 0 200 400 600 800 0 0.5 1 p = 25 p = 64 p = 100 p = 400 p = 900 n/ log p Grid graph Norm. Dist ( Σ R , Σ * R ) (d) Figure 4: Simulation results for grid-structured Markov graph with different size p. (a-b) Normalized edit distance between the estimated Markov component J M and the exact Markov component J * M . In panel (b), the horizontal axis is rescaled as n/ log p. (c-d) Normalized edit distance between the estimated residual component Σ R and the exact residual component Σ * R . In panel (d), the horizontal axis is rescaled as n/ log p. Each point in the figures is derived from averaging 10 trials. regularization parameter 10 λ = 0.2 and change the regularization parameter γ = c γ log p/n where c γ ∈ {1, 1.3, 2.08, 2.5, 3}. The edit distance between the estimated and exact Markov components J M and J * M , and the edit distance between the estimated and exact residual components Σ R and Σ * R are plotted in Figure 5. We observe the pattern that for c γ less than some optimal value c * γ , the Markov component is not recovered, and for values greater than the optimal value, the components are recovered with different statistical efficiency, where by increasing c γ , the statistical rate of Markov component recovery becomes worse. For the simulations of previous subsection provided in Figure 4, we choose some regularization parameter close to c * γ . For example, we choose c γ = 2.08 for p = 64 as suggested by Figure 5. 7.1.3 Comparing 1 + ∞ and 1

methods

We apply 1 + ∞ and 1 methods to a random realization of the above described gridstructured synthetic model 11 Σ * = J * M −1 − Σ * R with size p = 64. The edit distance between the estimated and exact Markov components J M and J * M is plotted in Figure 6.a. We observe that the behaviour of 1 + ∞ method is very close to 1 method which suggests that 10. λ is set to the maximum absolute value of off-diagonal entries of Markov matrix J * M . 11. Here, we choose the nonzero off-diagonal entries of J * M randomly from {−0.2, 0.2}.

24

(d) Figure 4: Simulation results for grid-structured Markov graph with different size p. (a-b) Normalized edit distance between the estimated Markov component J M and the exact Markov component J * M . In panel (b), the horizontal axis is rescaled as n/ log p. (c-d) Normalized edit distance between the estimated residual component Σ R and the exact residual component Σ * R . In panel (d), the horizontal axis is rescaled as n/ log p. Each point in the figures is derived from averaging 10 trials. regularization parameter 10 λ = 0.2 and change the regularization parameter γ = c γ log p/n where c γ ∈ {1, 1.3, 2.08, 2.5, 3}. The edit distance between the estimated and exact Markov components J M and J * M , and the edit distance between the estimated and exact residual components Σ R and Σ * R are plotted in Figure 5. We observe the pattern that for c γ less than some optimal value c * γ , the Markov component is not recovered, and for values greater than the optimal value, the components are recovered with different statistical efficiency, where by increasing c γ , the statistical rate of Markov component recovery becomes worse. For the simulations of previous subsection provided in Figure 4, we choose some regularization parameter close to c * γ . For example, we choose c γ = 2.08 for p = 64 as suggested by Figure 5. 7.1.3 Comparing 1 + ∞ and 1

methods

We apply 1 + ∞ and 1 methods to a random realization of the above described gridstructured synthetic model 11 Σ * = J * M −1 − Σ * R with size p = 64. The edit distance between the estimated and exact Markov components J M and J * M is plotted in Figure 6.a. We observe that the behaviour of 1 + ∞ method is very close to 1 method which suggests that 10. λ is set to the maximum absolute value of off-diagonal entries of Markov matrix J * M . 11. Here, we choose the nonzero off-diagonal entries of J * M randomly from {−0.2, 0.2}. High-Dimensional Covariance Decomposition High-Dimensional Covariance Decomposition 0 500 1000 1500 2000 2500 3000 0 50 100 150 cγ = 1 cγ = 1.3 cγ = 2.08 cγ = 2.5 cγ = 3 n Dist ( J M , J * M ) (a) 0 500 1000 1500 2000 2500 3000 0 5 10 15 20 25 cγ = 1 cγ = 1.3 cγ = 2.08 cγ = 2.5 cγ = 3 n Dist ( Σ R , Σ * R ) (b) Figure 5: Simulation results for grid graph with fixed size p = 64 and regularization parameters λ = 0.2 and varying c γ ∈ {1, 1.3, 2.08, 2.5, 3} where γ = c γ log p/n. (a) Edit distance between the estimated Markov component J M and the exact Markov component J * M . (b) Edit distance between the estimated residual component Σ R and the exact residual component Σ * R . Each point in the figures is derived from averaging 10 trials. sparsity pattern of J * M can be estimated efficiently under either methods. The edit distance between the estimated and exact residual components Σ R and Σ * R is plotted in Figure 6.b. Since there is not any off-diagonal ∞ constraints in 1 method, it can not recover the residual matrix Σ * R . Finally the ∞ -elementwise norm of error between the estimated precision matrix J and the exact precision matrix J * is sketched for both methods in Figure 6.c. We observe the advantage of proposed 1 + ∞ method in estimating the overall model precision matrix J * = Σ * −1 . Note that the same regularization parameters provided in Table 1 are used for the simulations of this subsection, except for 1 method that we have λ = ∞. 7.1.4 Benefit of applying LBP (Loopy Belief Propagation) to the proposed model We compare the result of applying LBP to J * and J * M components of a random realization of the above described grid-structured synthetic model. 12 The log of average mean and variance errors over all nodes are sketched in Figure 7 throughout the iterations. We observe that LBP does not converge for J * model. It is shown in Malioutov et al. (2006) that if a model is walk-summable, then the mean estimates under LBP converge and are correct. The spectral norms of the partial correlation matrices are |||R M ||| = 0.8613 and |||R||| = 3.2446 for J * M and J * models respectively. Thus, the matrix J * is not walk-summable and therefore its convergence under LBP is not guaranteed and this is seen in Figure 7. On the other hand, LBP is accurate for J * M matrix. Thus, our method learns models which are better suited for inference under loopy belief propagation.

Real Data

The proposed algorithm is also applied to foreign exchange rate and monthly stock returns data sets to learn a Markov plus residual model introduced in the paper. It is important to 12. Here, we choose 0.5 fraction of Markov edges randomly to introduce residual edges.

(a)

High-Dimensional Covariance Decomposition 0 500 1000 1500 2000 2500 3000 0 50 100 150 cγ = 1 cγ = 1.3 cγ = 2.08 cγ = 2.5 cγ = 3 n Dist ( J M , J * M ) (a) 0 500 1000 1500 2000 2500 3000 0 5 10 15 20 25 cγ = 1 cγ = 1.3 cγ = 2.08 cγ = 2.5 cγ = 3 n Dist ( Σ R , Σ * R ) (b) Figure 5: Simulation results for grid graph with fixed size p = 64 and regularization parameters λ = 0.2 and varying c γ ∈ {1, 1.3, 2.08, 2.5, 3} where γ = c γ log p/n. (a) Edit distance between the estimated Markov component J M and the exact Markov component J * M . (b) Edit distance between the estimated residual component Σ R and the exact residual component Σ * R . Each point in the figures is derived from averaging 10 trials. sparsity pattern of J * M can be estimated efficiently under either methods. The edit distance between the estimated and exact residual components Σ R and Σ * R is plotted in Figure 6.b. Since there is not any off-diagonal ∞ constraints in 1 method, it can not recover the residual matrix Σ * R . Finally the ∞ -elementwise norm of error between the estimated precision matrix J and the exact precision matrix J * is sketched for both methods in Figure 6.c. We observe the advantage of proposed 1 + ∞ method in estimating the overall model precision matrix J * = Σ * −1 . Note that the same regularization parameters provided in Table 1 are used for the simulations of this subsection, except for 1 method that we have λ = ∞. 7.1.4 Benefit of applying LBP (Loopy Belief Propagation) to the proposed model We compare the result of applying LBP to J * and J * M components of a random realization of the above described grid-structured synthetic model. 12 The log of average mean and variance errors over all nodes are sketched in Figure 7 throughout the iterations. We observe that LBP does not converge for J * model. It is shown in Malioutov et al. (2006) that if a model is walk-summable, then the mean estimates under LBP converge and are correct. The spectral norms of the partial correlation matrices are |||R M ||| = 0.8613 and |||R||| = 3.2446 for J * M and J * models respectively. Thus, the matrix J * is not walk-summable and therefore its convergence under LBP is not guaranteed and this is seen in Figure 7. On the other hand, LBP is accurate for J * M matrix. Thus, our method learns models which are better suited for inference under loopy belief propagation.

Real Data

The proposed algorithm is also applied to foreign exchange rate and monthly stock returns data sets to learn a Markov plus residual model introduced in the paper. It is important to 12. Here, we choose 0.5 fraction of Markov edges randomly to introduce residual edges.

(b)

Figure 5: Simulation results for grid graph with fixed size p = 64 and regularization parameters λ = 0.2 and varying c γ ∈ {1, 1.3, 2.08, 2.5, 3} where γ = c γ log p/n. (a) Edit distance between the estimated Markov component J M and the exact Markov component J * M . (b) Edit distance between the estimated residual component Σ R and the exact residual component Σ * R . Each point in the figures is derived from averaging 10 trials. sparsity pattern of J * M can be estimated efficiently under either methods. The edit distance between the estimated and exact residual components Σ R and Σ * R is plotted in Figure 6.b. Since there is not any off-diagonal ∞ constraints in 1 method, it can not recover the residual matrix Σ * R . Finally the ∞ -elementwise norm of error between the estimated precision matrix J and the exact precision matrix J * is sketched for both methods in Figure 6.c. We observe the advantage of proposed 1 + ∞ method in estimating the overall model precision matrix J * = Σ * −1 . Note that the same regularization parameters provided in Table 1 are used for the simulations of this subsection, except for 1 method that we have λ = ∞. 7.1.4 Benefit of applying LBP (Loopy Belief Propagation) to the proposed model We compare the result of applying LBP to J * and J * M components of a random realization of the above described grid-structured synthetic model. 12 The log of average mean and variance errors over all nodes are sketched in Figure 7 throughout the iterations. We observe that LBP does not converge for J * model. It is shown in Malioutov et al. (2006) that if a model is walk-summable, then the mean estimates under LBP converge and are correct. The spectral norms of the partial correlation matrices are |||R M ||| = 0.8613 and |||R||| = 3.2446 for J * M and J * models respectively. Thus, the matrix J * is not walk-summable and therefore its convergence under LBP is not guaranteed and this is seen in Figure 7. On the other hand, LBP is accurate for J * M matrix. Thus, our method learns models which are better suited for inference under loopy belief propagation.

Real Data

The proposed algorithm is also applied to foreign exchange rate and monthly stock returns data sets to learn a Markov plus residual model introduced in the paper. It is important to 12. Here, we choose 0.5 fraction of Markov edges randomly to introduce residual edges. Janzamin and Anandkumar Janzamin and Anandkumar 0 500 1000 1500 2000 2500 3000 0 20 40 60 80 100 120 1 + ∞ method 1 method n Dist ( J M , J * M ) (a) 0 500 1000 1500 2000 2500 3000 0 5 10 15 20 25 1 + ∞ method 1 method n Dist ( Σ R , Σ * R ) (b) 0 500 1000 1500 2000 2500 3000 0.2 0.4 0.6 0.8 1 1 + ∞ method 1 method n J * − J∞ (c) Figure 6: Simulation results for grid graph with size p = 64. (a) Edit distance between the estimated Markov component J M and the exact Markov component J * M . (b) Edit distance between the estimated residual component Σ R and the exact residual component Σ * R . (c) Precision matrix estimation error J * − J ∞ , where J = J M for 1 method and J = J −1 M − Σ R −1 for 1 + ∞ method. 0 5 10 15 −10 0 10 20 LBP applied to J * model LBP applied to J * M model iteration average mean error (a) 0 5 10 15 −2 −1 0 1 2 LBP applied to J * model LBP applied to J * M model iteration average variance error (b) Figure 7: Performance under loopy belief propagation for the overall model (J * ) and the Markov component (J * M ). note that the real data sets can be modeled by different models not necessarily satisfying the conditions proposed in this paper. But, here we observe that the resulting Markov plus residual models are fairly interpretable for the corresponding real data sets. The interpretations are discussed in detail in the following sections. 26 (a) Janzamin and Anandkumar 0 500 1000 1500 2000 2500 3000 0 20 40 60 80 100 120 1 + ∞ method 1 method n Dist ( J M , J * M ) (a) 0 500 1000 1500 2000 2500 3000 0 5 10 15 20 25 1 + ∞ method 1 method n Dist ( Σ R , Σ * R ) (b) 0 500 1000 1500 2000 2500 3000 0.2 0.4 0.6 0.8 1 1 + ∞ method 1 method n J * − J∞ (c) Figure 6: Simulation results for grid graph with size p = 64. (a) Edit distance between the estimated Markov component J M and the exact Markov component J * M . (b) Edit distance between the estimated residual component Σ R and the exact residual component Σ * R . (c) Precision matrix estimation error J * − J ∞ , where J = J M for 1 method and J = J −1 M − Σ R −1 for 1 + ∞ method. 0 5 10 15 −10 0 10 20 LBP applied to J * model LBP applied to J * M model iteration average mean error (a) 0 5 10 15 −2 −1 0 1 2 LBP applied to J * model LBP applied to J * M model iteration average variance error (b) Figure 7: Performance under loopy belief propagation for the overall model (J * ) and the Markov component (J * M ). note that the real data sets can be modeled by different models not necessarily satisfying the conditions proposed in this paper. But, here we observe that the resulting Markov plus residual models are fairly interpretable for the corresponding real data sets. The interpretations are discussed in detail in the following sections. 26 (b) Janzamin and Anandkumar 0 500 1000 1500 2000 2500 3000 0 20 40 60 80 100 120 1 + ∞ method 1 method n Dist ( J M , J * M ) (a) 0 500 1000 1500 2000 2500 3000 0 5 10 15 20 25 1 + ∞ method 1 method n Dist ( Σ R , Σ * R ) (b) 0 500 1000 1500 2000 2500 3000 0.2 0.4 0.6 0.8 1 1 + ∞ method 1 method n J * − J ∞ (c) Figure 6: Simulation results for grid graph with size p = 64. (a) Edit distance between the estimated Markov component J M and the exact Markov component J * M . (b) Edit distance between the estimated residual component Σ R and the exact residual component Σ * R . (c) Precision matrix estimation error J * − J ∞ , where J = J M for 1 method and J = J −1 M − Σ R −1 for 1 + ∞ method. 0 5 10 15 −10 0 10 20 LBP applied to J * model LBP applied to J * M model iteration average mean error (a) 0 5 10 15 −2 −1 0 1 2 LBP applied to J * model LBP applied to J * M model iteration average variance error (b) Figure 7: Performance under loopy belief propagation for the overall model (J * ) and the Markov component (J * M ). note that the real data sets can be modeled by different models not necessarily satisfying the conditions proposed in this paper. But, here we observe that the resulting Markov plus residual models are fairly interpretable for the corresponding real data sets. The interpretations are discussed in detail in the following sections. 26 (c) Figure 6: Simulation results for grid graph with size p = 64. (a) Edit distance between the estimated Markov component J M and the exact Markov component J * M . (b) Edit distance between the estimated residual component Σ R and the exact residual component Σ * R . (c) Precision matrix estimation error J * − J ∞ , where J = J M for 1 method and J = J −1 M − Σ R −1 for 1 + ∞ method. Janzamin and Anandkumar 0 500 1000 1500 2000 2500 3000 0 20 40 60 80 100 120 1 + ∞ method 1 method n Dist ( J M , J * M ) (a) 0 500 1000 1500 2000 2500 3000 0 5 10 15 20 25 1 + ∞ method 1 method n Dist ( Σ R , Σ * R ) (b) 0 500 1000 1500 2000 2500 3000 0.2 0.4 0.6 0.8 1 1 + ∞ method 1 method n J * − J∞ (c) Figure 6: Simulation results for grid graph with size p = 64. (a) Edit distance between the estimated Markov component J M and the exact Markov component J * M . (b) Edit distance between the estimated residual component Σ R and the exact residual component Σ * R . (c) Precision matrix estimation error J * − J ∞ , where J = J M for 1 method and J = J −1 M − Σ R −1 for 1 + ∞ method. 0 5 10 15 −10 0 10 20 LBP applied to J * model LBP applied to J * M model iteration average mean error (a) 0 5 10 15 −2 −1 0 1 2 LBP applied to J * model LBP applied to J * M model iteration average variance error (b) Figure 7: Performance under loopy belief propagation for the overall model (J * ) and the Markov component (J * M ). note that the real data sets can be modeled by different models not necessarily satisfying the conditions proposed in this paper. But, here we observe that the resulting Markov plus residual models are fairly interpretable for the corresponding real data sets. The interpretations are discussed in detail in the following sections. 26 (a) Janzamin and Anandkumar 0 500 1000 1500 2000 2500 3000 0 20 40 60 80 100 120 1 + ∞ method 1 method n Dist ( J M , J * M ) (a) 0 500 1000 1500 2000 2500 3000 0 5 10 15 20 25 1 + ∞ method 1 method n Dist ( Σ R , Σ * R ) (b) 0 500 1000 1500 2000 2500 3000 0.2 0.4 0.6 0.8 1 1 + ∞ method 1 method n J * − J ∞ (c) Figure 6: Simulation results for grid graph with size p = 64. (a) Edit distance between the estimated Markov component J M and the exact Markov component J * M . (b) Edit distance between the estimated residual component Σ R and the exact residual component Σ * R . (c) Precision matrix estimation error J * − J ∞ , where J = J M for 1 method and J = J −1 M − Σ R −1 for 1 + ∞ method. 0 5 10 15 −10 0 10 20 LBP applied to J * model LBP applied to J * M model iteration average mean error (a) 0 5 10 15 −2 −1 0 1 2 LBP applied to J * model LBP applied to J * M model iteration average variance error (b) Figure 7: Performance under loopy belief propagation for the overall model (J * ) and the Markov component (J * M ). note that the real data sets can be modeled by different models not necessarily satisfying the conditions proposed in this paper. But, here we observe that the resulting Markov plus residual models are fairly interpretable for the corresponding real data sets. The interpretations are discussed in detail in the following sections. 26 (b) Figure 7: Performance under loopy belief propagation for the overall model (J * ) and the Markov component (J * M ). note that the real data sets can be modeled by different models not necessarily satisfying the conditions proposed in this paper. But, here we observe that the resulting Markov plus residual models are fairly interpretable for the corresponding real data sets. The interpretations are discussed in detail in the following sections. High-Dimensional Covariance Decomposition 7.2.1 Foreign exchange rate data In this section, we apply the proposed algorithm to the foreign exchange rate data set. 13 The data set includes monthly exchange rates of 19 countries currency with respect to US dollars from October 1983 to January 2012. Thus, the data set has 340 samples of 19 variables. We apply the optimization program (7) with a slight modification. Since the underlying model for this data set does not necessarily satisfy the proposed eigenvalue condition (A.6), we need to make sure that the overall covariance matrix estimation Σ is positive definite and thus a valid covariance matrix. We add an additional constraint to the optimization program (7), imposing a lower bound on the minimum eigenvalue of overall covariance matrix λ min (Σ), i.e., λ min (Σ) ≥ σ min . The parameter σ min is set to 0.001 in this experiment. The resulting edges of Markov and residual matrices for some moderate choice of regularization parameters γ = 20 and λ = 0.004 are plotted in Figure 8. The choice of regularization parameters are further discussed at the end of this subsection. We observe sparsity on both Markov and residual structures. There are two main observations in the learned model in Figure 8. First, it is seen that the statistical dependencies of foreign exchange rates are correlated with the geographical locations of countries, e.g., it is observed in the learned model that the exchange rates of Asian countries are more correlated. We can refer to Asian countries " South Korea " , " Japan " , " China " , " Sri Lanka " , " Taiwan " , " Thailand " and " India " in the Markov model where several edges exist between them while other nodes in the graph have much lower degrees. We observe similar patterns in the residual matrix, e.g., there is an edge between " India " and " Sri Lanka " in the residual model. We also see the interesting phenomena in the Markov graph that there exist some high degree nodes such as " South Korea " and " Japan " . The presence of high degree nodes suggests that incorporating hidden variables can further lead to sparser representations, and this has been observed before in other works, e.g., Choi et al. (2010); Chandrasekaran et al. (2010a); Choi et al. (2011). The regularization parameters are chosen such that the resulting Markov and residual graphs are reasonably sparse, while still being informative. Increasing the parameter γ makes both Markov and residual components sparser, and increasing parameter λ makes the residual component sparser. In addition, it is worth discussing the fact that we chose parameter γ relatively large compared to parameter λ in this simulation. In Theorem 4, we have γ = C 1 log p/n and λ = λ * + C 2 log p/n. Now, if C 1 is large compared to C 2 and furthermore λ * is small, γ can be larger than λ. Hence, we have an agreement between theory and practice.

Monthly stock returns data

In this section we apply the algorithm to monthly stock returns of a number of companies in the S&P 100 stock index. We pick 17 companies in divisions " E.Trans, Comm, Elec&Gas " and " G.Retail Trade " and apply the optimization program (19) to their stock returns data to learn the model. The resulting edges for Markov and residual matrices are plotted in Figure 9 for regularization parameters γ = 2.2e − 03 and λ = 1e − 04. There is sparsity on both Markov and residual structure. The isolated nodes in the Markov graph are not Janzamin and Anandkumar Switzerland Singapore Hong Kong New Zeland Malaysia Australia South Korea South Africa India Japan Denmark Sweden UK Canada Taiwan Norway Thailand Sri Lanka

China

Figure 8: Markov and independence graph structures for the foreign exchange rate data set with regularization parameters γ = 20 and λ = 0.004. Solid edges indicate Markov model and dotted edges indicate independence model. CBS NSC SNS BNI HD CMCSA TGT MCD WMT CVS FDX ETR EXC T VZ Figure 9: Markov and independence graph structures for the monthly stock returns data set with regularization parameters γ = 2.2e − 03 and λ = 1e − 04. Solid edges indicate Markov model and dotted edges indicate independence model. presented in the figure. We see in both Markov and residual graphs that there exist higher correlations among stock returns of companies in the same division or industry. There are 5 connected partitions in the residual graph. e.g., nodes " HD " , " WMT " , " TGT " and " MCD " , all belonging to division Retail Trade form a partition. This is also observed for the telecommunication industries (companies " T " and " VZ " ) and energy industries (companies " ETR " and " EXC " ). We see a similar pattern in the Markov graph but with more edges. Similar to exchange rate data set results, we also observe high degree nodes in the Markov graph such as " HD " and " TGT " which suggest incorporating hidden nodes. High-Dimensional Covariance Decomposition

Conclusion

In this paper, we provided an in-depth study of convex optimization methods and guarantees for high-dimensional covariance matrix decomposition. Our methods unify the existing results for sparse covariance/precision estimation and introduce a richer class of models with sparsity in multiple domains. We provide consistency guarantees for estimation in both the Markov and the residual domains, and establish efficient sample complexity results for our method. These findings open up many future directions to explore. One important aspect is to relax the sparsity constraints imposed in the two domains, and to develop new methods to enable decomposition of such models. Other considerations include extension to discrete models and other models for the residual covariance matrix (e.g., low rank matrices). Such findings will push the envelope of efficient models for high-dimensional estimation. It is worth mentioning while in many scenarios it is important to incorporate latent variables, in our framework it is challenging to incorporate both latent variables as well as marginal independencies, and provide learning guarantees, and we defer it to future work.

Acknowledgements

We thank Karthik Mohan for helpful discussions on running experiments. We also acknowledge useful discussions with Max Welling, Babak Hassibi and Martin Wainwright. We also thank Bin Yu and the JMLR reviewers for valuable comments that have significantly improved the manuscript. M. In this section we prove duality between programs (19) and (7) (when the positive-definiteness constraint Σ M − Σ R 0 is dropped). By doing this, the duality between programs (6) and (4) is also proved since they are special cases of (19) and (7) when γ is set to zero and Σ n is substituted with Σ * . Before we prove duality, we introduce the concept of subdifferential or subgradient for a convex function not necessarily differentiable. Subgradient (subdifferential) generalizes the gradient (derivative) concept to nondifferentiable functions. Supposing convex function f : R n → R, the subgradient at a point x 0 which is usually denoted by ∂f (x 0 ) consists of all vectors c such that f (x) ≥ f (x 0 ) + c, x − x 0 , ∀x ∈ Dom f. In order to prove duality, we start from program (7) (when the positive-definiteness constraint Σ M − Σ R 0 is dropped) and derive the primal form (19). Program (7) can be written in the following equivalent form where λ 1 goes to infinity and λ 2 is used instead of λ. Σ M , Σ R := arg max Σ M 0,Σ R log det Σ M − λ 1 Σ R 1,on − λ 2 Σ R 1,off (23) s. t. Σ n − Σ M + Σ R ∞,off ≤ γ, Σ M d − Σ R d = Σ n d . By introducing the dual variable J M for above program, we have: min J M ∞,on≤λ 1 J M ∞,off ≤λ 2 −−J M , Σ R = −λ 1 Σ R 1,on − λ 2 Σ R 1,off , where ( J M ) on ∈ λ 1 ∂ Σ R 1,on , ( J M ) off ∈ λ 2 ∂ Σ R 1,off minimizes the above program. Thus, we have the following equivalent form for program (23): min J M ∞,on≤λ 1 J M ∞,off ≤λ 2 max Σ M 0,Σ R Σ n −Σ M +Σ R ∞,off ≤γ (Σ M ) d −(Σ R ) d =( Σ n ) d log det Σ M − J M , Σ R , where the order of programs is exchanged. If we define the new variable Σ = Σ M − Σ R , and use Σ as the new variable in the program instead of Σ R , the inner max program becomes max Σ M 0,Σ Σ n −Σ ∞,off ≤γ,Σ d =( Σ n ) d log det Σ M − J M , Σ M + J M , Σ. Since the objective function and constraints are disjoint functions of variables Σ and Σ M , we can do optimization individually for two variables. The optimizers are Σ M = J −1 M and Σ = Σ n + γZ γ , where Z γ is a member of the subgradient of · 1,off evaluated at point J M , i.e., (Z γ ) ij =      0 for i = j ∈ [−1, 1] for i = j, J M ij = 0 sign J M ij for i = j, J M ij = 0. Also note that since Σ M should be positive definite, the variable J M should be also positive definite. Therefore, it adds another constraint J M 0. If we substitute these optimizers, we get the dual program min J M 0 J M ∞,on≤λ 1 J M ∞,off ≤λ 2 Σ n , J M − log det J M + γJ M 1,off , which is equivalent to (19) when λ 1 goes to infinity and therefore the result is proved. Appendix B. Characterization of the Proposed Optimization Programs We proposed programs (6) and (19) to do decomposition and estimation respectively. Former is used to decompose exact statistics to its Markov and residual covariance components and the latter is used to estimate decomposition components given sample covariance matrix . In this appendix we characterize optimal solutions of these optimization programs. Both programs are convex and therefore the optimal solutions can be characterized using standard convex optimization theory. Note that the proof of following lemmas is mentioned after the remarks.

Lemma 1

For any λ > 0, primal problem (6) has a unique solution J M 0 which is characterized by the following equation: Σ * − J −1 M + Z = 0, (24) where Z has the following form Z ij =      0 for i = j 0 for i = j, | J M ij | < λ α ij sign J M ij for i = j, | J M ij | = λ, (25) in which α ij can only take nonnegative values, i.e., we have α ij ≥ 0.

Remark 10

Comparing Lagrangian optimality condition in (24) with relation Σ * = J −1 M − Σ R between solutions of primal-dual optimization programs (derived in Appendix A) implies the equality Σ R = Z. Thus, Σ R entries are determined by Lagrangian multipliers of primal program. More specifically, we have ( Σ R ) ij =      0 for i = j 0 for i = j, | J M ij | < λ α ij sign J M ij for i = j, | J M ij | = λ, (26) where α ij ≥ 0 are the Lagrangian multipliers of primal program (6).

Lemma 2

For any λ > 0, γ ≥ 0 and sample covariance matrix Σ n with strictly positive diagonal entries, primal problem (19) has a unique solution J M 0 which is characterized by the equation Σ n − J −1 M + Z = 0, (27) where Z = Z α + γ Z γ . Matrix Z γ ∈ ∂ J M 1,off and Z α is represented as in (25) for some Lagrangian multipliers α ij ≥ 0.

Remark 11

Comparing Lagrangian optimality condition in (27) with relation Σ n = J −1 M − Σ R − γ Z γ between solutions of primal-dual optimization programs (derived in Appendix A) implies the equality Σ R = Z α . Thus, Σ R entries are determined by the Lagrangian multipliers of primal program. More specifically, we have ( Σ R ) ij =      0 for i = j 0 for i = j, | J M ij | < λ α ij sign J M ij for i = j, | J M ij | = λ, (28) where α ij ≥ 0 are the Lagrangian multipliers of primal program (19).

Proof

We prove Lemma 2 here and Lemma 1 is a special case of that when γ is set to zero and Σ n is substituted with Σ * . For any λ > 0 and γ ≥ 0, the optimization problem (19) is a convex programming where the objective function is strictly convex. Therefore, if the minimum is achieved it is unique. Since off-diagonal entries of J M are bounded according to constraints, the only issue for minimum achievement may arises for unbounded diagonal entries. It is shown in Ravikumar et al. (2011) that if diagonal entries of Σ n are strictly positive, the function is coercive with respect to diagonal entries and therefore here is no issue regarding unbounded diagonal entries. Thus, the minimum is attained in J M 0. But since when J M approaches the boundary of positive definite cone, the objective function goes to infinity, the solution is attained in the interior of the cone J M 0. After showing that the unique minimum is achieved, let us characterize the minimum. Considering α ij as Lagrangian multipliers of inequality constraints of program (19), the Lagrangian function is L(J M , α) = Σ n , J M − log det J M + γJ M 1,off + i =j α ij J M ij − λ . We skipped positive definiteness constraint in writing Lagrangian function since it is inactive . Based on standard convex optimization theory, the matrix J M 0 is the optimal solution if and only if it satisfies KKT conditions. It should minimize the Lagrangian which happens if and only if 0 belongs to the subdifferential of Lagrangian or equivalently there exists a matrix Z such that Σ n − J −1 M + Z = 0, where Z = Z α + γ Z γ . Matrix Z γ ∈ ∂ J M 1,off and Z α is ( Z α ) ij =      0 for i = j ∈ α ij .[−1, 1] for i = j, J M ij = 0 α ij sign J M ) ij for i = j, J M ij = 0, for some Lagrangian multipliers α ij ≥ 0. The solution should also satisfy complementary slackness conditions α ij . J M ij − λ = 0 for i = j. Applying this condition to above Z α representation, results to (25) form proposed in the lemma. Appendix C. Proof of Theorem 4 First note that as mentioned in Remark 3, the pair J M , Σ R given by optimization program gives a decomposition Σ * = J −1 M − Σ R which is desired. Next, in order to prove the equivalence, we show that there is a one to one correspondence between the specified conditions (A.0)-(A.3) for valid decomposition and the characterization of optimal solution of optimization program given in lemma 1. We go through each of these conditions one by one in the following lines. Condition (A.0) is considered in optimization program as positive definiteness of Markov matrix J M . Condition (A.1) is exactly the primal constraint J * M ∞,off ≤ λ. Condition (A.2) is exactly the same as relation (26) where diagonal entries of residual covariance matrix are zero and its off-diagonal entries can be nonzero only if the absolute value of corresponding entry in Markov matrix takes the maximum value λ. Condition (A.3) is exactly the same as inequality α ij ≥ 0. In the above lines, we covered one by one correspondence for conditions (A.0)-(A.3). But note that we also covered all the equalities and inequalities that characterize unique optimal solution of optimization program. In other words by above correspondence we proved that both of the following derivations are true where second one is the reverse of first one. On one hand, any optimal solution of optimization program gives a valid decomposition under desired conditions. On the other hand, any valid decomposition under desired conditions is a solution of proposed optimization program. Thus, we can infer that these two are exactly equivalent and the result is proved. Since the solution of optimization program is unique and according to the equivalence between this solution and decomposition under those conditions, uniqueness is also established. Appendix D. Proof of Theorem 5 In this appendix, we first mention an outline of the primal-dual witness method and then provide the detailed proof of the theorem. First, continuing the proof outline presented in section 6, we provide an outline of the primaldual witness method steps in order to establish equivalence between optimal solutions of the original (19) and the modified (21) optimization programs. 1. The primal witness matrix J M is defined as in (21).

2

. The dual witness matrix is set as Z = − Σ n + J −1 M . It is defined in this way to satisfy original program optimal solution characterization mentioned in appendix B. 3. We need to check the following feasibility conditions under which the modified program solution is equivalent to the solution of original one: (a) J M ∞,off,S ≤ λ: Since we relaxed the ∞ bounds on off-diagonal entries in set S, we need to make sure that the modified solution satisfies this bound in order to have equivalence between modified and original programs solutions. (b) Set Z α S R = − Σ n + J −1 M − γ Z γ S R where Z γ ∈ ∂ J M 1,off . Note that since | J M ij | = λ = 0 for any (i, j) ∈ S R , then Z γ and therefore above equation is welldefined . Now we need to check: Z α ij J M ij ≥ 0 for all (i, j) ∈ S R . This means that they have the same sign or one of them is zero. We need this condition for equivalence between solutions because Lagrangian multipliers in original program (19) corresponding to inequality constraints should be nonnegative. (c) Z ∞,S c M < γ: According to the J M S c M = 0 constraint in the modified program , all the inequality constraints become inactive in the original one when desired J M = J M equality is satisfied. Then, complementary slackness condition enforce all the Lagrangian multipliers corresponding to set S c M to be zero. These can be satisfied by the above strict dual feasibility. Also note that having zero Lagrangian multipliers results in zero residual entries, i. e., Σ R S c M = 0 and therefore ∆ R ∞,S c M = 0 when this feasibility condition is satisfied. Also note that we dropped the positive-definiteness constraint Σ M −Σ R 0 in the proof outline. Thus, in addition to above conditions, we also need to show that Σ = Σ M − Σ R 0 in the modified program. Before we state the detailed proof for the theorem, we introduce a pair of definitions which are used in the analysis. Let us define matrix E as difference between sample covariance matrix and the exact covariance matrix E := Σ n − Σ * . (29) We also define R ∆ J as the difference between J −1 M and its first order Taylor expansion around J * M . Recall that ∆ J was defined as ∆ J := J M − J * M . According to results for first order derivative of inverse function J −1 M (Boyd and Vandenberghe, 2004), the remainder is R ∆ J = J −1 M − J * M −1 + J * M −1 ∆ J J * M −1 . (30) D.2 Proof of the Theorem Exploiting lemmata mentioned in Appendix E, Theorem 5 is proved as follows: Proof According to the sample error bound mentioned in Lemma 4, we have E ∞ ≤ δ f (p τ ; n) for some τ > 2 with probability greater than or equal to 1 − 1/p τ −2 . In the discussion after this, it is assumed that the above bound for E ∞ is satisfied and therefore the following results are valid with probability greater than or equal to 1 − 1/p τ −2 . By choosing γ = m α δ f (p τ ; n), we have E ∞ ≤ α m γ as desired for Lemma 5. Choosing λ δ as in (36) (compatible with what mentioned in the theorem), we only need to show that the other bound on R ∞ is also satisfied to be able to apply Lemma 5. As stated in the remark after Theorem 5, the bound on sample complexity is not asymptotic and we assume the following lower bound on the number of samples which is compatible with the asymptotic form mentioned in the theorem: n > n f p τ ; 1/ max v * , 4ld 1 + m α K SS K M max 1, 4 l − 1 1 + m α K SS K 2 M , (31) for some l > 1. Because of monotonic behaviour of the tail function, for any n satisfying above bound, we have: δ f p τ ; n) ≤ min 1 v * , 1 4ld(1 + m α )K SS K M , l − 1 16ld(1 + m α ) 2 K 2 SS K 3 M , (32) According to the selection for regularization parameters λ δ and γ and the bound on sample error E ∞ , we have: r := 2K SS R λ δ + 2K SS E ∞ + γ ≤ 4K SS R K SS 1 − 2K SS R 1 + α m m α + 2K SS 1 + m α δ f (p τ ; n) High-Dimensional Covariance Decomposition = 2K SS 1 + m α δ f (p τ ; n) 1 1 − 2K SS R (=: λ δ ) < 4K SS 1 + m α δ f (p τ ; n), where in the last inequality, we used the second condition is assumption (A.4) that K SS R < 1/4. Note that second line is equal to λ δ since we assigned the same value in (36). Applying the bound (32) on above inequality, we have 2K SS R λ δ + 2K SS E ∞ + γ < min 1 ldK M , l − 1 4ld(1 + m α )K SS K 3 M ≤ min 1 ldK M , l − 1 2ldK SS K 3 M . Thus, the conditions for Lemma 7 are satisfied and we have ∆ J ∞,S ≤ 2K SS R λ δ + 2K SS E ∞ + γ ≤ λ δ < 4K SS 1 + m α δ f (p τ ; n). (33) Above inequalities tell us multiple things. First, since the error ∆ J ∞,S is bounded by λ δ , the J M entries in set S can not deviate from exact one J * M more than λ δ . We also assumed that the off-diagonal entries in J * M are bounded by λ * . Therefore according to the definition of λ δ := λ − λ * , the entries in J M off,S are bounded by λ and therefore the condition (a) for feasibility of primal-dual witness method is satisfied, i.e., we have J M ∞,off,S ≤ λ. Second, since ∆ J ∞,S R = λ δ , we have ∆ J ∞,S ≤ ∆ J ∞,S R and therefore ∆ J ∞ = ∆ J ∞,S R = λ δ which results the following error bound ∆ J ∞ := J M − J * M ∞ ≤ 4K SS 1 + m α δ f (p τ ; n). (34) Furthermore, ∆ J ∞ < 1 ldK M bound can be concluded from above inequality by substituting δ f (p τ ; n) from (32). Thus, the condition for Lemma 6 is satisfied and we have the following bound on the remainder term R ∆ J ∞ ≤ l l − 1 d ∆ J 2 ∞ K 3 M ≤ 16l l − 1 dK 3 M K 2 SS 1 + m α 2 δ f (p τ ; n) 2 = 16l l − 1 dK 3 M K 2 SS 1 + m α 2 δ f (p τ ; n) δ f (p τ ; n) ≤ δ f (p τ ; n) = α m γ, where in the second inequality, we used error bound in (34) and the last inequality is concluded from bound (32). Now the conditions for Lemma 5 are satisfied and therefore we have the upper bound on ∆ R ∞,S R < C 3 γ and the strict dual feasibility on S c M . Second result satisfies condition (c) of the primal-dual witness method feasibility conditions. The upper bound on ∆ R ∞,S R in conjunction with the lower bound on Σ * R min > C 3 γ (mentioned in the theorem), ensures that the sign of Σ * R and Σ R are the same which results that the condition (b) of the feasibility conditions for primal-dual witness method is satisfied. Since all three conditions (a)-(c) are satisfied, we have equivalence between the modified program and the original one under conditions specified in the theorem. It gives us both results (a) and (b) in the theorem. Then by assuming lower bound on minimum nonzero value of J * M , the result in part (c) is also proved. As mentioned before, we need to show that the dropped constraint Σ = Σ M − Σ R 0 is also satisfied. Since the conditions for Corollary 13 in Appendix E.5 are satisfied, we have the spectral norm error bound (41) on overall covariance matrix Σ. Applying the inverse tail function for Gaussian distribution in (35) to assumption (A.6) results that the minimum eigenvalue of exact covariance matrix Σ * satisfies lower bound λ min (Σ * ) ≥ C 4 + m α C 3 dδ f (p τ ; n) + C 5 d 2 δ f (p τ ; n) 2 where C 6 := C 4 + m α C 3 2q 2 and C 7 := 2q 2 C 5 . Then by exploiting Weyl's theorem (Theorem 4.3.1 in Horn and Johnson (1985)), the estimated covariance matrix Σ is positive definite and thus valid. Therefore, the result is proved. Appendix E. Auxiliary Lemmata First, the tail condition for a probability distribution is defined as follows. Definition 12 (Tail Condition) The random vector X satisfies tail condition with parameters f and v * if there exists a constant v * ∈ (0, ∞) and function f : N×(0, ∞) → (0, ∞) such that for any (i, j) ∈ V × V : P[| Σ n − Σ * ij | ≥ δ] ≤ 1 f (n, δ) for all δ ∈ (0, 1 v * ]. Note that since the function f (n, δ) is an increasing function of both variables n and δ, we define the inverse functions n f (r; δ) and δ f (r; n) with respect to variables n and δ respectively (when the other argument is fixed), where f (n, δ) = r. , we have the following concentration bound for the empirical covariance matrix of Gaussian random variables. Lemma 3 (Ravikumar et al. 2011) Consider a set of Gaussian random variables with covariance matrix Σ * . Given n i.i.d. samples, the sample covariance matrix Σ n satisfies P[| Σ n ij − Σ * ij | > δ] ≤ 4 exp − nδ 2 2q 2 for all δ ∈ (0, q), for some constant q > 0. High-Dimensional Covariance Decomposition Thus the tail function for Gaussian random vector takes the exponential form with the following corresponding inverse functions: n f (r; δ) = 2q 2 log(4r) δ 2 , δ f (r; n) = 2q 2 log(4r) n (35) Applying above Lemma, we get the following bound for sampling error. Lemma 4 (Ravikumar et al. 2011) For any τ > 2 and sample size n such that δ f (p τ ; n) < 1/v * , we have P E ∞ ≥ δ f (p τ ; n) ≤ 1 p τ −2 → 0.

E.2 Feasibility Conditions

In the following lemma, we propose some conditions to bound the residual error ∆ R ∞,S R and also satisfy the condition (c) of feasibility conditions required for equivalence between the witness solution and the original one. Lemma 5 Suppose that max {{R ∞ , E ∞ } ≤ α m γ, λ δ = 2K SS 1 − 2K SS R 1 + α m γ, (36) then a) ∆ R ∞,S R ≤ C 3 γ for some C 3 > 0. b) Z ∞,S c M < γ.

Proof

Applying definitions (29) and (30) to optimality condition considered in second step of primal-dual witness method construction, gives the following equivalent equation Above equation is a p × p matrix equation. We can rewrite it as a linear equation with size p 2 if we use the vectorized form of matrices. Vectorized form of a matrix D ∈ R p×p is a column vector D ∈ R p 2 which is composed by concatenating the rows of matrix D in a single column vector. In the vectorized form, we have Decomposing the vectorized form of (37) into three disjoint partitions S, S R and S c M gives the following decomposed form    Γ * SS Γ * SS R Γ * SS c M Γ * S R S Γ * S R S R Γ * S R S c M Γ * S c M S Γ * S c M S R Γ * S c M S c M       ∆ J S − → λ δ 0    −   0 Σ * R S R 0   +     −R + E + Z S −R + E + Z S R −R + E + Z S c M     = 0, (38) where we used the equalities ∆ J S R = − → λ δ and ∆ J S c M = 0. Note that vector − → λ δ only includes ±λ δ entries according to the constraints in the modified program. Also note that Σ * R is zero in sets S and S c M . We also dropped the argument ∆ J from remainder function R ∆ J to simplify the notation. Similar to the original program, the matrix Z is composed of two parts, Z β and Z γ , i.e., Z = Z β + γ Z γ . Matrix Z β = Σ R from equation (22), includes Lagrangian multipliers and Z γ ∈ ∂ J M 1,off . For set S, Z β S = 0, since we don't have any constraint in the program and therefore the Lagrangian multipliers are zero. Applying this to the first row of equation (38) and since Γ * SS is invertible, we have the following for error ∆ J in set S ∆ J S = Γ * SS −1 −Γ * SS R − → λ δ + R S − E S − γ Z γ S , (39) In set S R , Z S R = Σ R S R + γ Z γ S R . Applying this to the second row of equation (38) results Γ * S R S ∆ J S + Γ * S R S R − → λ δ + ∆ R S R + γ Z γ S R − R S R + E S R = 0, Recall that we defined ∆ R := Σ R − Σ * R . Substituting (39) in above equation results the following for error ∆ R in set S R ∆ R S R = − Γ * S R S Γ * SS −1 −Γ * SS R − → λ δ + R S − E S − γ Z γ S − Γ * S R S R − → λ δ − γ Z γ S R + R S R − E S R . Taking ∞ element-wise norm from above equation and using inequality Ax ∞ ≤ |||A||| ∞ x ∞ for any matrix A ∈ R r×s and vector x ∈ R s , results the bound ∆ R ∞,S R ≤ |||−Γ * S R S Γ * SS −1 Γ * SS R + Γ * S R S R ||| ∞ λ δ + |||Γ * S R S Γ * SS −1 ||| ∞ R S ∞ + E S ∞ + γ + R S R ∞ + E S R ∞ + γ , where we used the fact that − → λ δ ∞ = λ δ and Z γ ∞ = 1. Now if we apply the assumptions mentioned in the lemma, ∆ R ∞,S R ≤ 2K SS (m + α) Taking ∞ element-wise norm from above equation gives the following bound Z ∞,S c where we used the bound on K SS R in assumption (A.4) in the second inequality and the fact that α > 0 in the third inequality. Final inequality is derived from assumption (A.5) which finishes the proof of part (b).

E.3 Control of Remainder

In the following Lemma which is stated and proved in lemma 5 in Ravikumar et al. (2011), the argument ∆ J controls the remainder function behavior. Lemma 6 Suppose that the element-wise ∞ bound ∆ J ∞ ≤ 1 lK M d for some l > 1 holds. Then R ∆ J = J * M −1 ∆ J 2 QJ * M −1 , where Q := ∞ k=0 (−1) k J * M −1 ∆ J k with bound |||Q T ||| ∞ ≤ l l−1 . Also, in terms of elementwise ∞ norm, we have R ∆ J ∞ ≤ l l − 1 d ∆ J 2 ∞ K 3 M . E.4 Control of ∆ J According to the primal-dual witness solutions construction, we have the error bounds on ∆ J within the sets S R and S c M such that ∆ J ∞,S R = λ δ and ∆ J ∞,S c M = 0. In the following lemma, we propose some conditions to control the error ∆ J ∞,S . Lemma 7 Suppose that r := 2K SS R λ δ + 2K SS E ∞ + γ ≤ min 1 ldK M , l − 1 2ldK SS K 3 M , then we have the following element-wise ∞ bound for ∆ J S , ∆ J ∞,S ≤ r. The proof is within the same lines of Lemma 6 proof in Ravikumar et al. (2011) but with some modifications since the error ∆ J ∞,S R is not zero and therefore the nonzero value λ δ arises in the final result. Since the modified optimization program (21) is different with the modified program in Ravikumar et al. (2011), it is worth discussing about existing a unique solution for the modified optimization program (21). This uniqueness can be shown with similar discussion presented in Appendix B for uniqueness of the solution of original program (19). We only need to show that there is no problem in uniqueness by removing the off-diagonal constaraints for set S in the modified program. By Lagrangian duality, the 1 penalty term γJ M 1,off can be moved to constraints as J M 1,off ≤ C(γ) for some bounded C(γ). Therefore, the off-diagonal entries in set S where the corresponding constraints were relaxed in the modified program are still bounded because of this 1 constraint. Hence, the modified program (21) has a unique solution. Under the same assumptions (excluding (A.6)) as Theorem 5, with probability greater than 1 − 1/p c , the overall covariance matrix estimate Σ = Σ M − Σ R satisfies spectral norm error bound ||| Σ − Σ * ||| ≤ C 4 + m α C 3 dδ f (p τ ; n) + C 5 d 2 δ f (p τ ; n) 2 . (41) Proof We first bound the spectral norm errors for the Markov and residual covariance matrices Σ M and Σ R . Along the same lines as Corollary 4 proof in Ravikumar et al. (2011), the spectral norm error ||| Σ M − Σ * M ||| can be bounded as ||| Σ M − Σ * M ||| ≤ C 4 dδ f (p τ ; n) + C 5 d 2 δ f (p τ ; n) 2 , where C 4 = 4 1 + m α K SS K 2 M and C 5 = 16l l−1 1 + m α 2 K 2 SS K 3 M . The spectral norm error ||| Σ R − Σ * R ||| can be also bounded as ||| Σ R − Σ * R ||| ≤ ||| Σ R − Σ * R ||| ∞ ≤ d Σ R − Σ * R ∞ ≤ m α C 3 dδ f (p τ ; n), where the first inequality is the property of spectral norm which is bounded by ∞ -operator norm, second inequality is a result of the fact that Σ R and Σ * R has at most d nonzero entries in each row (since S R ⊂ S M ) and the last inequality is concluded from the upper bound on ∞ element-wise norm error on residual matrix estimation stated in part (a) of Theorem 5. Applying the above bounds to the overall covariance matrix estimation Σ = Σ M − Σ R and using the triangular inequality for norms, the bound in (41) is proven. Appendix F. Proof of Corollary 9 Proof The result in this corollary is a special case of general result in Theorem 5 when λ * = 0 and some minor modifications are considered in problem formulation. Note that, it is expressed in assumption (A.1) that the off-diagonal entries of exact Markov matrix J * M are upper bounded by some positive λ * . In order to extend the proof to the case of λ * = 0 (The case in this corollary), we need some minor modifications. First, the identifiability assumptions (A.0)-(A.3) can be ignored and instead it is assumed that the Markov part J * M (or equivalently Σ * M ) is diagonal and the residual part Σ * R has only nonzero off-diagonal entries. Since the diagonal Markov matrix and off-diagonal residual matrix do not have any nonzero overlapping entries, it is natural that we do not require any more identifiability assumptions. Then, with these new assumptions, the set S M is defined as S M := S R ∪ {(i, i)|i = 1, ..., p} where S R is defined the same as (10) and also set S is defined the same as (11) which results that set S includes only diagonal entries. Thus, the off-diagonal entries belongs to sets S R and S c M . Since Σ * M is a diagonal matrix, all submatrices of Γ * which are indexed by sets S R or S c M are complete zero matrices. The result is that the terms which are bounded in the mutual incoherence condition (A.4) are already zero and thus there is no need to consider those additional assumptions in the corollary. By making these changes in the problem formulation, the result in Corollary 9 can be proven within the same lines of general result proof in Theorem 5. It is only required to change the constraint on set S R in the modified optimization program to J M S R = λ sign Σ * R S R .