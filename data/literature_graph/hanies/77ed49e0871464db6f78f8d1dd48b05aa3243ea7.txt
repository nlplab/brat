Title: Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods

Abstract: Training neural networks is a challenging non-convex optimization problem, and backpropa-gation or gradient descent can get stuck in spurious local optima. We propose a novel algorithm based on tensor decomposition for training a two-layer neural network. We prove efficient risk bounds for our proposed method, with a polynomial sample complexity in the relevant parameters , such as input dimension and number of neurons. While learning arbitrary target functions is NP-hard, we provide transparent conditions on the function and the input for generalizability. Our training method is based on tensor decomposition, which provably converges to the global optimum, under a set of mild non-degeneracy conditions. It consists of simple embarrassingly parallel linear and multi-linear operations, and is competitive with standard stochastic gradient descent (SGD), in terms of computational complexity. Thus, we have a computationally efficient method with guaranteed risk bounds for training neural networks with general non-linear activations.

Content: Introduction

Neural networks have revolutionized performance across multiple domains such as computer vision and speech recognition. However, a theoretical understanding of neural networks is mostly incom- plete. In this paper, we analyze methods for training neural networks with guaranteed risk bound. Risk measures the ability of the network to generalize to new unseen test samples. Analyzing only the training error can lead to overfitting and poor performance on new samples, and hence, risk is considered as a better measure of performance. Analysis of risk for neural networks is a classical problem. Barron (1994) provides risk bounds for a two-layer neural network, but for a computationally inefficient method. There are extensive negative results on training neural networks, and training even simple networks with guaranteed risk bounds is NP-hard (Rojas, 1996; ˇ Síma, 2002; Blum and Rivest, 1993; Bartlett and Ben-David, 1999; Kuhlmann, 2000), , e.g., even training a network with a single neuron is NP-hard. Local search methods such as gradient descent or backpropagation are inexpensive to run (as per iteration cost), but can get stuck in spurious local optima. Explicit examples of its failure have been accounted before (Brady et al., 1989; Gori and Tesi, 1992; Frasconi et al., 1997). Thus, up until now, there are no computationally efficient training methods with a theoretical analysis for training neural networks with general non-linear activation functions. These negative results have led to widespread pessimism regarding the relevance of theoretical analysis for neural networks. We provide a contrasting viewpoint in this paper, and demonstrate that a wide class of neural networks can indeed be trained using computationally inexpensive methods with guaranteed risk bound. Note that NP-hardness refers to the computational complexity of worst-case instances. Instead , we provide transparent conditions on the target functions and inputs for tractable learning. We propose a novel training method based on the method of moments, which involves decomposing the moments to learn the model parameters. While pairwise moments are represented using a matrix, higher order moments require tensors, and the learning problem can be formulated as tensor decomposition. A CP decomposition of a tensor involves finding a sum of rank-one components that best fit the input tensor. Even though it is non-convex, the global optimum of tensor decomposition can be achieved using computationally efficient techniques, under a set of mild nondegeneracy conditions (Anandkumar et al., 2014a,b,c; Bhaskara et al., 2013). These methods have been recently employed for learning a wide range of latent variable models. Incorporating tensor methods for training neural networks requires addressing a number of non-trivial questions: What form of moments are informative about network parameters? Earlier works using tensor methods assume a linear relationship between the hidden and observed variables. However, neural networks possess non-linear activation functions. How do we adapt tensor methods for this setting? How do these methods behave in the presence of approximation and sample perturbations? How can we establish efficient risk bounds? We address these questions shortly.

Summary of Results

The main contributions are: (a) we propose an efficient algorithm, termed as Neural Network- LearnIng using Feature Tensors (NN-LIFT), (b) we demonstrate that the method is embarrassingly parallel and is competitive with standard SGD in terms of computational complexity, and (c) we establish that it has efficient risk bounds, when the number of training samples scales polynomially in relevant parameters such as input dimension and number of neurons. We analyze training of a two-layer feedforward neural network, where the second layer has linear activation function. This is the classical neural network considered by Barron (1994), and a starting point for the analysis of any neural network algorithm. At a high level, NN-LIFT estimates the weights of the first layer using tensor CP decomposition. It then uses these estimates to learn the bias parameters and the weights of the second layer (jointly), using simple Fourier analysis. NN-LIFT consists of simple linear and multi-linear operations (Anandkumar et al., 2014a,b,c) and Fourier analysis, which are parallelizable to large-scale datasets. The computational complexity is comparable to that of the standard SGD. Score functions: The moment tensors employed in NN-LIFT involve certain (non-linear) transformations of the input. These transformations depend on the input distribution, and can be learnt in an unsupervised manner, without the need for labeled samples. Concretely, we employ score functions of the input, which are normalized derivatives of the input probability density function (pdf); see (5) for the definition. Unlike many theoretical works (Andoni et al., 2014), we do not limit ourselves to distributions such as product or Gaussian distributions for the input. Results on risk bounds: Risk bound is a combination of approximation and estimation errors. The approximation error is the error in fitting the target function to a neural network, and the estimation error is the error in estimating the weights of that neural network using the given samples. We show that the estimation error under NN-LIFT is bounded, when the number of training samples scales polynomially in the input dimension, number of neurons, etc. Moreover, we provide a bound on the approximation error, under which we can guarantee learning under NN- LIFT. In other words, we can learn target functions which are well approximated under the given network architecture. It is not of practical interest to consider functions with large approximation errors, since classification performance in that case is poor (Bartlett, 1998). Intuitions behind the conditions for risk bound and generalizability: Since there exist worst-case instances where learning is hard, it is natural to expect that NN-LIFT has guarantees only when certain conditions are met. We assume that the input has a regular continuous probability density function (pdf). This is a reasonable assumption, since under Boolean inputs (a special case of discrete input), it reduces to learning parity with noise (Kearns and Vazirani, 1994). We assume that the activating functions are sufficiently non-linear, since if they are linear, then the network can be collapsed into a single layer (Baldi and Hornik, 1989). We precisely characterize how the estimation error depends on the non-linearity of the activating function. Another condition for providing efficient risk bound is non-redundancy of the neurons. If the neurons are redundant, it is an over-specified network. In the realizable setting, where the target function is generated by a neural network with the given number of neurons k, we require (tensorizations of) network weights to be linearly independent. In the non-realizable setting, we require this to be satisfied by k vectors drawn randomly from the Fourier magnitude distribution of the target function. This is a mild condition which holds when the distribution is continuous in some domain. Thus, our conditions for achieving efficient risk bounds are mild and encompass a large class of target functions and input distributions. Why tensors are required? If the input is a vector (the typical case), the first order score function (i.e., the first derivative) is a vector, the second order score is a matrix, the third order is a tensor, etc. We employ the cross-moment which encodes the correlation between the third order score function and the label in NN-LIFT. We then decompose the moment tensor as a sum of rank-1 components to yield the weight vectors of the first layer. We require at least a third order tensor to learn the neural network weights for the following reasons: while a matrix decomposition is only identifiable up to orthogonal components, tensors can have identifiable non-orthogonal components. In general, it is not realistic to assume that the weight vectors are orthogonal, and hence, we require tensors to learn the weight vectors. Moreover, through tensors, we can learn overcomplete networks, where the number of hidden neurons can exceed the input/output dimensions. Note that matrix factorization methods are unable to learn overcomplete models, since the rank of the matrix cannot exceed its dimensions. Thus, it is critical to incorporate tensors for training neural networks. A recent set of papers have analyzed the tensor methods in detail, and established convergence and perturbation guarantees (Anandkumar et al., 2014a,b,c; Bhaskara et al., 2013), despite non-convexity of the decomposition problem. Such strong theoretical guarantees are essential for deriving provable risk bounds for NN-LIFT. Our algorithm NN-LIFT can be extended to more layers, by recursively estimating the weights layer by layer. In principle, our analysis can be extended by controlling the distortion introduced due to layer-by-layer estimation. Establishing precise guarantees is an exciting open problem.

Related works

We first review some works regarding the analysis of backpropagation, and then provide some theoretical results on training neural networks. Analysis of backpropagation: Baldi and Hornik (1989) show that when the activations are linear, then backpropagation has a unique local optimum, and it corresponds to the principal components of the covariance matrix of the training examples. However, it is known that there exist networks with non-linear activations where backpropagation fails. Brady et al. (1989) construct simple cases of linearly separable classes, where backpropagation fails. Note that the simple perceptron algorithm will succeed here due to linear separability. Gori and Tesi (1992) argue that such examples are artificial and that backpropagation succeeds in reaching the global optimum for linearly separable classes in practical settings. However, they show that under non-linear separability , backpropagation can get stuck in local optima. For a detailed survey, see Frasconi et al. (1997). Recently, Choromanska et al. (2015) analyze the loss surface of a multi-layer ReLU network by relating it to a spin glass system. They make several assumptions such as variable independence for the input, equally likely paths from input to output, redundancy in network parameterization and uniform distribution for unique weights. We agree with the authors of Choromanska et al. (2015) that some of these assumptions are not realistic. Based on these assumptions, they show that the lowest critical values of the random loss function form a layered structure, and the number of local minima outside that band diminishes exponentially with the network size. Previous theoretical works for training neural networks: Andoni et al. (2014) learn polynomial target functions using a two-layer neural network under Gaussian/uniform input distribution . They argue that the weights for the first layer can be selected randomly, and only the second layer weights, which are linear, need to be fitted optimally. However, in practice, Gaussian/uniform distributions are never encountered in classification problems. For general distributions, random weights in the first layer is not sufficient. Under our framework, we impose only mild non-degeneracy conditions on the weights. Livni et al. (2014) make the observation that sufficiently over-specified networks are easy to train. However, this leads to over-fitting and poor risk error, unless the weights are controlled appropriately (Bartlett, 1998). Livni et al. (2014) consider learning a two-layer neural network with quadratic activation functions and prove that an efficient forward greedy procedure has bounded error. However, the standard sigmoidal networks require a large depth polynomial network, which is not practical. Recently, Sedghi and Anandkumar (2014) consider learning neural networks with sparse connectivity. They employ the cross-moment between the (multi-class) label and (first order) score function of the input. They show that they can provably learn the weights of the first layer, as long as the weights are sparse enough, and there are enough number of input dimensions and output classes (at least linear up to log factor in the number of neurons in any layer). In this paper, we remove these restrictions and allow for the output to be just binary class (and indeed, our framework applies for multi-class setting as well, since the amount of information increases with more label classes), and for the number of neurons to exceed the input/output dimensions (overcomplete setting). Moreover, we extend beyond the realizable setting, and do not require the target functions to be generated from the class of neural networks under consideration.

Preliminaries and Problem Formulation

We first introduce some notations and then propose the problem formulation.

Notation

Let [n] denote the set {1, 2, . . . , n}. For matrix C ∈ R d×k , the j-th column is referred by C j or c j , j ∈ [k]. · denotes the ℓ 2 or Euclidean norm operator. Throughout this paper, ∇ (m) x denotes the m-th order derivative operator w.r.t. variable x. For matrices A, B ∈ R d×k , the Khatri-Rao product C := A ⊙ B ∈ R d 2 ×k is defined such that C(l + (i − 1)d, j) = A(i, j) · B(l, j), for i, l ∈ [d], j ∈ [k]. Tensor: A real m-th order tensor T ∈ m R d is a member of the outer product of Euclidean spaces R d . The different dimensions of the tensor are referred to as modes. For instance, for a matrix, the first mode refers to columns and the second mode refers to rows. For a third order tensor T ∈ R d×d×d , the matricized version along first mode denoted by M ∈ R d×d 2 is defined such that T (i, j, l) = M (i, l + (j − 1)d), i, j, l ∈ [d]. Tensor rank: A 3rd order tensor T ∈ R d×d×d is said to be rank-1 if it can be written in the form T = w · a ⊗ b ⊗ c ⇔ T (i, j, l) = w · a(i) · b(j) · c(l), (1) where ⊗ represents the outer product notation, and a, b, c ∈ R d are unit vectors. A tensor T ∈ R d×d×d is said to have a CP rank k if it can be (minimally) written as the sum of k rank-1 tensors T = i∈[k] w i a i ⊗ b i ⊗ c i , w i ∈ R, a i , b i , c i ∈ R d . (2) Fourier transform: For a function f (x) : R d → R, the multivariate Fourier transform F (ω) : R d → R is defined as F (ω) := · · · +∞ −∞ f (x)e −jω,x dx 1 · · · dx d , (3) where variable ω is called the frequency variable, and j denotes the imaginary unit. We also denote the Fourier pair (f (x), F (ω)) as f (x) Fourier ← −−− → F (ω).

Problem formulation

We now introduce the problem of training a neural network which is the goal of this paper. It is known that continuous functions with compact domain can be arbitrarily well approximated by feedforward neural networks with one hidden layer and sigmoidal nonlinear functions (Cybenko, 1989; Hornik et al., 1989; Barron, 1993). In this paper, we propose a new algorithm for training these neural networks and provide risk bounds with respect to the target function. Risk error of a neural network consists of two parts: approximation error and estimation error. Approximation error is the error in fitting the target function to a neural network with the given architecture, and estimation error is the error in training that network with finite number of samples. Thus, the risk error measures the ability of the trained neural network to generalize to new data. σ σ σ σ h x 1 x 2 x 3 x dx x E[y|x] A 2 A 1 · · · · · · · · · Figure 1: Graphical representation of a Neural Network, E[y|x] = A ⊤ 2 σ(A ⊤ 1 x + b 1 ) + b 2 . Note that this representation is for general vector output y which can be also written as E[y|x] = a 2 , σ(A ⊤ 1 x + b 1 ) + b 2 in the case of scalar output y. We consider a neural network with one hidden layer of dimension k. Let the output y ∈ {0, 1} be the binary label, and x ∈ R d be the feature vector. 1 We consider the label generating model f (x) := E[y|x] = a 2 , σ(A ⊤ 1 x + b 1 ) + b 2 , (4) where σ(·) is (linear/nonlinear) elementwise function. See Figure 1 for a schematic representation of label-function in (4) in the general case of vector output y. The first goal is to train the neural network in (4), i.e., to learn the weight matrices (vectors) A 1 ∈ R d×k , a 2 ∈ R k and bias vectors b 1 ∈ R k , b 2 ∈ R. This is the estimation part where we have a label-function f (x) specified in (4) with fixed unknown parameters, and the goal is to learn the parameters and finally bound the overall function estimation error |f (x) − ˆ f (x)|. For this task, we propose a computationally efficient algorithm which requires only polynomial number of samples for bounded estimation error. This is the first time that such a result has been established for any neural network. We then combine this result with the approximation bounds in Barron (1993) leading to risk bounds with respect to the target function. The detailed results are provided in Section 4.

NN-LIFT Algorithm

In this section, we introduce our proposed method for learning neural networks using tensor and Fourier techniques. Our method is shown in Algorithm 1 named NN-LIFT (Neural Network Learn- Ing using Feature Tensors). The algorithm has two main components. The first component involves estimating the weight matrix of the first layer denoted by A 1 ∈ R d×k . The second component involves estimating the weight vector of the second layer a 2 ∈ R k , and the bias vector of the first layer b 1 ∈ R k . Note that most of the unknown parameters (compare the dimension of matrix A 1 , and vectors a 2 , b 1 ) are estimated in the first part, and thus, this is the main part of the algorithm. We now explain the steps of Algorithm 1 in more details. input Labeled samples {(x i , y i ) : i ∈ [n]}, parameter˜ǫ . 1: Estimate third-order Score function S 3 (x); see Equation (5) for the definition. 2: Compute T = 1 n i∈[n] y i · S 3 (x i ) 3: {( ˆ A 1 ) j } j∈[k] = tensor decomposition( T ); see Appendix B for details. 4: (ˆ a 2 , ˆ b 1 ) = Fourier method({(x i , y i )}, ˆ A 1 , ˜ ǫ); see Procedure 2. 5: returnˆA 1 , ˆ a 2 , ˆ b 1 .

Score function

In our framework, we consider any general input with pdf p(x) satisfying mild regularity conditions. The first step is to estimate the score function S 3 (x) for the input layer x. The m-th order score function S m (x) ∈ m R d is defined as (Janzamin et al., 2014) S m (x) := ( −1) m ∇ (m) x p(x) p(x) , (5) where p(x) is the joint probability density function of random vector x ∈ R d , and ∇ (m) x denotes the m-th order derivative operator. The main property of score functions as yielding differential operators which enables us to estimate the weight matrix A 1 via tensor decomposition is discussed in the next subsection; see Equation (6). Estimation of score function: There are various efficient methods for estimating the score function. The framework of score matching is popular for parameter estimation in probabilistic models (Hyvärinen, 2005; Swersky et al., 2011), where the criterion is to fit parameters based on matching the data score function. In addition, Swersky et al. (2011) analyze the score matching for latent energy-based models. In deep learning, the framework of auto-encoders attempts to find encoding and decoding functions which minimize the reconstruction error under noise (the so-called denoising auto-encoders or DAE). This is an unsupervised framework involving only unlabeled samples. Alain and Bengio (2012) argue that the DAE approximately learns the first order score function of the input, as the noise variance goes to zero. Therefore, we can use any of these methods for estimating S 1 (x) and use the recursive form (Janzamin et al., 2014) S m (x) = −S m−1 (x) ⊗ ∇ x log p(x) − ∇ x S m−1 (x) to estimate higher order score functions.

Tensor decomposition

The score functions are new representations (extracted features) of input data x that are used for training neural networks. We use score functions and labels to form the empirical cross-moment T = 1 n i∈[n] y i · S 3 (x i ). We decompose tensor T to estimate the columns of A 1 . The following discussion makes this part of the algorithm clear. The score functions have the property of yielding differential operators with respect to the input distribution. More precisely, for label-function f (x) := E[y|x], Janzamin et al. (2014) show that E[y · S 3 (x)] = E[∇ (3) x f (x)]. Note that the function f (x) is a non-linear function of both input x and weight matrix A 1 ; see (4). The expectation operator E[·] averages out the dependency on x, and the derivative acts as a linearization operator as follows. In the neural network output (4), we observe that the columns of weight vector A 1 are the linear coefficients involved with input variable x. When taking the derivative of this function, by the chain rule, these linear coefficients shows up in the final form. In particular, we show in Lemma 4 (see Section 5) that for neural network in (4), we have E [y · S 3 (x)] = j∈[k] λ j · (A 1 ) j ⊗ (A 1 ) j ⊗ (A 1 ) j ∈ R d×d×d , (6) where (A 1 ) j ∈ R d denotes the j-th column of A 1 ; refer to Equation (2) for the notion of tensor rank and its rank-1 components. This clarifies how the score function acts as a linearization operator while the final output is nonlinear in terms of A 1 . The above form also clarifies the reason behind using tensor decomposition in the learning framework. Tensor decomposition algorithm: The goal of tensor decomposition algorithm is to recover the rank-1 components of tensor. For this step, we use the tensor decomposition algorithm proposed in (Anandkumar et al., 2014b,c); see Appendix B for details. The main step of the tensor decomposition method is the tensor power iteration which is the generalization of matrix power iteration to 3rd order tensors. The tensor power iteration is given by u ← T (I, u, u) T (I, u, u) , where u ∈ R d , T (I, u, u) := j,l∈[d] u j u l T (:, j, l) ∈ R d is a multilinear combination of tensor fibers. 2 The convergence guarantees of tensor power iteration for orthogonal tensor decomposition have been developed in the literature (Zhang and Golub, 2001; Anandkumar et al., 2014b). Note that we first orthogonalize the tensor via whitening procedure and then apply the tensor power iteration. Thus, the original tensor decomposition need not to be orthogonal. To increase speed, we can perform the tensor decomposition in a stochastic manner. This is done by splitting the data into mini-batches. Starting with the first mini-batch, we perform a small number of iterations and use the result as initialization for the next mini-batch. Overall computational complexity of NN-LIFT, when performed in parallel, is O(log(min(d x , d h ))) per iteration with O(kd 2 x d h / log(min(d h , d x ))) processors. Here d h is the number of neurons of the first layer of the auto-encoder used for approximating the score function. Note that the dominant step in terms of computational complexity is the tensor decomposition step. This is comparable with computational complexity of parallel stochastic backpropagation which is O(log d x ) with O(kd x / log(d x )) processors.

Fourier method

The second part of the algorithm estimates second layer weight vector a 2 ∈ R k and first layer bias vector b 1 ∈ R k . This step is very different from the previous step for estimating A 1 which was based on tensor decomposition methods. This is a Fourier-based method where complex variables Procedure 2 Fourier method for estimating a 2 and b 1 . input Labeled samples {(x i , y i ) : i ∈ [n]}, estimatê A 1 , parameter˜ǫ , and let p(x) be the input probability density function. 1: for l = 1 to k do

2:

Let Ω l := ω ∈ R d : ω = 1 2 , ω, ( ˆ A 1 ) l ≥ 1−˜ǫ 2 /2 2 , and |Ω l | denotes the surface area of d−1 dimensional manifold Ω l .

3:

Draw n i.i.d. random frequencies ω i , i ∈ [n], uniformly from set Ω l . 4: Let v := 1 n i∈[n] y i p(x i ) e −jω i ,x i which is a complex number as v = |v|e j∠v . Note that | · | and ∠· respectively denote the magnitude and phase operators applied to the complex number. 5: 6: end for 7: returnâ 2 , ˆ b 1 . are formed using labeled data and random frequencies in the Fourier domain; see Procedure 2. We prove in Lemma 5 that the entries of a 2 and b 1 can be estimated from the magnitude and phase of these complex numbers. Polynomial-time random draw from set Ω l : Note that the random frequencies are drawn from a d − 1 dimensional manifold denoted by Ω l which is the intersection of sphere ω = 1 2 and cone ω, ( ˆ A 1 ) l ≥ 1−˜ǫ 2 /2 2 in R d . This manifold is actually the surface of a spherical cap. In order to draw these frequencies in polynomial time, we consider the d-dimensional spherical coordinate system such that one of the angles is defined based on the cone axis. We can then directly impose the cone constraint by limiting the corresponding angle in the random draw. In addition, Kothari and Meka (2014) propose a method for generating pseudo-random variables from the spherical cap in logarithmic time.

4

Risk Bound for NN-LIFT In this section, we provide the risk bound. We first provide the estimation bound in the realizable setting where the output is generated by a neural network as in (4), and we estimate it using Algorithm 1. Next, we consider the general setting where the output need not be generated by a neural network. In this case, we consider the approximation error in addition to the estimation error to obtain the overall risk bound with respect to the target function.

Estimation bound: realizable setting

In this section, we provide guarantees in the realizable setting, where the function f (x) := E[y|x] is generated by a neural network in (4). We provide the estimation error bound on the overall function recovery |f (x) − ˆ f (x)| when the estimation is done by Algorithm 1. We provide guarantees in the following settings. 1) In the basic case, we consider the undercomplete regime k ≤ d, and provide the results assuming A 1 is full column rank. 2) In the second case, we form higher order cross-moments and tensorize it into a lower order tensor. This enables us to learn the network in the overcomplete regime k > d, when the Khatri-Rao product A 1 ⊙A 1 ∈ R d 2 ×k is full column rank. We call this the overcomplete setting and this can handle up to k = O(d 2 ). Similarly, we can extend to larger k by tensorizing higher order moments. Note that in the binary classification setting (y ∈ {0, 1}), we have E[y|x] := f (x) ∈ [0, 1], and we define the following parameter for label function f (·) as ζ f := +∞ −∞ f (x)dx. Conditions for Theorem 1: • Non-degeneracy of weight vectors: In the undercomplete setting (k ≤ d), the weight matrix A 1 ∈ R d×k is full column rank and s min (A 1 ) > ǫ, where s min (·) denotes the minimum singular value and ǫ > 0 is the target error. In the overcomplete setting (k ≤ d 2 ), the Khatri- Rao product A 1 ⊙ A 1 ∈ R d 2 ×k is full column rank, 3 and s min (A 1 ⊙ A 1 ) > ǫ; see Remark 2 for generalization.

•

Conditions on nonlinear activating function σ(·): the coefficients λ j := E [σ ′′′ (z j )] · a 2 (j), j ∈ [k], in (11) are nonzero. Here, z := A ⊤ 1 x + b 1 is the input to the nonlinear operator σ(·). In addition, σ(·) satisfies the Lipschitz property with constant L such that |σ(u) − σ(u ′ )| ≤ L · |u − u ′ |, for u, u ′ ∈ R. We now elaborate on these conditions. The non-degeneracy of weight vectors are required for the tensor decomposition analysis in the estimation of A 1 . In the undercomplete setting, the algorithm first orthogonalizes (through whitening procedure) the tensor given in (6), and then decomposes it through tensor power iteration. Note that the convergence of power iteration for orthogonal tensor decomposition is guaranteed (Zhang and Golub, 2001; Anandkumar et al., 2014b). For the orthogonalization procedure to work, we need the tensor components (the columns of matrix A 1 ) to be linearly independent. In the overcomplete setting, algorithm performs the same steps with the additional tensorizing procedure; see Appendix B for details. In this case, a higher order tensor is given to the algorithm and it is first tensorized before performing the same steps as in the undercomplete setting. Thus, the same conditions are now imposed on A 1 ⊙ A 1 . Furthermore, the coefficients condition on λ j 's is also imposed to ensure the corresponding rank-1 components in (6) do not vanish, and we can recover that by tensor decomposition. Note that the amount of non-linearity of σ(·) affects the magnitude of coefficients. It is also worth mentioning that although we use the third derivative notation σ ′′′ (·) in characterizing the coefficients λ j , we do not need the differentiability of non-linear function σ(·) in general. In particular, when input x is a continuous variable, we use Dirac delta function δ(·) as the derivative in non-continuous points; for instance, for the derivative of sign function, we have d dx sign(x) = 2δ(x). Thus, in general, we only need the expectation E [σ ′′′ (z j )] to exist for these type of functions and the corresponding higher order derivatives. The proposed algorithm for training the neural network estimates the parameters of label function in (4), i.e., the parameters A 1 , a 2 , b 1 . In order to translate these estimation guarantees to the overall function estimation error |f (x) − ˆ f (x)|, we require a Lipschitz condition on the non-linear function σ(·) which holds for all commonly used activations. The additional bounds imposed on the parameters of the neural network are useful in learning these parameters with computationally efficient algorithms since it limits the searching space for training these parameters. In particular, for the Fourier analysis, we assume the following conditions . Suppose the columns of weight matrix A 1 are normalized, i.e., (A 1 ) j = 1, j ∈ [k], and the entries of first layer bias vector b 1 are bounded as |b 1 (l)| ≤ 1, l ∈ [k]. Note that the normalization condition on the columns of A 1 is also needed for identifiability of the parameters. For instance, if the non-linear operator is the sign function σ(z) = sign(z), then matrix A 1 is only identifiable up to its norm, and thus, such normalization condition is required for identifiability. The estimation of entries of bias vector b 1 is from the phase of a complex number through Fourier analysis; see Procedure 2 for details. Since there is ambiguity in the phase of a complex number (note that a complex number does not change if an integer multiple of 2π is added to its phase), we impose the bounded assumption on the entries of b 1 to avoid this ambiguity. It is reasonable to assume a small amount of bias in practice. In addition to the above main conditions, we also need some mild conditions which are not crucial for the recovery guarantees and are mostly assumed to simplify the presentation of main results. These conditions can be relaxed more. Suppose the input x is bounded, i.e., x ∈ B r , where B r := {x : x ≤ r}. Assume the input probability density function p(x) ≥ ψ for some ψ > 0, and for any x ∈ B r . Let p(x) satisfy some mild regularity conditions on the boundaries of support of p(x). These regularity conditions are required for the properties of score function to hold; see Janzamin et al. (2014) for the details of these regularity conditions. The regularity conditions might seem contradictory with the lower bound condition p(x) ≥ ψ, but there is an easy fix for that. The lower bound on p(x) is required for the analysis of Fourier part of the algorithm. We can have a continuous p(x), while in the Fourier part, we only use x's such that p(x) ≥ ψ, and ignore the rest. This only introduces a probability factor Pr[x : p(x) ≥ ψ] in the analysis. Settings of Algorithm in Theorem 1: • No. of iterations for Algorithms 6: N = Θ log 1 ǫ . • No. of initialization in Procedure 5: L ≥ k Ω(γ) , where γ = (k/d) 2 in the undercomplete setting, and γ = (k/d 2 ) 2 in the overcomplete setting. • Parameter˜ǫ = 1 poly(n) , where n is the number of samples. Theorem 1 (NN-LIFT guarantees: estimation bound). Suppose the above settings and conditions hold. Suppose the number of samples satisfy 4 n ≥ max˜O ζ f ψ˜ǫ 2 2 , ˜ O E M 3 (x)M ⊤ 3 (x) λ 2 min · s 3 min (A 1 ) d 1.5 k 3 , ˜ O λ max · E M 3 (x)M ⊤ 3 (x) λ 3 min · s 6 min (A 1 ) k 4 , where M 3 (x) ∈ R d×d 2 denotes the matricization of score function tensor S 3 (x) ∈ R d×d×d .

4

By doing tensorization in overcomplete regime, the singular value condition is changed as 1 s min (A 1 ⊙A 1 ) . 5 Recall that k is the hidden layer size, r is the bound on input as x ≤ r, and L is the Lipschitz constant of σ(·). Thus, we estimate the neural network in polynomial time and sample complexity. This is one of the first results to provide a guaranteed method for training neural networks with efficient computational and statistical complexity. See Section 5 and Appendix C for the proof of theorem. Remark 1 (Sample complexity for Gaussian input). If the input x is Gaussian as x ∼ N (0, I d ), then the number of required samples is simplified as n ≥ max˜O ζ f ψ˜ǫ 2 2 , ˜ O 1 λ 2 min · s 3 min (A 1 ) d 3 k 3 , ˜ O λ max λ 3 min · s 6 min (A 1 ) d 1.5 k 4 , where E M 3 (x)M ⊤ 3 (x) = ˜ O d 1.5 is used. Remark 2 (Higher order tensorization). We stated that by tensorizing higher order tensors to lower order ones, we can estimate overcomplete models where the hidden layer dimension k is larger than the input dimension d. We can generalize this idea to higher order tensorizing such that m modes of the higher order tensor are tensorized into a single mode in the resulting lower order tensor. This enables us to estimate the models up to k = O(d m ) assuming the matrix A 1 ⊙ · · · ⊙ A 1 (m Khatri-Rao products) is full column rank. This is possible with the additional computational complexity.

Risk bound

In order to provide the risk bound with respect to the target function, we also need to argue the approximation error in addition to the estimation error. For an arbitrary function f (x), we need to find a neural network whose error in approximating the function can be bounded. We then combine it with the estimation error in training that neural network. This yields the final risk bound. The approximation problem is about finding a neural network that approximates an arbitrary function f (x) with bounded error. Thus, this is different from realizable setting where there is a fixed neural network and we only analyze its estimation. Barron (1993) provides an approximation bound for the two-layer neural network and we exploit that here. His result is based on the Fourier properties of function f (x). Recall from (3) the definition of Fourier transform of f (x) denoted by F (ω) where ω is called the frequency variable. Define the first absolute moment of the Fourier magnitude distribution as C f := R d ω 2 · |F (ω)|dω. (7) Barron (1993) analyzes the approximation properties of ˆ f (x) = j∈[k] a 2 (j)σ (A 1 ) j , x + b 1 (j) , (A 1 ) j = 1, |b 1 (j)| ≤ 1, |a 2 (j)| ≤ 2C f , j ∈ [k], (8) where the columns of weight matrix A 1 are the normalized version of random frequencies drawn from Fourier magnitude distribution |F (ω)|. More precisely, ω j i.i.d. ∼ ω C f |F (ω)|, (A 1 ) j = ω j ω j , j ∈ [k]. See Section 5.2.1 for a detailed discussion on this connection between the columns of weight matrix A 1 and the random frequency draws from Fourier magnitude distribution, and see how this is argued in the proof of approximation bound. The other parameters a 2 , b 1 need to be also found. He then shows the following approximation bound for (8). Theorem 2 (Approximation bound, Theorem 3 of Barron (1993)). For function f (x) with bounded C f , the approximationˆf (x) in (8) satisfies the approximation bound E x [|f (x) − ˆ f (x)| 2 ] ≤ O(r 2 C 2 f ) · 1 √ k + δ 1 2 , where f (x) = f (x) − f (0). Here for τ > 0, δ τ := inf 0<ξ≤1/2 2ξ + sup |z|≥ξ σ(τ z) − 1 {z>0} (9) is a distance between the unit step function 1 {z>0} and the scaled sigmoidal function σ(τ z). See Barron (1993) for the complete proof of above theorem. For completeness, we have also reviewed the main ideas of this proof in Section 5.2. We now provide the formal statement of our risk bound. Conditions for Theorem 3: • The nonlinear activating function σ(·) is an arbitrary sigmoidal function. Note that a sigmoidal function is a bounded measurable function on the real line for which σ(z) → 1 when z → ∞ and σ(z) → 0 when z → −∞.

•

The target function f (x) has bounded C f as C f ≤ 1 r 1 √ k + δ 1 −1 · λ min E [S 3 (x) 2 ] · min O s 1.5 min (A 1 ) k 1.5 d 0.75 , O λ min λ max s 3 min (A 1 ) √ log k k 2 + ˜ O ˜ ǫ 2 ψ , where for Gaussian input x ∼ N (0, I d ), we have E [S 3 (x) 2 ] = ˜ O d 1.5 , and r = ˜ O( √ d). • k i.i.d. draws of frequencies from distribution 6 of magnitude Fourier spectrum |F (ω)| are linearly independent w.h.p. In the overcomplete regime, (k > d), the linear independence property holds for tensorizations of the frequency draws. Theorem 3 (NN-LIFT guarantees: risk bound). In addition to the estimation bound conditions in previous section, suppose the above conditions hold. Then the target function f is approximated by the network whose output isˆf learnt using NN-LIFT in Algorithm 1 satisfying w.h.p. E x [|f (x) − ˆ f (x)| 2 ] ≤ O(r 2 C 2 f ) · 1 √ k + δ 1 2 + O(ǫ 2 ), where δ τ is defined in (9). Recall x ∈ B r , where B r := {x : x ≤ r}.

6

Note that it should be normalized to be a probability distribution. The theorem is proved by combining the estimation bound guarantees in Theorem 1, and the approximation bound results for neural networks in Barron (1993). Note that the estimation guarantees in Theorem 1 is for the realizable setting where the observations are the outputs of neural network, while in the above theorem we have samples of arbitrary function f (x). But, the same analysis as in Theorem 1 also works here. See Appendix C.1 for the detailed discussion for this subtle difference where the decomposition of error to approximation and estimation parts are discussed. The above risk bound includes two terms. The first term as O(r 2 C 2 f )· 1 √ k + δ 1 2 represents the approximation error on how the arbitrary function f (x) with parameter C f can be approximated by the neural network, whose weights are drawn from the magnitude Fourier distribution; see Theorem 2 for the formal statement. From the definition of C f in (7), this bound C f is larger when the Fourier spectrum of target f (x) has more energy in higher frequencies. This makes intuitive since it should be easier to approximate a function which is more smooth and has less fluctuations. The second term O(ǫ 2 ) is from estimation error for NN-LIFT algorithm, which is analyzed in Theorem 1. The polynomial factors for sample complexity in our estimation error are slightly worse than the bound provided in Barron (1994), but note that we provide an estimation method which is both computationally and statistically efficient, while the method in Barron (1994) is not computationally efficient. Thus, for the first time, we have a computationally efficient method with guaranteed risk bounds for training neural networks. Discussion on δ τ in approximation bound: The approximation bound involves a term δ τ which is a constant and does not shrink with increasing the neuron size k. Recall that δ τ measures the distance between the unit step function 1 {z>0} and the scaled sigmoidal function σ(τ z) (which is used in the neural network specified in (4)). We now provide the following two observations The above risk bound is only provided for the case τ = 1. We can generalize this result by imposing different constraint on the norm of columns of A 1 in (8). In general, if we impose (A 1 ) j = τ, j ∈ [k], for some τ > 0, then we have the approximation bound 7 O(r 2 C 2 f )· 1 √ k + δ τ 2 . Note that δ τ → 0 when τ → ∞ (the scaled sigmoidal function σ(τ z) converges to the unit step function), and thus, this constant approximation error vanishes. If the sigmoidal function is the unit step function as σ(z) = 1 {z>0} , then δ τ = 0 for all τ > 0, and hence, there is no such constant approximation error.

Proof sketch

In this section, we provide key ideas for proving our main results in Theorems 1 and 2.

Estimation bound

The estimation bound is proposed in Theorem 1, and the complete proof is provided in Appendix C. Recall that NN-LIFT algorithm includes a tensor decomposition part for estimating A 1 , and a Fourier technique for estimating a 2 , b 1 . We propose two main lemmas which clarify why these methods are useful for estimating these unknown parameters in the realizable setting, where the label y is generated by the neural network with the given architecture. In the following lemma, we show how the cross-moment between label and score function as E[y · S 3 (x)] leads to a tensor decomposition form for estimating weight matrix A 1 . Lemma 4. For the two-layer neural network specified in (4), we have E [y · S 3 (x)] = j∈[k] λ j · (A 1 ) j ⊗ (A 1 ) j ⊗ (A 1 ) j , (10) where (A 1 ) j ∈ R d denotes the j-th column of A 1 , and λ j = E σ ′′′ (z j ) · a 2 (j), (11) for vector z := A ⊤ 1 x + b 1 as the input to the nonlinear operator σ(·). This is proved by the main property of score functions as yielding differential operators, where for label-function f (x) := E[y|x], we have E[y · S 3 ( x)] = E[∇ (3) x f (x)] (Janzamin et al., 2014); see Section C.1 for a complete proof of the lemma. This lemma shows that by decomposing the crossmoment tensor E[y · S 3 (x)], we can recover the columns of A 1 . We also exploit the magnitude and phase of complex number v to estimate the weight vector a 2 and bias vector b 1 ; see Procedure 2. The following lemma clarifies this. The perturbation analysis is provided in the appendix. Lemma 5. When ω i 's are uniformly i.i.d. drawn from set Ω l , then the variable v := 1 n i∈[n] y i p(x i ) e −jω i ,x i , (12) has mean (which is computed over x, y and ω) E[v] = 1 |Ω l | Σ 1 2 a 2 (l)e jπb 1 (l) , (13) where |Ω l | denotes the surface area of d − 1 dimensional manifold Ω l , and Σ(·) denotes the Fourier transform of σ(·). This lemma is proved in Appendix C.2.

Approximation bound

We exploit the approximation bound argued in Barron (1993) provided in Theorem 2. We first discuss his main result arguing an approximation bound O(r 2 C 2 f /k) for a function f (x) with bounded parameter C f ; see (7) for the definition of C f . Note that this corresponds to the first term in the approximation error proposed in Theorem 2. For this result, Barron (1993) does not consider any bound on the parameters of first layer A 1 and b 1 . He then provides a refinement of this result where he also bounds the parameters of neural network as we also do in (8). This leads to the additional term involving δ τ in the approximation error as seen in Theorem 2. Note that bounding the parameters of neural network is also useful in learning these parameters with computationally efficient algorithms since it limits the searching space for training these parameters. We now provide the main ideas of proving these bounds as follows.

5.2.1

No bounds on the parameters of the neural network We first provide the proof outline when there is no additional constraints on the parameters of neural network; see set G defined in (15), and compare it with the form we use in (8) where there are additional bounds. In this case Barron (1993) argue approximation bound O(r 2 C 2 f /k) which is proved based on two main results. The first result says that if a function f is in the closure of the convex hull of a set G in a Hilbert space, then for every k ≥ 1, there is an f k as the convex combination of k points in G such that E[|f − f k | 2 ] ≤ c ′ k , (14) for any constant c ′ satisfying some lower bound related to the properties of set G and function f ; see Lemma 1 in Barron (1993) for the precise statement and the proof of this result. The second part of the proof is to argue that arbitrary function f ∈ Γ (where Γ denotes the set of functions with bounded C f ) is in the closure of the convex hull of sigmoidal functions G := γσ α, x + β : α ∈ R d , β ∈ R, |γ| ≤ 2C . (15) Barron (1993) proves this result by arguing the following chain of inclusions as Γ ⊂ cl G cos ⊂ cl G step ⊂ cl G, where cl G denotes the closure of set G, and sets G cos and G step respectively denote set of some sinusoidal and step functions. See Theorem 2 in Barron (1993) for the precise statement and the proof of this result. Random frequency draws from Fourier magnitude distribution: Recall from Section 4.2 that the columns of weight matrix A 1 are the normalized version of random frequencies drawn from Fourier magnitude distribution |F (ω)|. This connection is along the proof of relation Γ ⊂ cl G cos that we recap here; see proof of Lemma 2 in Barron (1993) for more details. By expanding the Fourier transform as magnitude and phase parts F (ω) = e jθ(ω) |F (ω)|, we have f (x) := f (x) − f (0) = g(x, ω)Λ(dω), (16) where Λ(ω) := ωF (ω)/C f is the normalized Fourier magnitude distribution (as a probability distribution), and g(x, ω) := C f ω (cos(ω, x + θ(ω)) − cos(θ(ω))) . The integral in (16) is an infinite convex combination of functions in the class G cos := γ ω (cos(ω, x + β) − cos(β)) : ω = 0, |γ| ≤ C, β ∈ R . Now if ω 1 , ω 2 , . . . , ω k is a random sample of k points independently drawn from Fourier magnitude distribution Λ, then by Fubini's Theorem, we have E Br   f (x) − 1 k j∈[k] g(x, ω j )   2 µ(dx) ≤ C 2 k , where µ(·) is the probability measure for x. This shows function f is in the convex hull of G cos . Note that the bound C 2 k complies the bound in (14).

5.2.2

Bounding the parameters of the neural network Barron (1993) then imposes additional bounds on the weights of first layer, considering the following class of sigmoidal functions as G τ := γσ τ (α, x + β) : α ≤ 1, |β| ≤ 1, |γ| ≤ 2C . (17) Note that the approximation proposed in (8) is a convex combination of k points in (17) with τ = 1. Barron (1993) concludes Theorem 2 by the following lemma. Lemma 6 (Lemma 5 in Barron (1993)). If g is a function on [−1, 1] with derivative bounded 8 by a constant C, then for every τ > 0, we have inf gτ ∈cl Gτ sup |z|≤τ |g(z) − g τ (z)| ≤ 2Cδ τ . Finally Theorem 2 is proved by applying triangle inequality to bounds argued in the above two cases.

Conclusion

We have proposed a novel algorithm based on tensor decomposition for training two-layer neural networks. This is a computationally efficient method with guaranteed risk bounds with respect to the target function under polynomial sample complexity in the input/neuron dimensions. The tensor method is embarrassingly parallel and has a parallel time computational complexity which is logarithmic in input dimension which is comparable with parallel stochastic backpropagation. There are number of open problems to consider in future. Extending this framework to a multi-layer network is of great interest. Exploring the score function framework to train other discriminative models is also interesting.

Acknowledgements

We thank Ben Recht for pointing out to us the Barron's work on approximation bounds for neural networks (Barron, 1993). M. Janzamin is supported by NSF Award CCF-1219234. H. Sedghi is supported by ONR Award N00014-14-1-0665. A. Anandkumar is supported in part by Microsoft Faculty Fellowship, NSF Career award CCF-1254106, NSF Award CCF-1219234, ARO YIP Award W911NF-13-1-0084 and ONR Award N00014-14-1-0665.

A Tensor Notation

In this Section, we provide the additional tensor notation required for the analysis provided in the supplementary material. Multilinear form: The multilinear form for a tensor T ∈ R q 1 ×q 2 ×q 3 is defined as follows. Consider matrices M r ∈ R qr×pr , r ∈ {1, 2, 3}. Then tensor T (M 1 , M 2 , M 3 ) ∈ R p 1 ×p 2 ×p 3 is defined as T (M 1 , M 2 , M 3 ) := j 1 ∈[q 1 ] j 2 ∈[q 2 ] j 3 ∈[q 3 ] T j 1 ,j 2 ,j 3 · M 1 (j 1 , :) ⊗ M 2 (j 2 , :) ⊗ M 3 (j 3 , :). (18) As a simpler case, for vectors u, v, w ∈ R d , we have 9 T (I, v, w) := j,l∈[d] v j w l T (:, j, l) ∈ R d , (19) which is a multilinear combination of the tensor mode-1 fibers.

B Details of Tensor Decomposition Algorithm

The goal of tensor decomposition algorithm is to recover the rank-1 components of tensor; refer to Equation (2) for the notion of tensor rank and its rank-1 components. We exploit the tensor decomposition algorithm proposed in (Anandkumar et al., 2014b,c). Figure 2 depicts the flowchart of this method where the corresponding algorithms and procedures are also specified. Similarly, Algorithm 3 states the high-level steps of tensor decomposition algorithm. The main step of the tensor decomposition method is the tensor power iteration which is the generalization of matrix power iteration to 3rd order tensors. The tensor power iteration is given by u ← T (I, u, u) T (I, u, u) , where u ∈ R d , T (I, u, u) := j,l∈[d] u j u l T (:, j, l) ∈ R d is a multilinear combination of tensor fibers. Note that tensor fibers are the vectors which are derived by fixing all the indices of the tensor except one of them, e.g., T (:, j, l) in the above expansion. The initialization for different runs of tensor power iteration is performed by the SVD-based technique proposed in Procedure 5. This helps to initialize non-convex tensor power iteration with good initialization vectors. The whitening preprocessing is applied to orthogonalize the components of input tensor. Note that the convergence guarantees of tensor power iteration for orthogonal tensor decomposition have been developed in the literature (Zhang and Golub, 2001; Anandkumar et al., 2014b). The tensorization step works as follows. Tensorization: The tensorizing step is applied when we want to decompose overcomplete tensors where the rank k is larger than the dimension d. For instance, for getting rank up to k = O(d 2 ), we first form the 6th order input tensor with decomposition as T = j∈[k] λ j a ⊗6 j ∈ 6 R d . Given T , we form the 3rd order tensor˜T ∈ 3 R d 2 which is the tensorization of T such that ˜ T i 2 + d(i 1 − 1), j 2 + d(j 1 − 1), l 2 + d(l 1 − 1) := T (i 1 , i 2 , j 1 , j 2 , l 1 , l 2 ). (20) Input: Tensor T = i∈[k] λ i u Output: {u i } i∈[k] Figure 2: Overview of tensor decomposition algorithm for third order tensor (without tensorization). Algorithm 3 Tensor Decomposition Algorithm Setup input symmetric tensor T . 1: if Whitening then 2: Calculate T =Whiten(T ); see Procedure 4. 3: else if Tensorizing then 4: Tensorize the input tensor. 5: Calculate T =Whiten(T ); see Procedure 4. 6: end if 7: for j = 1 to k do 8: (u j , λ j , T ) = tensor power decomposition(T ); see Algorithm 6. 9: end for 10: return {u j , λ j } j∈[k] . This leads tõ T having decomposition ˜ T = j∈[k] λ j (a j ⊙ a j ) ⊗3 . We then apply the tensor decomposition algorithm to this new tensor˜T . This now clarifies why the full column rank condition is applied to the columns of A ⊙ A = [a 1 ⊙ a 1 · · · a k ⊙ a k ]. Similarly, we can perform higher order tensorizations leading to more overcomplete models by exploiting initial higher order tensor T ; see also Remark 2. Efficient implementation of tensor decomposition given samples: The main update steps in the tensor decomposition algorithm is the tensor power iteration for which a multilinear operation is performed on tensor T . However, the tensor is not available beforehand, and needs to be estimated using the samples (as in Algorithm 1 in the main text). Computing and storing the tensor can be enormously expensive for high-dimensional problems. But, it is essential to note that since we can form a factor form of tensor T using the samples and other parameters in the model, we can manipulate the samples directly to perform the power update as multi-linear operations without explicitly forming the tensor. This leads to efficient computational complexity. See (Anandkumar et al., 2014a) for details on these implicit update forms. Procedure 4 Whitening

input

Tensor T ∈ R d×d×d . 1: Draw a random standard Gaussian vector θ ∼ N (0, I d ). 2: Compute M 2 = T (I, I, θ) ∈ R d×d . 3: Compute the rank-k SVD M 2 = U Diag( λ) U ⊤ . 4: Compute the whitening matrix W = U Diag( λ −1/2 ). 5: return T W , W , W . Procedure 5 SVD-based initialization when k = O(d) (Anandkumar et al., 2014c) input Tensor T ∈ R d×d×d . 1: for τ = 1 to log(1/ ˆ δ) do

2:

Draw a random standard Gaussian vector θ (τ ) ∼ N (0, I d ). 3: Compute u (τ ) 1 as the top left singular vector of T (I, I, θ (τ ) ) ∈ R d×d . 4: end for 5: ˆ a 0 ← max τ ∈[log(1/ ˆ δ)] u (τ ) 1 min . 6: returnâ 0 .

C Proof of Theorem 1

Proof of Theorem 1 includes three main pieces. As the first piece, we show that the tensor decomposition algorithm for estimating weight matrix A 1 (see Algorithm 1 for the details) recovers it with the desired error. In the second part, we analyze the performance of Fourier technique for estimating weight vector a 2 , and bias vector b 1 (see Algorithm 1 and Procedure 2 for the details) proving the error in the recovery is small. Finally as the last step, the error in the recovery of these parameters is translated to the error in estimating the overall function f (x) using the Lipschitz property of the nonlinear activating function σ(·). In the following lemma, we state the last step described above which leads to the final result in Theorem 1. The analysis for the first two steps are provided in the following subsections. Lemma 7. Given the parameter recovery results in Lemmata 8 and 10 on A 1 , a 2 , b 1 , and assuming the Lipschitz property on nonlinear activating function σ(·) such that |σ(u) − σ(u ′ )| ≤ L · |u − u ′ |, for u, u ′ ∈ R, we have |f (x) − ˆ f (x)| ≤ O(ǫ). Proof: Expanding f (x) andˆf (x), and exploiting triangle and Cauchy-Schwartz inequalities, we Algorithm 6 Robust tensor power method (Anandkumar et al., 2014b) input symmetric tensor˜T ∈ R d×d×d , number of iterations N , number of initializations L. output the estimated eigenvector/eigenvalue pair; the deflated tensor. 1: for τ = 1 to L do 2: Initializê a (τ ) 0 with SVD-based method in Procedure 5. 3: for t = 1 to N do

4:

Compute power iteration update ˆ a (τ ) t := ˜ T (I, ˆ a (τ ) t−1 , ˆ a (τ ) t−1 ) ˜ T (I, ˆ a (τ ) t−1 , ˆ a (τ ) t−1 ) (21) 5: end for 6: end for 7: Let τ * := arg max τ ∈[L] { ˜ T (ˆ a (τ ) N , ˆ a (τ ) N , ˆ a (τ ) N )}. 8: Do N power iteration updates (21) starting fromâ (τ * ) N to obtainâ , and setˆλ := ˜ T (ˆ a, ˆ a, ˆ a). 9: return the estimated eigenvector/eigenvalue pair (ˆ a, ˆ λ); the deflated tensor˜T − ˆ λ ˆ a ⊗3 . have |f (x) − ˆ f (x)| = |a 2 , σ(A ⊤ 1 x + b 1 ) + b 2 − ˆ a 2 , σ( ˆ A ⊤ 1 x + ˆ b 1 ) − ˆ b 2 | ≤ |a 2 , σ(A ⊤ 1 x + b 1 ) + b 2 − ˆ a 2 , σ(A ⊤ 1 x + b 1 ) − b 2 | + |â 2 , σ(A ⊤ 1 x + b 1 ) + b 2 − ˆ a 2 , σ( ˆ A ⊤ 1 x + ˆ b 1 ) − ˆ b 2 | ≤ |a 2 − ˆ a 2 , σ(A ⊤ 1 x + b 1 )| + |â 2 , σ(A ⊤ 1 x + b 1 ) − σ( ˆ A ⊤ 1 x + ˆ b 1 )| + |b 2 − ˆ b 2 | ≤ a 2 − ˆ a 2 · σ(A ⊤ 1 x + b 1 ) + ˆ a 2 · σ(A ⊤ 1 x + b 1 ) − σ( ˆ A ⊤ 1 x + ˆ b 1 ) For the first term, we have a 2 − ˆ a 2 · σ(A ⊤ 1 x + b 1 ) ≤ √ k |Ω l | |Σ(1/2)| O(˜ ǫ) · √ k = k |Ω l | |Σ(1/2)| O(˜ ǫ), where we use the bound proved in Lemma 10, and assuming the output of sigmoidal function σ(·) is bounded in [0, 1]. For the second term, using the Lipschitz property of nonlinear function σ(·), we have |σ((A 1 ) l , x + b 1 (l)) − σ(( ˆ A 1 ) l , x + ˆ b 1 (l))| ≤ L · |(A 1 ) l − ( ˆ A 1 ) l , x| + |b 1 (l) − ˆ b 1 (l)| ≤ L · rO(˜ ǫ) + |Ω l | π|Σ(1/2)||a 2 (l)| O(˜ ǫ) = L · r + |Ω l | π|Σ(1/2)||a 2 (l)| O(˜ ǫ), where in the second inequality, we use the bounds in Lemmata 8 and 10, and bounded x such that x ≤ r. Combining all these bounds we have |f (x) − ˆ f (x)| ≤ k |Ω l | |Σ(1/2)| + Lr √ ka 2 + L √ ka 2 |Ω l | π|Σ(1/2)| · min l∈[k] |a 2 (l)| O(˜ ǫ) =: O(ǫ).

C.1 Tensor decomposition guarantees

We first provide a short proof for Lemma 4 which shows how the rank-1 components of third order tensor E [y · S 3 (x)] are the columns of weight matrix A 1 . Proof of Lemma 4: It is shown by Janzamin et al. (2014) that the score function yields differential operator such that for label-function f (x) := E[y|x] , we have E[y · S 3 (x)] = E[∇ (3) x f (x)]. Applying this property to the form of label function f (x) in (4), we have E [y · S 3 (x)] = E[σ ′′′ (·)(a 2 , A ⊤ 1 , A ⊤ 1 , A ⊤ 1 )], where σ ′′′ (·) denotes the third order derivative of element-wise function σ(z) : R k → R k . More concretely, with slightly abuse of notation, σ ′′′ (z) ∈ R k×k×k×k is a diagonal 4th order tensor with its j-th diagonal entry equal to ∂ 3 σ(z j ) ∂z 3 j : R → R. Here two properties are used to compute the third order derivative ∇ (3) x f (x) on the R.H.S. of above equation as follows. 1) We apply chain rule to take the derivatives which generates a new factor of A 1 for each derivative. Since we take 3rd order derivative, we have 3 factors of A 1 . 2) The linearity of next layers leads to the derivatives from them being vanished, and thus, we only have the above term as the derivative. Expanding the above multilinear form finishes the proof; see (18) for the definition of multilinear form. We now provide the recovery guarantees of weight matrix A 1 through tensor decomposition as follows. Lemma 8. Among the conditions for Theorem 1, consider the rank constraint on A 1 , and the non-vanishing assumption on coefficients λ j 's. Suppose the sample complexity n ≥ max˜O ζ f ψ˜ǫ 2 2 , ˜ O E M 3 (x)M ⊤ 3 (x) λ 2 min · s 3 min (A 1 ) d 1.5 k

3

, ˜ O λ max · E M 3 (x)M ⊤ 3 (x) λ 3 min · s 6 min ( A 1 ) k 4 , where M 3 (x) ∈ R d×d 2 denotes the matricization of score function tensor S 3 (x) ∈ R d×d×d . Then the estimationˆA 1 by NN-LIFT Algorithm 1 satisfies w.h.p. (A 1 ) j − ( ˆ A 1 ) j ≤ O 1 λ min d 0.75 √ n , j ∈ [k] . Proof: From Lemma 4, we know that the exact cross-moment E[y · S 3 (x)] has rank-one components as columns of matrix A 1 ; see Equation (10) for the tensor decomposition form. Thus given the exact moment, the theorem is proved by applying the tensor decomposition guarantees in Anandkumar et al. (2014b). For the noisy case, we use the analysis and results of tensor power iteration in Anandkumar et al. (2014c). We only need to adapt the perturbation analysis of Anandkumar et al. (2014c) to our additional whitening procedure. This is done in the rest of this section. Furthermore, note that the above lemma only concerns about the estimation bound in the realizable setting, and thus there is no need to consider the approximation error. The final sample complexity bound is from combining analysis of estimation bound in Appendix C.1.3 and Lemma 9. The perturbation analysis of proposed tensor decomposition method in Algorithm 6 with the corresponding SVD-based initialization in Procedure 5 is provided in Anandkumar et al. (2014c). But, the effect of whitening with random slices of tensor as proposed in Procedure 4 is not considered there. Thus, we need to adapt the perturbation analysis of Anandkumar et al. (2014c) when the whitening procedure is incorporated as follows. Perturbation decomposition to approximation and estimation parts: The main part of the tensor decomposition analysis is bounding the perturbation term. Consider the general case when function f (x) is not necessarily the output of neural network, and thus, the approximation analysis is also involved. Then, there are two perturbation sources in the tensor analysis. One is from the approximation part and the other is from the estimation part. In this section we use y to denote the arbitrary function we want to approximate, and we exploitˆy to denote the approximation given by the output of neural network. By Lemma 4, the CP decomposition form is given by T = E[ˆ y · S 3 (x)], and thus, the perturbation tensor is written as E := T − T = E[ˆ y · S 3 (x)] − 1 n i∈[n] y i · S 3 (x i ), where T = 1 n i∈[n] y i · S 3 (x i ) is the empirical form used in NN-LIFT Algorithm 1. This error can be expanded as E := E[ˆ y · S 3 (x)] − E[y · S 3 (x)] :=Eapx. + E[y · S 3 (x)] − 1 n i∈[n] y i · S 3 (x i ) :=Eest. , where E apx. and E est. respectively denote the perturbations from approximation and estimation parts. Notice the difference in notation of y andˆy as explained above. It is also worth mentioning that in Theorem 1 where there is no approximation effect, we only need to analyze the estimation perturbation as E[ˆ y · S 3 (x)] − 1 n i∈[n] ˆ y i · S 3 (x i ) where here we usê y since the neural network output is analyzed. In the following subsections we first analyze the required perturbation bound for tensor decomposition to work, and then we bound the corresponding approximation and estimation perturbation terms. C.1.

Required perturbation bound

The perturbation analysis of proposed tensor decomposition method in Algorithm 6 with the corresponding SVD-based initialization in Procedure 5 is provided in Anandkumar et al. (2014c). But, the effect of whitening with random slices of tensor as proposed in Procedure 4 is not considered there. Thus, we need to adapt the perturbation analysis of Anandkumar et al. (2014c) when the whitening procedure is incorporated as follows. Lemma 9. By applying the whitening Procedure 4, the perturbation analysis in Anandkumar et al. (2014c) needs to be adapted as follows. If the perturbation tensor satisfies the bound E ≤ min O λ min · s 1.5 min (A 1 ) 1 k 1.5 d 0.75 , O λ min λ min λ max s 3 min (A 1 ) √ log k k 2 , then w.h.p., the same guarantees as in Anandkumar et al. (2014c) holds for the proposed tensor decomposition algorithm.

Proof:

For this, we follow the steps of whitening procedure as follows. We do not have access to the true tensor T = j∈[k] λ j · (A 1 ) j ⊗ (A 1 ) j ⊗ (A 1 ) j , and the perturbed version T = T − E is fed to the whitening procedure. The matrix M 2 = T (I, I, θ) is then expanded as M 2 = T (I, I, θ) = T (I, I, θ) − E(I, I, θ) = A 1 Diag(λ) Diag(u) Diag( ˜ λ) A ⊤ 1 − E(I, I, θ), where u = A ⊤ 1 θ, and hence, u ∼ N (0, A ⊤ 1 A 1 ). Notice that the proposed whitening matrix W such that W ⊤ M 2 W = I orthogonalizes the noisy matrix M 2 (see above). Assume whitening matrix W orthogonalizes non-noisy tensor T as T (W, W, W ) = j∈[k] µ j v ⊗3 j , where the new factor matrix V = [v 1 v 2 · · · v k ] ∈ R k×k is orthogonal such that V ⊤ V = I k . The coefficients µ j are also related to the original coefficients λ j as µ j = λ j W ⊤ (A 1 ) j 3 = λ j (λ j u j ) 3/2 = 1 λ j u 3/2 j , (22) where the first equality is from the normalization of columns of V , and the second equality is from W ⊤ A 1 Diag˜λ 1/2 = I, and thus, W ⊤ (A 1 ) j = 1/ ˜ λ j . Given this discussion, the output of whitening procedure can be expanded as T ( W , W , W ) = T (W, W, W ) − T (W − W , W − W , W − W ) − E( W , W , W ) = j∈[k] µ j v ⊗3 j − E W , where E W := T (W − W , W − W , W − W ) + E( W , W , W ) (23) is the perturbation tensor after whitening. Note that the perturbation is from two sources; one is the error in the tensor T , and the other is from the error in computing whitening matrix as in W − W . Anandkumar et al. (2014c) show that the tensor power iteration applied to the whitened tensor recovers the tensor components when E W ≤ µ min √ log k α 0 √ k , for some constant α 0 > 1. From (22), we have µ min = 1 √ λmaxu 3/2 max We now relate the norm of E W to the norm of original perturbation E. For the first term in (23), from Lemmata 4 and 5 of Song et al. (2013), we have T (W − W , W − W , W − W ) ≤ 64E(I, I, θ) 3 ˜ λ 3.5 min · s 6 min (A 1 ) ≤ 64E 3 θ 3 ˜ λ 3.5 min · s 6 min (A 1 ) , where we used sub-multiplicative property in the second inequality. For the second term, by the sub-multiplicative property we have E( W , W , W ) ≤ E · W 3 ≤ 8E · W 3 = 8E s 3 min (A 1 )λ 3/2 min u 3/2 min , where we used the equality W = s 2 min (A 1 )λ min u min −1/2 . Thus, we finally need the condition 64E 3 θ 3 λ 3.5 min u 4.5 min · s 6 min (A 1 ) + 8E λ 1.5 min u 1.5 min · s 3 min (A 1 ) ≤ √ log k α 0 √ k √ λ max u 1.5 max . The first bound assumed in the lemma ensures the second perturbation term is dominant. In addition, since θ ∼ N (0, I d ), its norm is w.h.p. bounded as θ ≤ O( √ d). The second bound assumed in the lemma is from the above inequality. Furthermore, we also used the bounds on Gaussian vector u ∼ N (0, A ⊤ A) as follows. Using the anti-concentration of Gaussian random variables, we have Pr[|u j | ≤ ǫ] ≤ Cǫ for some constant C; see Lovett (2010). Now if we choose ǫ = O(1/k), and apply the union bound on all entries of u, we have u min ≥ 1 k with some constant probability. Doing log(1/ ˆ δ) independent trials, we get lower bound u min ≥ 1 k with probability at least 1 − ˆ δ. For the upper bound we have u max ≤ ˜ O(1). C.1.2 Approximation perturbation bound The norm of approximation perturbation E apx. := E[(ˆ y − y) · S 3 (x)] is bounded as E apx. = E[(ˆ y − y) · S 3 (x)] ≤ E[(ˆ y − y) · S 3 (x)] = E[|ˆy − y| · S 3 (x)] ≤ E[|ˆy − y| 2 ] · E[S 3 (x) 2 ] 1/2 , where the first inequality is from the Jensen's inequality applied to convex norm function, and we used Cauchy-Schwartz in the last inequality. By the approximation bound in Theorem 2, we have E[|y − ˆ y| 2 ] ≤ O(r 2 C 2 f ) · 1 √ k + δ 1 2 . C.1.3 Estimation perturbation bound The estimation error E est. := E[y · S 3 (x)] − 1 n i∈[n] y i · S 3 (x i ) is matricized as ˜ E est. := E[y · M 3 (x)] − 1 n i∈[n] y i · M 3 (x i ) = i∈[n] 1 n (E[y · M 3 (x)] − y i · M 3 (x i )) , where M 3 (x) ∈ R d ×d 2 is the matricization of S 3 (x) ∈ R d×d×d . Now the norm of˜E est. can be bounded by the matrix Bernstein's inequality. The norm of each (centered) random variable is bounded as ymax n M 3 (x), where y max is the bound on arbitrary function y. The variance term is also bounded as 1 n 2 i∈[n] E y 2 i · M 3 (x i )M ⊤ 3 (x i ) ≤ 1 n y 2 max E M 3 (x)M ⊤ 3 (x) . C.2 Fourier analysis guarantees The analysis of Fourier method for estimating parameters a 2 and b 1 includes the following two lemmas. In the first lemma, we argue the mean of random variable v introduced in Algorithm 1. This clarifies why the magnitude and phase of v are related to unknown parameters a 2 and b 1 . In the second lemma, we argue the concentration of v around its mean leading to the sample complexity result. Lemma 5 (Restated). When ω i 's are uniformly i.i.d. drawn from set Ω l , then the variable v := 1 n i∈[n] y i p(x i ) e −jω i ,x i , (24) has mean (which is computed over x, y and ω) E[v] = 1 |Ω l | Σ 1 2 a 2 (l)e jπb 1 (l) , (25) where |Ω l | denotes the surface area of d − 1 dimensional manifold Ω l , and Σ(·) denotes the Fourier transform of σ(·). Proof: Let F (ω) denote the Fourier transform of label function f (x) := E[y|x] = a 2 , σ(A ⊤ 1 x + b 1 ) which is (Marks II and Arabshahi, 1994) F (ω) = j∈[k] a 2 (j) |A 1 (d, j)| Σ ω d A 1 (d, j) e j2πb 1 (j) ω d A 1 (d,j) δ ω − − ω d A 1 (d, j) A 1 (\d, j) , (26) where Σ(·) is the Fourier transform of σ(·), u ⊤ − = [u 1 , u 2 , . . . , u d−1 ] is vector u ⊤ with the last entry removed, A 1 (\d, j) ∈ R d−1 is the j-th column of matrix A 1 with the d-th (last) entry removed, and finally δ(u) = δ(u 1 )δ(u 2 ) · · · δ(u d ). Let p(ω) and p(η) respectively denote the probability density function of frequency ω and noise η where y = f (x) + η such that η is the noise which is dependent on input x to make the output a binary label as y ∈ {0, 1}. We have E[v] = E x,y,ω y p(x) e −jω,x = Ω l (f (x) + η)e −jω,x p(η)p(ω)dxdηdω, = Ω l f (x)e −jω,x p(ω)dxdω = Ω l F (ω)p(ω)dω, where the second equality uses the label generating model in (4), the third equality uses the zeromean property of noise η, and final equality is from the definition of Fourier transform. The variable ω ∈ R d is drawn from a d−1 dimensional manifold Ω l ⊂ R d . In order to compute the above integral, we define d dimensional set Ω l;ν := ω ∈ R d : 1 2 − ν 2 ≤ ω ≤ 1 2 + ν 2 , ω, ( ˆ A 1 ) l ≥ 1 − ˜ ǫ 2 /2 2 , for which Ω l = lim ν→0 + Ω l;ν . Assuming ω's are uniformly drawn from Ω l;ν , we have E[v] = lim ν→0 + Ω l ;ν F (ω)p(ω)dω = lim ν→0 + 1 |Ω l;ν | +∞ −∞ F (ω)1 Ω l;ν (ω)dω. The second equality is from uniform draws of ω from set Ω l;ν such that p(ω) =

1

|Ω l;ν | 1 Ω l;ν (ω), where 1 S (·) denotes the indicator function for set S. Here, |Ω l;ν | denotes the volume of d dimensional subspace Ω l;ν , for which in the limit ν → 0 + , we have |Ω l;ν | = ν · |Ω l |, where |Ω l | denotes the surface area of d − 1 dimensional manifold Ω l . For small enough˜ǫ in the definition of Ω l;ν , only the delta function for j = l in the expansion of F (ω) in (26) is survived from the above integral, and thus, E[v] = lim ν→0 + 1 |Ω l;ν | +∞ −∞ a 2 (l) |A 1 (d, l)| Σ ω d A 1 (d, l) e j2πb 1 (l) ω d A 1 (d,l) δ ω − − ω d A 1 (d, l) A 1 (\d, l) 1 Ω l;ν (ω)dω. In order to simplify the notations, in the rest of the proof we denote l-th column of matrix A 1 by vector α, i.e., α := (A 1 ) l . Thus, the goal is to compute the integral I := +∞ −∞ 1 |α d | Σ ω d α d e j2πb 1 (l) ω d α d δ ω − − ω d α d α − 1 Ω l;ν (ω)dω, and note that E[v] = a 2 (l) · lim ν→0 + I |Ω l;ν | . The rest of the proof is about computing the above integral. The integral involves delta functions where the final value is expected to be computed at a single point specified by the intersection of line ω − = ω d α d α − , and sphere ω = 1 2 (when we consider the limit ν → 0 + ). This is based on the following integration property of delta functions such that for function g(·) : R → R, +∞ −∞ g(t)δ(t)dt = g(0). (27) We first expand the delta function as follows. I = +∞ −∞ 1 |α d | Σ ω d α d e j2πb 1 (l) ω d α d δ ω 1 − α 1 α d ω d · · · δ ω d−1 − α d−1 α d ω d 1 Ω l;ν (ω)dω, = · · · +∞ −∞ Σ ω d α d e j2πb 1 (l) ω d α d δ ω 1 − α 1 α d ω d · · · δ ω d−2 − α d−2 α d ω d 1 Ω l;ν (ω) · δ (α d ω d−1 − α d−1 ω d ) dω 1 · · · ω d , where we used the property 1 |β| δ(t) = δ(βt) in the second equality. Introducing new variable z, and applying the change of variable ω d = 1 α d−1 (α d ω d−1 − z), we have I = · · · +∞ −∞ Σ ω d α d e j2πb 1 (l) ω d α d δ ω 1 − α 1 α d ω d · · · δ ω d−2 − α d−2 α d ω d 1 Ω l;ν (ω) · δ(z)dω 1 · · · dω d−1 dz α d−1 , = · · · +∞ −∞ 1 α d−1 Σ ω d−1 α d−1 e j2πb 1 (l) ω d−1 α d−1 δ ω 1 − α 1 α d−1 ω d−1 · · · δ ω d−2 − α d−2 α d−1 ω d−1 1 Ω l;ν ω 1 , ω 2 , . . . , ω d−1 , α d α d−1 ω d−1 dω 1 · · · dω d−1 . For the sake of simplifying the mathematical notations, we did not substitute all the ω d 's with z in the first equality, but note that all ω d 's are implicitly a function of z which is finally considered in the second equality where the delta integration property in (27) is applied to variable z (note that z = 0 is the same as ω d α d = ω d−1 α d−1 ). Repeating the above process several times, we finally have I = +∞ −∞ 1 α 1 Σ ω 1 α 1 e j2πb 1 (l) ω 1 α 1 · 1 Ω l;ν ω 1 , α 2 α 1 ω 1 , . . . , α d−1 α 1 ω 1 , α d α 1 ω 1 dω 1 . There is a line constraint as ω 1 α 1 = ω 2 α 2 = · · · = ω d α d in the argument of indicator function. This implies that ω = α α 1 ω 1 = ω 1 α 1 , where we used α = (A 1 ) l = 1. Incorporating this in the norm bound imposed by the definition of Ω l;ν , we have α 1 2 (1 − ν) ≤ ω 1 ≤ α 1 2 (1 + ν), and hence, I = α 1 2 (1+ν) α 1 2 (1−ν) 1 α 1 Σ ω 1 α 1 e j2πb 1 (l) ω 1 α 1 dω 1 . We know E[v] = a 2 (l) · lim ν→0 + I |Ω l;ν | , and thus, E[v] = a 2 (l) · 1 ν · |Ω l | · α 1 ν 1 α 1 Σ 1 2 e j2πb 1 (l) 1 2 = 1 |Ω l | a 2 (l)Σ 1 2 e jπb 1 (l) , where in the first step we use |Ω l;ν | = ν · |Ω l |, and write the integral I in the limit ν → 0 + . This finishes the proof. In Algorithm 1, we exploit the magnitude and phase of v to estimate the weight vector a 2 , and bias vector b 1 . In the following lemma, we argue the concentration of v which leads to the sample complexity bound for estimating the parameters a 2 and b 1 within the desired error. Lemma 10. If the sample complexity n ≥ O ζ f ψ˜ǫ 2 log k δ holds for small enough˜ǫ ≤ ζ f , then the estimatesâ 2 (l) = |Ω l | |Σ(1/2)| |v|, andˆb 1 (l) = 1 π (∠v − ∠Σ(1/2)) for l ∈ [k], in NN-LIFT Algorithm 1 (see the definition of v in (24)) satisfy with probability at least 1 − δ, Proof: The result is proved by arguing the concentration of variable v in (24) around its mean characterized in (25). We use the Bernstein's inequality to do this. Let v := i∈[n] v i where v i = 1 n y i p(x i ) e −jω i ,x i . By the lower bound p(x) ≥ ψ assumed in Theorem 1 and labels y i 's being bounded, the magnitude of centered v i 's (v i − E[v i ]) are bounded by O( 1 ψn ). The variance term is also bounded as σ 2 = i∈[n] E (v i − E[v i ])(v i − E[v i ]) , where u denotes the complex conjugate of complex number u. This is bounded as σ 2 ≤ i∈[n] E [v i v i ] = 1 n 2 i∈[n] E y 2 i p(x i ) 2 Since output y is a binary label (y ∈ {0, 1}), we have E[y 2 |x] = E[y|x] = f (x), and thus, E y 2 p(x) 2 = E E y 2 p(x) 2 |x = E f (x) p(x) 2 ≤ 1 ψ +∞ −∞ f (x)dx = ζ f ψ , where the inequality uses the bound p(x) ≥ ψ and the last equality is from definition of ζ f . This provides us the bound on variance as σ 2 ≤ ζ f ψn . Applying Bernstein's inequality concludes the concentration bound such that with probability at least 1 − δ, we have |v − E[v]| ≤ O 1 ψn log 1 δ + ζ f ψn log 1 δ ≤ O(˜ ǫ), where the last inequality is from sample complexity bound. This implies that ||v| − |E[v]|| ≤ O(˜ ǫ). Substituting |E[v] which finishes the first part of the proof. For the phase, we have φ := ∠v − ∠E[v] = π( ˆ b 1 (l) − b 1 (l)). On the other hand, for small enough error˜ǫ (and thus small φ), we have the approximation φ ∼ tan(φ) ∼ |v−E[v] | |E[v] | (note that this is actually an upper bound such that φ ≤ tan(φ)). Thus, | ˆ b 1 (l) − b 1 (l)| ≤ 1 π|E[v] This finishes the proof of second bound. The bound on the recovery of a 2 can be also generalized to ℓ 2 error norm as a 2 − ˆ a 2 by applying the vector Bernstein's bound.