Word embeddings represent words in a language's vocabulary as points in a d-dimensional space such that nearby words (points) are similar in terms of their distributional properties. A variety of techniques for learning embeddings have been proposed, e.g., matrix factorization (Deerwester et al., 1990; Dhillon et al., 2011) and neural language modeling (Mikolov et al., 2011; Collobert and Weston, 2008). For the POS induction task, we specifically need embeddings that capture syntactic similarities. arXiv:1503.06760v1 [cs.CL] 23 Mar 2015 Therefore we experiment with two types of embeddings that are known for such properties: • Skip-gram embeddings (Mikolov et al., 2013) are based on a log bilinear model that predicts an unordered set of context words given a target word. Bansal et al. (2014) found that smaller context window sizes tend to result in embeddings with more syntactic information. We confirm this finding in our experiments. • Structured skip-gram embeddings (Ling et al., 2015) extend the standard skip-gram embeddings (Mikolov et al., 2013) by taking into account the relative positions of words in a given context. We use the tool word2vec 3 and Ling et al. (2015)'s modified version 4 to generate both plain and structured skip-gram embeddings in nine languages.
