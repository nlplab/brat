T1	METHOD 0 15	Word embeddings
T2	TASK 211 230	learning embeddings
T3	METHOD 257 277	matrix factorization
R1	addresses Arg1:T3 Arg2:T2	
T4	METHOD 330 354	neural language modeling
R2	addresses Arg1:T4 Arg2:T2	
T5	TASK 415 428	POS induction
T6	TASK 472 502	capture syntactic similarities
T7	METHOD 634 654	Skip-gram embeddings
T8	METHOD 693 711	log bilinear model
R3	uses Arg1:T7 Arg2:T8	
T9	TASK 717 725	predicts
R4	addresses Arg1:T8 Arg2:T9	
T10	METHOD 950 981	Structured skip-gram embeddings
T11	METHOD 1013 1042	standard skip-gram embeddings
R5	is_subclass_of Arg1:T11 Arg2:T10	
T12	SOFTWARE 1157 1165	word2vec
T14	TASK 1250 1270	skip-gram embeddings
R6	addresses Arg1:T12 Arg2:T14	
T13	TASK 16 31	represent words
R7	addresses Arg1:T1 Arg2:T13	
T15	METRIC 156 181	distributional properties
T16	METHOD 456 466	embeddings
R8	addresses Arg1:T16 Arg2:T6	
T17	METHOD 585 595	embeddings
