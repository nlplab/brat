<full-text-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/article/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:bk="http://www.elsevier.com/xml/bk/dtd"><coredata><prism:url>http://api.elsevier.com/content/article/pii/S2212667814001440</prism:url><dc:identifier>doi:10.1016/j.ieri.2014.09.096</dc:identifier><eid>1-s2.0-S2212667814001440</eid><prism:doi>10.1016/j.ieri.2014.09.096</prism:doi><pii>S2212-6678(14)00144-0</pii><dc:title>Human-robot Communication for Surveillance of Elderly People in Remote Distance </dc:title><prism:publicationName>IERI Procedia</prism:publicationName><prism:aggregationType>Journal</prism:aggregationType><prism:issn>22126678</prism:issn><prism:volume>10</prism:volume><prism:startingPage>92</prism:startingPage><prism:endingPage>97</prism:endingPage><prism:pageRange>92-97</prism:pageRange><dc:format>text/xml</dc:format><prism:coverDate>2014-12-31</prism:coverDate><prism:coverDisplayDate>2014</prism:coverDisplayDate><prism:copyright>Copyright © 2014 The Authors. Published by Elsevier B.V.</prism:copyright><prism:publisher>The Authors. Published by Elsevier B.V.</prism:publisher><prism:issueName>International Conference on Future Information Engineering (FIE 2014)</prism:issueName><dc:creator>Kaneko, Shin-ichiro</dc:creator><dc:creator>Capi, Genci</dc:creator><dc:description>AbstractIn this paper, we present a tele-operated mobile robot system for old age surveillance. The robot operates in autonomous mode in which the robots navigates in the environment and search for unusual situation of elderly people. If a patient is lying on the floor, the robot informs the user. The user switches the control mode from autonomous to haptic based user control. In the autonomous mode, the robot utilizes the visual sensor and landmarks to monitor the entire environment. The robot is equipped microphone, speaker and monitor making it possible to communicate with the user in remote place. In addition, the robot utilizes the vital sensors to check the patient's condition. The preliminary surveillance experiments show a good performance.</dc:description><openaccess>1</openaccess><openaccessArticle>true</openaccessArticle><openaccessType>Full</openaccessType><openArchiveArticle>false</openArchiveArticle><openaccessSponsorName/><openaccessSponsorType>ElsevierWaived</openaccessSponsorType><openaccessUserLicense>http://creativecommons.org/licenses/by-nc-nd/3.0/</openaccessUserLicense><dcterms:subject>mobile robot</dcterms:subject><dcterms:subject>old age surveillance</dcterms:subject><dcterms:subject>remote control;</dcterms:subject><link rel="self" href="http://api.elsevier.com/content/article/pii/S2212667814001440"/><link rel="scidir" href="http://www.sciencedirect.com/science/article/pii/S2212667814001440"/></coredata><originalText><xocs:doc xmlns:xoe="http://www.elsevier.com/xml/xoe/dtd" xsi:schemaLocation="http://www.elsevier.com/xml/xocs/dtd http://be-prod3a/schema/dtds/document/fulltext/xcr/xocs-article.xsd"><xocs:meta><xocs:content-family>serial</xocs:content-family><xocs:content-type>JL</xocs:content-type><xocs:cid>282178</xocs:cid><xocs:ssids><xocs:ssid type="alllist">291210</xocs:ssid><xocs:ssid type="subj">291773</xocs:ssid><xocs:ssid type="subj">291800</xocs:ssid><xocs:ssid type="subj">291880</xocs:ssid><xocs:ssid type="subj">291882</xocs:ssid><xocs:ssid type="content">31</xocs:ssid><xocs:ssid type="oa">90</xocs:ssid></xocs:ssids><xocs:srctitle>IERI Procedia</xocs:srctitle><xocs:normalized-srctitle>IERIPROCEDIA</xocs:normalized-srctitle><xocs:orig-load-date yyyymmdd="20141001">2014-10-01</xocs:orig-load-date><xocs:available-online-date yyyymmdd="20141001">2014-10-01</xocs:available-online-date><xocs:ew-transaction-id>2014-10-10T04:56:58</xocs:ew-transaction-id><xocs:eid>1-s2.0-S2212667814001440</xocs:eid><xocs:pii-formatted>S2212-6678(14)00144-0</xocs:pii-formatted><xocs:pii-unformatted>S2212667814001440</xocs:pii-unformatted><xocs:doi>10.1016/j.ieri.2014.09.096</xocs:doi><xocs:item-stage>S300</xocs:item-stage><xocs:item-version-number>S300.2</xocs:item-version-number><xocs:item-weight>HEAD-AND-TAIL</xocs:item-weight><xocs:hub-eid>1-s2.0-S2212667814X00067</xocs:hub-eid><xocs:timestamp yyyymmdd="20150515">2015-05-15T07:34:50.756359-04:00</xocs:timestamp><xocs:dco>0</xocs:dco><xocs:tomb>0</xocs:tomb><xocs:date-search-begin>20140101</xocs:date-search-begin><xocs:date-search-end>20141231</xocs:date-search-end><xocs:year-nav>2014</xocs:year-nav><xocs:indexeddate epoch="1412186304">2014-10-01T17:58:24.910601Z</xocs:indexeddate><xocs:articleinfo>rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist webpdf webpdfpagecount yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref</xocs:articleinfo><xocs:issns><xocs:issn-primary-formatted>2212-6678</xocs:issn-primary-formatted><xocs:issn-primary-unformatted>22126678</xocs:issn-primary-unformatted></xocs:issns><xocs:crossmark is-crossmark="1">true</xocs:crossmark><xocs:vol-first>10</xocs:vol-first><xocs:volume-list><xocs:volume>10</xocs:volume></xocs:volume-list><xocs:suppl>C</xocs:suppl><xocs:vol-iss-suppl-text>Volume 10</xocs:vol-iss-suppl-text><xocs:sort-order>15</xocs:sort-order><xocs:first-fp>92</xocs:first-fp><xocs:last-lp>97</xocs:last-lp><xocs:pages><xocs:first-page>92</xocs:first-page><xocs:last-page>97</xocs:last-page></xocs:pages><xocs:cover-date-orig><xocs:start-date>2014</xocs:start-date></xocs:cover-date-orig><xocs:cover-date-text>2014</xocs:cover-date-text><xocs:cover-date-start>2014-01-01</xocs:cover-date-start><xocs:cover-date-end>2014-12-31</xocs:cover-date-end><xocs:cover-date-year>2014</xocs:cover-date-year><xocs:title-editors-groups><xocs:title-editors-group><ce:title>International Conference on Future Information Engineering (FIE 2014)</ce:title><ce:editors><ce:author-group><ce:author><ce:given-name>Garry</ce:given-name><ce:surname>Lee</ce:surname></ce:author></ce:author-group></ce:editors></xocs:title-editors-group></xocs:title-editors-groups><xocs:hub-sec><xocs:hub-sec-title>Information Engineering</xocs:hub-sec-title></xocs:hub-sec><xocs:document-type>article</xocs:document-type><xocs:document-subtype>fla</xocs:document-subtype><xocs:copyright-line>Copyright © 2014 The Authors. Published by Elsevier B.V.</xocs:copyright-line><xocs:normalized-article-title>HUMANROBOTCOMMUNICATIONFORSURVEILLANCEELDERLYPEOPLEINREMOTEDISTANCE</xocs:normalized-article-title><xocs:normalized-first-auth-surname>KANEKO</xocs:normalized-first-auth-surname><xocs:normalized-first-auth-initial>S</xocs:normalized-first-auth-initial><xocs:references><xocs:ref-info refid="oref0005"/><xocs:ref-info refid="sbref0010"><xocs:ref-normalized-surname>MATSUI</xocs:ref-normalized-surname><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-first-fp>351</xocs:ref-first-fp><xocs:ref-last-lp>356</xocs:ref-last-lp><xocs:ref-normalized-initial>T</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0015"><xocs:ref-normalized-surname>ODA</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>1075</xocs:ref-first-fp><xocs:ref-last-lp>1076</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0020"><xocs:ref-normalized-surname>SAWASHIMA</xocs:ref-normalized-surname><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-first-fp>6</xocs:ref-first-fp><xocs:ref-last-lp>10</xocs:ref-last-lp><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="oref0025"/></xocs:references><xocs:refkeys><xocs:refkey3>KANEKOX2014X92</xocs:refkey3><xocs:refkey4lp>KANEKOX2014X92X97</xocs:refkey4lp><xocs:refkey4ai>KANEKOX2014X92XS</xocs:refkey4ai><xocs:refkey5>KANEKOX2014X92X97XS</xocs:refkey5></xocs:refkeys><xocs:open-access><xocs:oa-article-status is-open-access="1" is-open-archive="0">Full</xocs:oa-article-status><xocs:oa-access-effective-date>2014-10-01T11:57:40Z</xocs:oa-access-effective-date><xocs:oa-sponsor><xocs:oa-sponsor-type>ElsevierWaived</xocs:oa-sponsor-type></xocs:oa-sponsor><xocs:oa-user-license>http://creativecommons.org/licenses/by-nc-nd/3.0/</xocs:oa-user-license><xocs:oa-access-inherited-from winid="http://vtw.elsevier.com/content/oaw/PROC_UNBOUNDED_ESWaived">OA-Window</xocs:oa-access-inherited-from></xocs:open-access><xocs:attachment-metadata-doc><xocs:attachment-set-type>item</xocs:attachment-set-type><xocs:pii-formatted>S2212-6678(14)00144-0</xocs:pii-formatted><xocs:pii-unformatted>S2212667814001440</xocs:pii-unformatted><xocs:eid>1-s2.0-S2212667814001440</xocs:eid><xocs:doi>10.1016/j.ieri.2014.09.096</xocs:doi><xocs:cid>282178</xocs:cid><xocs:timestamp>2014-10-10T10:10:04.774707-04:00</xocs:timestamp><xocs:cover-date-start>2014-01-01</xocs:cover-date-start><xocs:cover-date-end>2014-12-31</xocs:cover-date-end><xocs:attachments><xocs:web-pdf><xocs:attachment-eid>1-s2.0-S2212667814001440-main.pdf</xocs:attachment-eid><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2212667814001440/MAIN/application/pdf/0ed6f5c2352b247d26fa40396836e62a/main.pdf</xocs:ucs-locator><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2212667814001440/MAIN/application/pdf/0ed6f5c2352b247d26fa40396836e62a/main.pdf</xocs:ucs-locator><xocs:filename>main.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:pdf-optimized>true</xocs:pdf-optimized><xocs:filesize>1264934</xocs:filesize><xocs:web-pdf-purpose>MAIN</xocs:web-pdf-purpose><xocs:web-pdf-page-count>6</xocs:web-pdf-page-count><xocs:web-pdf-images><xocs:web-pdf-image><xocs:attachment-eid>1-s2.0-S2212667814001440-main_1.png</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2212667814001440/PREVIEW/image/png/c83b56c4922abb268aab61dcd27aee9e/main_1.png</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2212667814001440/PREVIEW/image/png/c83b56c4922abb268aab61dcd27aee9e/main_1.png</xocs:ucs-locator><xocs:filename>main_1.png</xocs:filename><xocs:extension>png</xocs:extension><xocs:filesize>45838</xocs:filesize><xocs:pixel-height>849</xocs:pixel-height><xocs:pixel-width>656</xocs:pixel-width><xocs:attachment-type>IMAGE-WEB-PDF</xocs:attachment-type><xocs:pdf-page-num>1</xocs:pdf-page-num></xocs:web-pdf-image></xocs:web-pdf-images></xocs:web-pdf></xocs:attachments></xocs:attachment-metadata-doc></xocs:meta><xocs:rawtext> IERI Procedia   10  ( 2014 )  92 â€“ 97  Available online at www.sciencedirect.com 2212-6678  2014 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license  (http://creativecommons.org/licenses/by-nc-nd/3.0/). Selection and peer review under responsibility of Information Engineering Research Institute doi: 10.1016/j.ieri.2014.09.096  ScienceDirect 2014 International Conference on Future Information Engineering  Human-Robot Communication for Surveillance of Elderly  People in Remote Distance  Shin-ichiro Kaneko a *, Genci Capi b   a National Institute of Technology Toyama, Department of Electrical and Control Systems Engineering, 13,Hongo-machi, Toyama, Japan  b University of Toyama, Department of Electrical and Electronic Systems Eng. Faculty of Engineering. 3190 Gofuku, Toyama, Japan    Abstract  In this paper, we present a tele-operated mobile robot system for old age surveillance. The robot operates in autonomous  mode in which the robots navigates in the environment and search for unusual situation of elderly people. If a patient is  lying on the floor, the robot informs the user. The user switches the control mode from autonomous to haptic based user  control. In the autonomous mode, the robot utilizes the visual sensor and landmarks to monitor the entire environment.  The robot is equipped microphone, speaker and monitor making it possible to communicate with the user in remote place.   In addition, the robot utilizes the vital sensors to check the patient's condition. The preliminary surveillance experiments  show a good performance.    Â© 2014.Published by Elsevier B.V.  Selection and peer review under responsibility of Information Engineering Research Institute    Keywords: mobile robot; old age surveillance; remote control;  1. Introduction  Recently, Japan is rapidly going to an aging society, and in 2015 the elderly population will make up  26.8% of the total, or one out of four people will be a senior person [1]. Simultaneously, the number of home      * Corresponding author. Tel.: +81-76-493-5451; fax: +81-76-493-5451.  E-mail address:skaneko@nc-toyama.ac.jp.   2014 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license  (http://creativecommons.org/licenses/by-nc-nd/3.0/). Selection and peer review under responsibility of Information Engineering Research Institute 93 Shin-ichiro Kaneko and Genci Capi /  IERI Procedia  10 ( 2014 )  92 â€“ 97  and/or visiting care service facilities is increasing. The care service utilization factor is more than 90%. On the  other hand the number of stuff working in these facilities is not sufficient. It has caused the increasing of work  load per person resulting in a very tiring job and low quality of care service. Therefore, application of  autonomous robots to replace staff work is highly required.  Development of robots for surveillance of elderly people has been presented in previous research works.  For example, Matsui et al. [2] developed a noncontact vital data monitoring system using microwaves for  bedridden old men. Oda et al. [3] developed a watching system for the elderly in room using image processing  and sensor embedded bed. However, these systems are available only in a single room, making it expensive to  be implemented in whole facility. On the other, Sawashima et al. [4] developed elderly people monitoring  system in care house using a mobile robot. The robot system can move from room to room autonomously.  However, in an emergency which the elderly lying down, the robot canâ€™t approach to check their  consciousness and vital conditions.  In this work, we propose a tele-operated mobile robot system for old age surveillance. Our system is  operated under the LAN/Internet environment in the facility. The robot surveys the environment  autonomously. In addition, the operator can grasp the remote environment using the robot camera image.  When the robot finds a patient lying on the floor, the robot notices an alert to the operator, and then changes  to the manual operation mode, automatically. In the manual mode, the operator can communicate with the  patient vie an interactive user interface, and check his vital conditions using the sensors attached to the robot.  The preliminary surveillance experiments show a good performance.  2. Developed System  Fig.1 shows a schematic diagram of our developed system. The operational target facility is a hospital or  care house under the LAN/Internet environment. The robot surveys along a corridor and room autonomously  by collecting and processing the environment information. The operator can monitor in real time the  environment, and communicates with a patient or elderly as needed. In an emergency, when a patient is lying  on the floor, the robot informs the operator. The operator controls the robot in the manual mode using the  haptic device.   Fig.1 Schematic diagram of developed system.                                                            Fig.2 Surveillance Robot  2.1. Surveillance Robot  The developed surveillance robot is shown in Fig.2. The robot has a depth/RGB image sensor (Xtion Pro  LIVE, ASUS) and a arm equipped a temperature sensor and a pressure sensors. In addition, the robot has a  94   Shin-ichiro Kaneko and Genci Capi /  IERI Procedia  10 ( 2014 )  92 â€“ 97  microphone and speaker and a small crystal monitor which is used for operator patient communication. The  robot length, height, width are 600[mm], 560[mm], 320[mm], respectively. Its weight is 12 kg (including  motor and PC batteries). It has 4 degree of freedom (2 crawler, Pitch and Yaw axis) sensor arm. An equipped  control PC has 1[GB] memory and Intel Core i3 CPU, and its OS is Ubuntu 12.04LTS installed in a SSD.  2.2. Operation Environment  A user environment is shown in Fig.3(a). There is a web camera on the monitor and a microphone and a  speaker. The haptic device (PHANToM Omni, SensAble Technologies Inc.) is used to control the robot in the  manual mode. A user PC has 8[GB] memories and an Intel Core i5 CPU, and its OS is Windows7.On the user  PC monitor, a camera real image and a depth image are shown to grasp the remote place situation, as shown  in Fig.3(b). The depth image is also used to improve a userâ€™s feeling to depth and distance on manual control  operation. In addition, the operator can monitor the temperature and pressure sensors data.  Fig.3(a) User environment;                                                                      (b) Sensor data on user PC monitor  2.3. Software Configuration  Our developed system is based on OpenRTM-aist [5]. OpenRTM-aist is a component-oriented software for  robotic systems. Fig.5 shows the configuration of components and all components are connected to each other  on the TCP/IP network. In robot side, the image component captures the images using Xtion Pro LIVE and  transmits them to the operator side. The driver component transmits the reference motor data to the motor  driver module, which collects/transmits motor status. The voice component captures/transmits voice sound  data from a microphone and decodes sound data received from the operator side.In the operator side, the  image processing and control input component is a main motion control manager which processes image data  and calculates reference motor data. In addition, it processes the haptic device data input for manual control of  the robot and feedbacks a pressure sensor data to the device. The voice component in the operator side is same  as the robot side.  Fig.4 OpenRTM-aist component configuration  95 Shin-ichiro Kaneko and Genci Capi /  IERI Procedia  10 ( 2014 )  92 â€“ 97  3. Surveillance System  The surveillance system has two operation modes: autonomous and manually. In the autonomous mode,  the robot goes along the regular route using the landmarks distributed in the environment. In the manual mode,  the operator controls the robot using the haptic device. These two modes are changeable at any time according  to the situation (Fig.5).  Fig.5 Surveillance system                                                             Fig.6 Marker extraction (a) Real image; (b) Center of the blob  3.1. Autonomous Navigation Mode  In autonomous survey mode, the robot navigates at a regular speed utilizing the landmarks located on the  route which is the corridor and/or room. The marker is a red color 150x150 mm square. By RGB image  processing, the center of the blob is extracted (Fig.6). Reference motor speed ratio is calculated by the  horizontal offset between a center of image and the blob position, as follows:  Ý’ à¯‹ àµŒÜ­ à¬µ àµ…Ü­ à¬¶ àµ¬Ýƒ à¯« àµ† Ü¹ÜªÜ¶Ü¦Ü« Í´ àµ°  (1)  Ý’ à¯… àµŒÜ­ à¬µ àµ†Ü­ à¬¶ àµ¬Ýƒ à¯« àµ† Ü¹ÜªÜ¶Ü¦Ü« Í´ àµ°  (2)  where, v R  and  v L  are the right/left side crawler reference speed,K 1  and K 2  are constant values  corresponding to the straight and the rotary component,g x  is a horizontal coordinate of extracted the center of  blob and theWIDTH is a width size of RGB image.  The discrimination method at finding an emergency situation, e.g. a patient lying on the floor, is realized  by distinguishing color of patientâ€™s clothes and skin color. In this system, a target cloth color is blue (Fig.7  (a)). It is supposed that all the patients have the same clothes. If the extracting coordinates of center of blob  for blue and skin color (Fig.7 (b) and (c)) is less than 40pixels, the system judges as an emergency situation.  The robot informs the operator that switches to the manual operation mode.  Fig.7 Human detection (a) Real image;       (b) Blue color extraction;   (c) Skin color extraction                      Fig.8 Haptic device  96   Shin-ichiro Kaneko and Genci Capi /  IERI Procedia  10 ( 2014 )  92 â€“ 97  3.2. Manual Control Mode  In this mode the user control the robot motion using the haptic device. In addition, by the Haptic device,  the arm where sensors are mounted is also controlled (Fig.8). The operator can switch between robot and arm  motion control at any time.    Î¾ Robot motion control  The axis Î¸ 1  and Î¸ 3  are used to calculate the right/left crawler reference speeds as follows:  Ý’ à¯‹ àµŒÜ­ à¬µ ßšàµ…Ü­ à¬¶ 3) ß™)  Ý’ à¯… àµŒÜ­ à¬µ ßšàµ†Ü­ à¬¶ 4) ß™)  È½àµŒ ß  à¬µ ß  à¬µà¯ à¯”à¯«  (5)  È¾àµŒ ß  à¬· ß  à¬·à¯ à¯”à¯«  (6)  whereK 1  and K 2  are constant values determined by experiments. Î¸ 1max  and Î¸ 3max are the maximum joint  angle and âˆ†   is the ratio of forward/backward crawler speed, and Î•   is ratio of rotation.   Î¾ Arm control  In this mode, the reference Yaw joint angle Î¸ Y  and Pitch joint angle Î¸ P of the sensor arm are calculated as  follows:  ß  à¯’ àµŒ ß  à¬µ ß  à¬µà¯ à¯”à¯«  (7)  ß  à¯‰ àµŒ ß  à¬¶ ß  à¬¶à¯ à¯”à¯«  (8)  In the manual control mode, the operator has to controlthe arm with great caution. This is because it is  difficult to grasp the situation using only the information on PC monitor. In order to solve this problem, the  pressure sensor data is feed-backed to the joint Î¸ 2 of the haptic device. It interpolates contact condition of  sensor arm on the patient and restricts the arm operation. This protects an overly contact pressure on the  patient skin and an excessive action of force on the arm joint.  Fig.9 Experimental environment                      Fig.10 Autonomous robot motion        Fig.11 User based robot control using haptic device  97 Shin-ichiro Kaneko and Genci Capi /  IERI Procedia  10 ( 2014 )  92 â€“ 97  4. Experiments  4.1. Experimental Environment  Fig.9 shows the experimental environment. The robot starts from initial position and goes along the  corridor, enters in room1 and room2. There is a pseudo lying patient in the room2. The robot approaches the  man and then checks its condition using the sensors mounted in the arm.  4.2. Results  Fig.10 shows video capture images of the robot operating in the autonomous mode. The picture number  corresponds to the robot location in the environment, as shown in Fig.10. The reference moving speed in the  autonomous mode was about 100 [mm/sec].Result show that the robot was able to go along the corridor by  finding located markers. After the robot enters in room 1 and check the situation, the robot returns back to the  corridor and continues its motion toward room 2.  In room 2 the robot was able to find the lying person using  the camera image. Next, we evaluated the performance of the arm motion control to check the vital signs  using the pressure and temperature sensors. After the robot reaches the man lying in the floor, the operator  starts to move the robot arm toward the manâ€™s body (Fig.11). The operator control the arm motion using the  haptic device based on the information from the camera image and pressure sensor shown in his/her PC.  The result showed that the developed system was able to feed-back the sampled pressure data to haptic  device and the robot was able to detect and transmit the body temperature to the operator PC.However in  some cases the contact pressure between the robot arm and human body higher than expected. The reason was  related with the time delay in the communication between the robot and the operator. These issues must be  considered in order to improve the performance of the robot. However, the robot operated safely.  5. Concluding Remarks  In this paper, we presented a tele-operated mobile robot system for old age surveillance. The robot utilizes  the visual sensor data to move autonomously on the surveillance route utilizing the landmarks distributed in  the environment. The developed robot operated in autonomous and manually control mode. In the manually  control mode the operator utilized he haptic device, remotely. If a patient is lying on the floor, the robot  recognizes the situation and transits the autonomous mode to the user control mode automatically. In addition,  the robot utilizes the vital sensors to check the patient's condition.   References  [1] Annual Report on the Aging Society: 2012 (Summary), Cabinet Office, Government of Japan,  http://www8.cao.go.jp/kourei/english/annualreport/  [2] Matsui T, Yoshida Y, Kagawa M, Kubota M, Kurita A.â€œDevelopment of a practicable non-contact  bedside autonomic activation monitoring system using microwave radars and its clinical ap4plication in  elderly peopleâ€�, J ClinMonitComput,  2013 Jun;27(3):351-6.  [3] Oda A, Fujiwara Y, â€œWatching System for Care Facilities of The Elderlyâ€�, Journal of the Robotics  Society of Japan, Vol.28, No.9, pp.1075-1076, 2010. (in Japanese)  [4] Sawashima H, Yano Y, â€œDevelopment of Elderly people Monitoring System in Care Houseâ€�,  The  Resaerch Report, Nara Prefecture Institute of Industrial Development,  No.39, pp.6-10, 2013.   [5] OpenRTM-aist, http://openrtm.org/  à¬µ ß  à¬µà¯ à¯”à¯«  (7)  ß  à¯‰ àµŒ ß  à¬¶ ß  à¬¶à¯ à¯”à¯«  (8)  In the manual control mode, the operator has to controlthe arm with great caution. This is because it is  difficult to grasp the situation using only the information on PC monitor. In order to solve this problem, the  pressure sensor data is feed-backed to the joint Î¸ 2 of the haptic device. It interpolates contact condition of  sensor arm on the patient and restricts the arm operation. This protects an overly contact pressure on the  patient skin and an excessive action of force on the arm joint.  Fig.9 Experimental environment                      Fig.10 Autonomous robot motion        Fig.11 User based robot control using haptic device  97 Shin-ichiro Kaneko and Genci Capi /  IERI Procedia  10 ( 2014 )  92 â€“ 97  4. Experiments  4.1. Experimental Environment  Fig.9 shows the experimental environment. The robot starts from initial position and goes along the  corridor, enters in room1 and room2. There is a pseudo lying patient in the room2. The robot approaches the  man and then checks its condition using the sensors mounted in the arm.  4.2. Results  Fig.10 shows video capture images of the robot operating in the autonomous mode. The picture number  corresponds to the robot location in the environment, as shown in Fig.10. The reference moving speed in the  autonomous mode was about 100 [mm/sec].Result show that the robot was able to go along the corridor by  finding located markers. After the robot enters in room 1 and check the situation, the robot returns back to th</xocs:rawtext><xocs:serial-item><article xmlns="http://www.elsevier.com/xml/ja/dtd" version="5.2" xml:lang="en" docsubtype="fla"><item-info><jid>IERI</jid><aid>461</aid><ce:pii>S2212-6678(14)00144-0</ce:pii><ce:doi>10.1016/j.ieri.2014.09.096</ce:doi><ce:copyright type="other" year="2014">The Authors</ce:copyright></item-info><head><ce:article-footnote><ce:label>☆</ce:label><ce:note-para id="npar0005" view="all">Selection and peer review under responsibility of Information Engineering Research Institute.</ce:note-para></ce:article-footnote><ce:title id="tit0005">Human-robot Communication for Surveillance of Elderly People in Remote Distance</ce:title><ce:author-group id="aug0005"><ce:author id="aut0005"><ce:given-name>Shin-ichiro</ce:given-name><ce:surname>Kaneko</ce:surname><ce:cross-ref id="crf0005" refid="aff0005"><ce:sup loc="post">a</ce:sup></ce:cross-ref><ce:cross-ref id="crf0010" refid="cor0005"><ce:sup loc="post">⁎</ce:sup></ce:cross-ref><ce:e-address id="eadd0005" type="email">skaneko@nc-toyama.ac.jp</ce:e-address></ce:author><ce:author id="aut0010"><ce:given-name>Genci</ce:given-name><ce:surname>Capi</ce:surname><ce:cross-ref id="crf0015" refid="aff0010"><ce:sup loc="post">b</ce:sup></ce:cross-ref></ce:author><ce:affiliation id="aff0005"><ce:label>a</ce:label><ce:textfn>National Institute of Technology Toyama, Department of Electrical and Control Systems Engineering, 13,Hongo-machi, Toyama, Japan</ce:textfn></ce:affiliation><ce:affiliation id="aff0010"><ce:label>b</ce:label><ce:textfn>University of Toyama, Department of Electrical and Electronic Systems Eng. Faculty of Engineering. 3190 Gofuku, Toyama, Japan</ce:textfn></ce:affiliation><ce:correspondence id="cor0005"><ce:label>⁎</ce:label><ce:text>Corresponding author. Tel.: +81-76-493-5451; fax: +81-76-493-5451.</ce:text></ce:correspondence></ce:author-group><ce:abstract id="abs0005" view="all" class="author"><ce:section-title id="sect0005">Abstract</ce:section-title><ce:abstract-sec id="abst0005" view="all"><ce:simple-para id="spar0005" view="all">In this paper, we present a tele-operated mobile robot system for old age surveillance. The robot operates in autonomous mode in which the robots navigates in the environment and search for unusual situation of elderly people. If a patient is lying on the floor, the robot informs the user. The user switches the control mode from autonomous to haptic based user control. In the autonomous mode, the robot utilizes the visual sensor and landmarks to monitor the entire environment. The robot is equipped microphone, speaker and monitor making it possible to communicate with the user in remote place. In addition, the robot utilizes the vital sensors to check the patient's condition. The preliminary surveillance experiments show a good performance.</ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords id="kwd0005" class="keyword" view="all"><ce:section-title id="sect0010">Keywords</ce:section-title><ce:keyword id="kw0005"><ce:text>mobile robot</ce:text></ce:keyword><ce:keyword id="kw0010"><ce:text>old age surveillance</ce:text></ce:keyword><ce:keyword id="kw0015"><ce:text>remote control;</ce:text></ce:keyword></ce:keywords></head><tail view="all"><ce:bibliography id="bibl0005" view="all"><ce:section-title id="sect0020">References</ce:section-title><ce:bibliography-sec id="bibs0005" view="all"><ce:bib-reference id="bib0005"><ce:label>[1]</ce:label><ce:other-ref id="oref0005"><ce:textref>Annual Report on the Aging Society: 2012 (Summary), Cabinet Office, Government of Japan, http://www8.cao.go.jp/kourei/english/annualreport/.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0010"><ce:label>[2]</ce:label><sb:reference id="sbref0010"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Matsui</ce:surname></sb:author><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Yoshida</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Kagawa</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Kubota</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Kurita</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Development of a practicable non-contact bedside autonomic activation monitoring system using microwave radars and its clinical ap4plication in elderly people</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J ClinMonitComput</sb:maintitle></sb:title><sb:volume-nr>27</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>2013</sb:date></sb:issue><sb:pages><sb:first-page>351</sb:first-page><sb:last-page>356</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0015"><ce:label>[3]</ce:label><sb:reference id="sbref0015"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Oda</ce:surname></sb:author><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Fujiwara</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Watching System for Care Facilities of The Elderly</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of the Robotics Society of Japan</sb:maintitle></sb:title><sb:volume-nr>28</sb:volume-nr></sb:series><sb:issue-nr>9</sb:issue-nr><sb:date>2010</sb:date></sb:issue><sb:pages><sb:first-page>1075</sb:first-page><sb:last-page>1076</sb:last-page></sb:pages></sb:host><sb:comment>(in Japanese)</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0020"><ce:label>[4]</ce:label><sb:reference id="sbref0020"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Sawashima</ce:surname></sb:author><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Yano</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Development of Elderly people Monitoring System in Care House</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>The Resaerch Report, Nara Prefecture Institute of Industrial Development</sb:maintitle></sb:title></sb:series><sb:issue-nr>39</sb:issue-nr><sb:date>2013</sb:date></sb:issue><sb:pages><sb:first-page>6</sb:first-page><sb:last-page>10</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0025"><ce:label>[5]</ce:label><ce:other-ref id="oref0025"><ce:textref>OpenRTM-aist, http://openrtm.org/.</ce:textref></ce:other-ref></ce:bib-reference></ce:bibliography-sec></ce:bibliography></tail></article></xocs:serial-item></xocs:doc></originalText></full-text-retrieval-response>