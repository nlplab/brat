<full-text-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/article/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:bk="http://www.elsevier.com/xml/bk/dtd"><coredata><prism:url>http://api.elsevier.com/content/article/pii/S2212671612000613</prism:url><dc:identifier>doi:10.1016/j.aasri.2012.06.060</dc:identifier><eid>1-s2.0-S2212671612000613</eid><prism:doi>10.1016/j.aasri.2012.06.060</prism:doi><pii>S2212-6716(12)00061-3</pii><dc:title>A Pupil Location Method for WIFI-based Video-oculography System </dc:title><prism:publicationName>AASRI Procedia</prism:publicationName><prism:aggregationType>Journal</prism:aggregationType><prism:issn>22126716</prism:issn><prism:volume>1</prism:volume><prism:startingPage>389</prism:startingPage><prism:endingPage>393</prism:endingPage><prism:pageRange>389-393</prism:pageRange><dc:format>text/xml</dc:format><prism:coverDate>2012-12-31</prism:coverDate><prism:coverDisplayDate>2012</prism:coverDisplayDate><prism:copyright>Copyright © 2012 Published by Elsevier B.V.</prism:copyright><prism:publisher>Published by Elsevier B.V.</prism:publisher><prism:issueName>AASRI Conference on Computational Intelligence and Bioinformatics</prism:issueName><dc:creator>Chen, Xuejun</dc:creator><dc:creator>Li, Wenfang</dc:creator><dc:description>AbstractVideo-oculography (VOG) is one of eye movement measurement methods. A key problem of VOG is to accurately estimate the pupil center. Then a pupil location method based on morphology and Canny algorithm was proposed for a WIFI-based VOG system which was developed our latest work. Moreover, a healthy volunteer was introduced to do sinusoidal tracking test to evaluate the pupil location method. Experimental results showed that the method could well trace eye movement and meet the anticipated results with stimulation.</dc:description><openaccess>1</openaccess><openaccessArticle>true</openaccessArticle><openaccessType>Full</openaccessType><openArchiveArticle>false</openArchiveArticle><openaccessSponsorName/><openaccessSponsorType/><openaccessUserLicense>http://creativecommons.org/licenses/by-nc-nd/3.0/</openaccessUserLicense><dcterms:subject>Video-oculography (VOG)</dcterms:subject><dcterms:subject>Canny</dcterms:subject><dcterms:subject>morphology</dcterms:subject><dcterms:subject>Pupil</dcterms:subject><dcterms:subject>Location</dcterms:subject><link rel="self" href="http://api.elsevier.com/content/article/pii/S2212671612000613"/><link rel="scidir" href="http://www.sciencedirect.com/science/article/pii/S2212671612000613"/></coredata><scopus-id>84924175005</scopus-id><scopus-eid>2-s2.0-84924175005</scopus-eid><link rel="abstract" href="http://api.elsevier.com/content/abstract/scopus_id/84924175005"/><originalText><xocs:doc xmlns:xoe="http://www.elsevier.com/xml/xoe/dtd" xsi:schemaLocation="http://www.elsevier.com/xml/xocs/dtd http://be-prod3a/schema/dtds/document/fulltext/xcr/xocs-article.xsd"><xocs:meta><xocs:content-family>serial</xocs:content-family><xocs:content-type>JL</xocs:content-type><xocs:cid>282179</xocs:cid><xocs:ssids><xocs:ssid type="alllist">291210</xocs:ssid><xocs:ssid type="subj">291791</xocs:ssid><xocs:ssid type="subj">291877</xocs:ssid><xocs:ssid type="subj">291882</xocs:ssid><xocs:ssid type="subj">291883</xocs:ssid><xocs:ssid type="content">31</xocs:ssid><xocs:ssid type="oa">90</xocs:ssid></xocs:ssids><xocs:srctitle>AASRI Procedia</xocs:srctitle><xocs:normalized-srctitle>AASRIPROCEDIA</xocs:normalized-srctitle><xocs:orig-load-date yyyymmdd="20120821">2012-08-21</xocs:orig-load-date><xocs:available-online-date yyyymmdd="20120823">2012-08-23</xocs:available-online-date><xocs:ew-transaction-id>2014-11-18T16:43:20</xocs:ew-transaction-id><xocs:eid>1-s2.0-S2212671612000613</xocs:eid><xocs:pii-formatted>S2212-6716(12)00061-3</xocs:pii-formatted><xocs:pii-unformatted>S2212671612000613</xocs:pii-unformatted><xocs:doi>10.1016/j.aasri.2012.06.060</xocs:doi><xocs:item-stage>S300</xocs:item-stage><xocs:item-version-number>S300.3</xocs:item-version-number><xocs:item-weight>HEAD-AND-TAIL</xocs:item-weight><xocs:hub-eid>1-s2.0-S2212671612X00027</xocs:hub-eid><xocs:timestamp yyyymmdd="20150515">2015-05-15T07:34:50.698394-04:00</xocs:timestamp><xocs:dco>0</xocs:dco><xocs:tomb>0</xocs:tomb><xocs:date-search-begin>20120101</xocs:date-search-begin><xocs:date-search-end>20121231</xocs:date-search-end><xocs:year-nav>2012</xocs:year-nav><xocs:indexeddate epoch="1345507200">2012-08-21T00:00:00Z</xocs:indexeddate><xocs:articleinfo>rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist webpdf webpdfpagecount yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref</xocs:articleinfo><xocs:issns><xocs:issn-primary-formatted>2212-6716</xocs:issn-primary-formatted><xocs:issn-primary-unformatted>22126716</xocs:issn-primary-unformatted></xocs:issns><xocs:crossmark is-crossmark="0">false</xocs:crossmark><xocs:vol-first>1</xocs:vol-first><xocs:volume-list><xocs:volume>1</xocs:volume></xocs:volume-list><xocs:suppl>C</xocs:suppl><xocs:vol-iss-suppl-text>Volume 1</xocs:vol-iss-suppl-text><xocs:sort-order>60</xocs:sort-order><xocs:first-fp>389</xocs:first-fp><xocs:last-lp>393</xocs:last-lp><xocs:pages><xocs:first-page>389</xocs:first-page><xocs:last-page>393</xocs:last-page></xocs:pages><xocs:cover-date-orig><xocs:start-date>2012</xocs:start-date></xocs:cover-date-orig><xocs:cover-date-text>2012</xocs:cover-date-text><xocs:cover-date-start>2012-01-01</xocs:cover-date-start><xocs:cover-date-end>2012-12-31</xocs:cover-date-end><xocs:cover-date-year>2012</xocs:cover-date-year><xocs:title-editors-groups><xocs:title-editors-group><ce:title>AASRI Conference on Computational Intelligence and Bioinformatics</ce:title><ce:editors><ce:author-group><ce:author><ce:degrees>Dr.</ce:degrees><ce:given-name>Wei</ce:given-name><ce:surname>Deng</ce:surname></ce:author><ce:affiliation><ce:textfn>American Applied Sciences Research Institute, Suit C, 637 Vineland Ave, La Puente, CA 91746, United States of America</ce:textfn></ce:affiliation></ce:author-group></ce:editors></xocs:title-editors-group></xocs:title-editors-groups><xocs:document-type>article</xocs:document-type><xocs:document-subtype>fla</xocs:document-subtype><xocs:copyright-line>Copyright © 2012 Published by Elsevier B.V.</xocs:copyright-line><xocs:normalized-article-title>APUPILLOCATIONMETHODFORWIFIBASEDVIDEOOCULOGRAPHYSYSTEM</xocs:normalized-article-title><xocs:normalized-first-auth-surname>CHEN</xocs:normalized-first-auth-surname><xocs:normalized-first-auth-initial>X</xocs:normalized-first-auth-initial><xocs:references><xocs:ref-info refid="bib0005"><xocs:ref-normalized-surname>MARG</xocs:ref-normalized-surname><xocs:ref-pub-year>1951</xocs:ref-pub-year><xocs:ref-first-fp>169</xocs:ref-first-fp><xocs:ref-last-lp>185</xocs:ref-last-lp><xocs:ref-normalized-initial>E</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="bib0010"><xocs:ref-normalized-surname>ROBINSON</xocs:ref-normalized-surname><xocs:ref-pub-year>1963</xocs:ref-pub-year><xocs:ref-first-fp>137</xocs:ref-first-fp><xocs:ref-last-lp>145</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="bib0015"><xocs:ref-normalized-surname>TRAISK</xocs:ref-normalized-surname><xocs:ref-pub-year>2005</xocs:ref-pub-year><xocs:ref-first-fp>791</xocs:ref-first-fp><xocs:ref-last-lp>797</xocs:ref-last-lp><xocs:ref-normalized-initial>F</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="bib0020"><xocs:ref-normalized-surname>SCHWORM</xocs:ref-normalized-surname><xocs:ref-pub-year>2002</xocs:ref-pub-year><xocs:ref-first-fp>662</xocs:ref-first-fp><xocs:ref-last-lp>667</xocs:ref-last-lp><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="bib0025"><xocs:ref-normalized-surname>VANDERGEEST</xocs:ref-normalized-surname><xocs:ref-pub-year>2002</xocs:ref-pub-year><xocs:ref-first-fp>185</xocs:ref-first-fp><xocs:ref-last-lp>195</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="bib0030"><xocs:ref-normalized-surname>LIN</xocs:ref-normalized-surname><xocs:ref-pub-year>2003</xocs:ref-pub-year><xocs:ref-first-fp>505</xocs:ref-first-fp><xocs:ref-last-lp>515</xocs:ref-last-lp><xocs:ref-normalized-initial>C</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="bib0035"><xocs:ref-normalized-surname>LEE</xocs:ref-normalized-surname><xocs:ref-pub-year>1987</xocs:ref-pub-year><xocs:ref-first-fp>142</xocs:ref-first-fp><xocs:ref-last-lp>157</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="bib0040"><xocs:ref-normalized-surname>CANNY</xocs:ref-normalized-surname><xocs:ref-pub-year>1986</xocs:ref-pub-year><xocs:ref-first-fp>679</xocs:ref-first-fp><xocs:ref-last-lp>698</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="bib0045"><xocs:ref-normalized-surname>MCILHAGGA</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>251</xocs:ref-first-fp><xocs:ref-last-lp>261</xocs:ref-last-lp><xocs:ref-normalized-initial>W</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="bib0050"><xocs:ref-normalized-surname>BAO</xocs:ref-normalized-surname><xocs:ref-pub-year>2005</xocs:ref-pub-year><xocs:ref-first-fp>1485</xocs:ref-first-fp><xocs:ref-last-lp>1490</xocs:ref-last-lp><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial></xocs:ref-info></xocs:references><xocs:refkeys><xocs:refkey3>CHENX2012X389</xocs:refkey3><xocs:refkey4lp>CHENX2012X389X393</xocs:refkey4lp><xocs:refkey4ai>CHENX2012X389XX</xocs:refkey4ai><xocs:refkey5>CHENX2012X389X393XX</xocs:refkey5></xocs:refkeys><xocs:open-access><xocs:oa-article-status is-open-access="1" is-open-archive="0">Full</xocs:oa-article-status><xocs:oa-access-effective-date>2013-07-15T12:00:47Z</xocs:oa-access-effective-date><xocs:oa-user-license>http://creativecommons.org/licenses/by-nc-nd/3.0/</xocs:oa-user-license><xocs:oa-access-inherited-from winid="http://vtw.elsevier.com/content/oaw/GEN_UNBOUNDED_ESWaived">OA-Window</xocs:oa-access-inherited-from></xocs:open-access><xocs:attachment-metadata-doc><xocs:attachment-set-type>item</xocs:attachment-set-type><xocs:pii-formatted>S2212-6716(12)00061-3</xocs:pii-formatted><xocs:pii-unformatted>S2212671612000613</xocs:pii-unformatted><xocs:eid>1-s2.0-S2212671612000613</xocs:eid><xocs:doi>10.1016/j.aasri.2012.06.060</xocs:doi><xocs:cid>282179</xocs:cid><xocs:timestamp>2014-11-19T14:21:05.831214-05:00</xocs:timestamp><xocs:cover-date-start>2012-01-01</xocs:cover-date-start><xocs:cover-date-end>2012-12-31</xocs:cover-date-end><xocs:attachments><xocs:web-pdf><xocs:attachment-eid>1-s2.0-S2212671612000613-main.pdf</xocs:attachment-eid><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2212671612000613/MAIN/application/pdf/a788c74ce36718b8928e6e42832edfb7/main.pdf</xocs:ucs-locator><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2212671612000613/MAIN/application/pdf/a788c74ce36718b8928e6e42832edfb7/main.pdf</xocs:ucs-locator><xocs:filename>main.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:pdf-optimized>true</xocs:pdf-optimized><xocs:filesize>195332</xocs:filesize><xocs:web-pdf-purpose>MAIN</xocs:web-pdf-purpose><xocs:web-pdf-page-count>5</xocs:web-pdf-page-count><xocs:web-pdf-images><xocs:web-pdf-image><xocs:attachment-eid>1-s2.0-S2212671612000613-main_1.png</xocs:attachment-eid><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2212671612000613/PREVIEW/image/png/591bba7fd5328a1b9748a09aec45a225/main_1.png</xocs:ucs-locator><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2212671612000613/PREVIEW/image/png/591bba7fd5328a1b9748a09aec45a225/main_1.png</xocs:ucs-locator><xocs:filename>main_1.png</xocs:filename><xocs:extension>png</xocs:extension><xocs:filesize>42693</xocs:filesize><xocs:pixel-height>849</xocs:pixel-height><xocs:pixel-width>656</xocs:pixel-width><xocs:attachment-type>IMAGE-WEB-PDF</xocs:attachment-type><xocs:pdf-page-num>1</xocs:pdf-page-num></xocs:web-pdf-image></xocs:web-pdf-images></xocs:web-pdf></xocs:attachments></xocs:attachment-metadata-doc></xocs:meta><xocs:rawtext> AASRI Procedia   1  ( 2012 )  389 â€“ 393  2212-6716  2012 Published by Elsevier Ltd.  doi: 10.1016/j.aasri.2012.06.060  2012 AASRI Conference on Computational Intelligence and Bioinformatics    A Pupil Location Method for WIFI-based Video-oculography  System Xuejun Chen a,* , Wenfang Li  a ,a* a Department of Electronic Engineering, Putian University, Putian 351100, China  Abstract  Video-oculography (VOG) is one of eye movement measurement methods. A key problem of VOG is to accurately  estimate the pupil center. Then a pupil location method based on morphology and Canny algorithm was proposed for  a WIFI-based VOG system which was developed our latest work. Moreover, a healthy volunteer was introduced to do  sinusoidal tracking test to evaluate the pupil location method. Experimental results showed that the method could  well trace eye movement and meet the anticipated results with stimulation.  2012 Published by Elsevier B.V. Selection and/or peer review under responsibility of American Applied  Science Research Institute  Keywords: Video-oculography (VOG), Canny, morphology, Pupil, Location;  1. Introduction  There is a common saying that eyes are â€˜windows to our soulâ€™. In fact, eyes are the main â€˜interfaceâ€™  between environment and human brain. Therefore, research on the eye, not only helps to understand the  many laws of mental activity, but also to the clinical diagnosis of the eye. With the development of eye  movement research, until now a lot of different methods for measuring eye movements have been  developed. They include Electro-oculography (EOG) [1], magnetic search coil [2], Infrared corneal  reflection oculography (IROG) [3], Video-oculography (VOG) [4-5] and other methods.  * Corresponding author. Tel.&amp; fax: +086-0594-2696450.  E-mail address: cxjnet@126.com.  AASRI Procedia www.elsevier.com/locate/procedia Available online at www.sciencedirect.com Open access under CC BY-NC-ND license. Open access under CC BY-NC-ND license. 390   Xuejun Chen and Wenfang Li /  AASRI Procedia  1 ( 2012 )  389 â€“ 393  VOG is one of the most popular methods and becoming more and more attractive, due to high accuracy,  insignificant artifacts, non-invasiveness, and freedom of head movement [6]. VOG analysis is based on  eye movement tracing. A key problem of VOG is to accurately estimate the pupil center. Therefore, the  purpose of this study is (i) to present an analysis system based on WIFI for nystagmus which we have  developed, and (ii) a pupil location method based on morphology and Canny algorithm was proposed, and  then (iii) experiment was done to evaluate the method.  2. System  Fig. 1. (a) first picture; (b) second picture  The WIFI-based VOG system is shown in Fig. 1. The system consists of a helmet, Wireless router and  a nystagmus measuring system. The helmet is used as video transmission server, which acquire videos of  pupil movement and transmitted them to the wireless network through the WIFI communication module.  Wireless router was used as an access point (AP). It connects with nystagmus measuring system. If there  are not any routers in the LAN or WLAN, the USB-router would be directly put into nystagmus  measuring system as an AP. The video signals through wireless network would be received and  decompressed by nystagmus measuring system. Users can analyze the pupil videos and send the  controller signals to the helmet with nystagmus analysis software.  3. Pupil location method   Pupil center localization is crucial for the performance of VOG analysis system. It aims to find the  coordinates of pupil center. Then eye movements can be obtained by fitting the coordinates of pupil  center. The proposed pupil localization algorithm includes three steps, morphology processing, Canny  edge detecting and calculating coordinates of pupil center. The algorithm flow chart of pupil center  localization and tracing is shown in Fig. 2.  Fig. 2. Algorithm flow chart of Pupil center tracing  391 Xuejun Chen and Wenfang Li /  AASRI Procedia  1 ( 2012 )  389 â€“ 393  3.1. Morphology processing  After the original image of pupil has been processed to binarization image, mathematical morphology  was used to adaptively process the structuring elements for pupil extraction in binarization image. Among  the operation of mathematical morphology, erosion and dilation is the base of other complicated  operationsâ€™ implementation. Erosion is a transformation of shrinking, which decreases the grey-scale  value of the image, while dilation is a transformation of expanding, which increases the grey-scale value  of the image.  In order to obtain more obvious pupil area from binarization image, we use closing and opening  operation to denoise the binarization image and detect edge of pupil area. Let F(x,y) denote a binarization  image, and B denote structuring element. Opening and closing of F(x,y) by B are denoted respectively by  equation (1) and (2)[7-8].  FÇ„B= (FÂ‘B)Í°B                                                                                                                         (1)  FgB= (FÍ°B)Â‘%                                                                                                                         (2)  Opening is erosion followed by dilation and closing is dilation followed by erosion. Opening can  smooth the contour of the binarization image and break narrow gaps. As opposed to opening, closing  tends to fuse narrow breaks, eliminates small holes, and fills gaps in the contours.   A frame image of pupil videos is shown in Fig.3 (a), and Fig.3 (b) was its binarization image. When  the binarization image was processed by closing and opening, the results were shown in Fig.3 (c) and (d)  respectively. Fig.3 (d) shows that there are three connected regions, but their sizes are different. In order  to gain pupil region, median filter and connected region processing were used for Fig.3 (d), and Fig.3 (e)  show the processing result. Only was one biggest connected region obtained, which is pupil. Then next  step is to get coordinates of pupil center.  (a)                            (b)                                  (c)                          (d)                           (e)  Fig.3  Image of pupil video preprocessing.(a)Original image;(b) binarization image;(c) binarization image after closing; (d)  binarization image after opening; (e) binarization image after median filter and connected region processing;  3.2. Canny edge detecting   First of all to get boundaries of pupil from Fig.3 (e), an algorithm based on Canny was used to detect  edge of pupil. Canny brought out the rule that excellent edge-detection method in [1986]. Canny  generalized three criteria to evaluate an edge detector: good detectionÇƒgood localization and only one  response to a single edge. Good detection is that the signal-to-noise ratio (SNR) should be as high as  possible. Good localization is that the detected edge-point should be at the center of real edge to its best  possibility. And only one response to a single edge means that the detector should not produce multiple  maxima [8].   Following the three criteria, we present an edge detecting algorithm based on Canny. The course of the  algorithm by canny operator includes lowpass filtering with Gauss function, calculating the value and  392   Xuejun Chen and Wenfang Li /  AASRI Procedia  1 ( 2012 )  389 â€“ 393  direction of grads, non-maxima suppression to grad value and checking and connecting edges. Let f(x,y)  denote the binarization image of  Fig.3 (e). The steps are as follows [8-10]:  Ä·The row and column of f(x,y) were convolved with 1D Gaussian  2 2 2 2 1 () 2 x gx e V SV    Â˜ and  2 2 2 2 1 () 2 y gy e V SV    Â˜respectively to smooth the image and eliminate image noise.Â³is the parameter of  Gauss filter and control the extend of smoothing image.  Ä¸To obtain grads vector, the results of Ä· were processed by  22 2 2 2 2 (, ) x y dx x gxy e V SV        Â˜  and  22 2 2 2 2 (, ) x y dy y gxy e V SV        Â˜  respectively, and the value and direction of grads were calculated.  Ä¹Non-maxima suppression (NMS) was used to process the value of grads. Traversing the processed  image domain, 8-neighborhood of each pixel was divided into four quadrants according to the gradient  direction. Then 8-neighborhood was divided into two cases along the 45 degrees of gradient direction.  That is, Gray value of each pixel was compared to the two pixels of its neighbor along the gradient  direction. If the former is smaller, then set the pixel value is 0, that is, not the edges.  ÄºDouble-threshold method was adapted to get edges.  After the binarization image of Fig.3 (e) was processed by the proposed algorithm, the edge contour of  pupil would be obtained and shown in Fig.4 (a). Then Hough transform is applied to the edge contour for  obtaining the parameters of the pupil circle. Fig. 4(b) gives edge contour of pupil and fine localization of  the pupil center. Finally, the coordinates of the pupil of each frame images were fitted and the pupil  movement tracking were obtained.  (a)                             (b)  Fig.4 Canny algorithm preprocessing of Fig.3 (e). (a)the results of Canny algorithm preprocessing;(b) pupil center location.  Fig.5 The pupil tracing of a healthy volunteer for sinusoidal tracking test   393 Xuejun Chen and Wenfang Li /  AASRI Procedia  1 ( 2012 )  389 â€“ 393  4. Experiment A healthy volunteer without any history of vertigo, disequilibrium and nerve participated in the study.  The volunteer was introduced to do sinusoidal tracking test. A sampling rate of 25 frames/s was employed  for the WIFI-based VOG system. The system generated sinusoidal tracking eye movements with the  proposed pupil location method, when the light dot was moved horizontally like a pendulum from left to  right and vice versa. The velocity of the light dot varied along with two mixed sine waves in order to  create unpredictable tracking movements for subjects. The pupil location tracing was shown in Fig.5.  Fig.5 shows that horizontal eye movement was sawtooth waves and vertical eye movement was sine  waves for the healthy volunteer. Thy met the anticipated results.  5. Conclusions   Authors presented a WIFI-based Video-oculography system. Then a pupil location method based on  morphology and Canny algorithm was proposed and used for the system. A healthy volunteer was  introduced to do sinusoidal tracking test to verify the pupil location method for WIFI-based VOG system.  The results show that this method can well meet the anticipated results.   The future work, we will use the system and method to gather every kind of eye movement signals of  patients and analyze the characteristic of eye movements.  Acknowledgements  This work is supported by the Fundamental Research Funds for Universities in FujianË„JK2011049Ë…. References  [1] E Marg. Development of electro-oculography. A. M. A. Arch. Ophthalmol 1951;45:169-185.  [2] DA Robinson. A method  of measuring  eye  movement using  a scleral search  coil  in  a  magnetic   field. IEEE Biomed Eng. 1963;10:137-145.  [3] F. Traisk,R. Bolzani,J. Ygge. A comparison between the magnetic scleral search coil and infrared  reflection methods for saccadic eye movement analysis. Graefeâ€™s Arch Clin Exp Ophthalmol  2005;243:791-797.  [4] H.D. Schworm, J. Ygge, T. Pansell, and G. Lennerstrand. Assessment of Ocular Counterroll during  Head Tilt Using Binocular Video Oculography. Investigative Ophthalmology &amp; Visual Science  2002;43:662-667.  [5] J.N. van der Geest, M.A. Frens. Recording eye movements with video-oculography and scleral search  coils: a direct comparison of two methods. Journal of Neuroscience Methods 2002;114:185-195.  [6] C.S. Lin, L.W. Lue, M.S. Yeh, T.S. Hwang, S.H. Lee. A new image processing method for evaluating  the pupillary responses in a HMD-type eye-tracking device. Optics &amp; Laser Technology 2003;35:505-515.  [7] J.S.J. Lee, R.M. Haralick, and L.G. Shapiro. Morphological Edge Detection. IEEE J. Robot. Automat  1987;3:142-157.  [8] J. Canny. A Computational Approach to Edge Detection. IEEE Trans. Pattern Analysis and Machine  Intelligence 1986;8:679-698.  [9] W. McIlhagga. The Canny Edge Detector Revisited. Int J Comput Vis 2011;91:251-261.  [10] P. Bao, L. Zhang, and X. Wu. Canny Edge Detection Enhancement by Scale Multiplication. IEEE  Trans.Pattern Analysis and Machine Intelligence 2005;27:1485-1490.   the 45 degrees of gradient direction.  That is, Gray value of each pixel was compared to the two pixels of its neighbor along the gradient  direction. If the former is smaller, then set the pixel value is 0, that is, not the edges.  ÄºDouble-threshold method was adapted to get edges.  After the binarization image of Fig.3 (e) was processed by the proposed algorithm, the edge contour of  pupil would be obtained and shown in Fig.4 (a). Then Hough transform is applied to the edge contour for  obtaining the parameters of the pupil circle. Fig. 4(b) gives edge contour of pupil and fine localization of  the pupil center. Finally, the coordinates of the pupil of each frame images were fitted and the pupil  movement tracking were obtained.  (a)                             (b)  Fig.4 Canny algorithm preprocessing of Fig.3 (e). (a)the results of Canny algorithm preprocessing;(b) pupil center location.  Fig.5 The pupil tracing of a healthy volunteer for sinusoidal tracking test   393 Xuejun Chen and Wenfang Li /  AASRI Procedia  1 ( 2012 )  389 â€“ 393  4. Experiment A healthy volunteer without any history of vertigo, disequilibrium and nerve participated in the study.  The volunteer was introduced to do sinusoidal tracking test. A sampling rate of 25 frames/s was employed  for the WIFI-based VOG system. The system generated sinusoidal tracking eye movements with the  proposed pupil location method, when the light dot was moved horizontally like a pendulum from left to  right and vice versa. The velocity of the light dot varied along with two mixed sine waves in order to  create unpredictable tracking movements for subjects. The pupil location tracing was shown in Fig.5.  Fig.5 shows that horizontal eye movement was sawtooth waves and vertical eye movement was sine  waves for the healthy volunteer. Thy met the anticipated results.  5. Conclusions   Authors presented a WIFI-based Video-oculography system. Then a pupil location method based on  morphology and Canny algorithm was proposed and used for the system. A healthy volunteer was  introduced to do sinusoidal tracking test to verify the pupil location method for WIFI-based VOG system.  The results show that this method can well meet the anticipated results.   The future work, we will use the system and method to gather every kind of eye movement signals of  patients and analyze the characteristic of eye movements.  Acknowledgements  This work is supported by the Fundamental Research Funds for Universities in FujianË„JK2011049Ë…. References  [1] E Marg. Development of electro-oculography. A. M. A. Arch. Ophthalmol 1951;45:169-185.  [2] DA Robinson. A method  of measuring  eye  movement using  a scleral search  coil  in  a  magnetic   field. IEEE Biomed Eng. 1963;10:137-145.  [3] F. Traisk,R. Bolzani,J. Ygge. A comparison between the magnetic scleral search coil and infrared  reflection methods for saccadic eye movement analysis. Graefeâ€™s Arch Clin Exp Ophthalmol  2005;243:791-797.  [4] H.D. Schworm, J. Ygge, T. Pansell, and G. Lennerstrand. Assessment of Ocular Counterroll during  Head Tilt Using Binocular Video Oculography. Investigative Ophthalmology &amp; Visual Science  2002;43:662-667.  [5] J.N. van der Geest, M.A. Frens. Recording eye movements with video-oculography and scleral search  coils: a direct comparison of two methods. Journal of Neuroscience Methods 2002;114:185-195.  [6] C.S. Lin, L.W. Lue, M.S. Yeh, T.S. Hwang, S.H. Lee. A new image processing method for evaluating  the pupillary responses in a HMD-type eye-tracking device. Optics &amp; Laser Technology 2003;35:505-515.  [7] J.S.J. Lee, R.M. Haralick, and L.G. Shapiro. Morphological Edge Detection. IEEE J. Robot. Automat  1987;3:142-157.  [8] J. Canny. A Computational Approach to Edge Detection. IEEE Trans. Pattern Analysis and Machine  Intelligence 1986;8:679-698.  [9] W. McIlhagga. The Canny Edge Detector Revisited. Int J Comput Vis 2011;91:251-261</xocs:rawtext><xocs:serial-item><article xmlns="http://www.elsevier.com/xml/ja/dtd" version="5.1" xml:lang="en" docsubtype="fla"><item-info><jid>AASRI</jid><aid>60</aid><ce:pii>S2212-6716(12)00061-3</ce:pii><ce:doi>10.1016/j.aasri.2012.06.060</ce:doi><ce:copyright type="unknown" year="2012"/></item-info><head><ce:title>A Pupil Location Method for WIFI-based Video-oculography System</ce:title><ce:author-group><ce:author><ce:given-name>Xuejun</ce:given-name><ce:surname>Chen</ce:surname><ce:cross-ref refid="cor0005"><ce:sup loc="post">⁎</ce:sup></ce:cross-ref><ce:e-address type="email">cxjnet@126.com</ce:e-address></ce:author><ce:author><ce:given-name>Wenfang</ce:given-name><ce:surname>Li</ce:surname><ce:cross-ref refid="cor0005"><ce:sup loc="post">⁎</ce:sup></ce:cross-ref></ce:author><ce:affiliation><ce:textfn>Department of Electronic Engineering, Putian University, Putian 351100, China</ce:textfn></ce:affiliation><ce:correspondence id="cor0005"><ce:label>⁎</ce:label><ce:text>Corresponding author. Tel.: +086 0594 2696450; fax: +086 0594 2696450.</ce:text></ce:correspondence></ce:author-group><ce:abstract class="author"><ce:section-title>Abstract</ce:section-title><ce:abstract-sec><ce:simple-para id="spar0005" view="all">Video-oculography (VOG) is one of eye movement measurement methods. A key problem of VOG is to accurately estimate the pupil center. Then a pupil location method based on morphology and Canny algorithm was proposed for a WIFI-based VOG system which was developed our latest work. Moreover, a healthy volunteer was introduced to do sinusoidal tracking test to evaluate the pupil location method. Experimental results showed that the method could well trace eye movement and meet the anticipated results with stimulation.</ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords class="keyword"><ce:section-title>Keywords</ce:section-title><ce:keyword><ce:text>Video-oculography (VOG)</ce:text></ce:keyword><ce:keyword><ce:text>Canny</ce:text></ce:keyword><ce:keyword><ce:text>morphology</ce:text></ce:keyword><ce:keyword><ce:text>Pupil</ce:text></ce:keyword><ce:keyword><ce:text>Location</ce:text></ce:keyword></ce:keywords></head><tail view="all"><ce:bibliography id="bibl0005" view="all"><ce:section-title>References</ce:section-title><ce:bibliography-sec id="bibs0005"><ce:bib-reference id="bib0005"><ce:label>[1]</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Marg</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Development of electro-oculography</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>A. M. A. Arch. Ophthalmol</sb:maintitle></sb:title><sb:volume-nr>45</sb:volume-nr></sb:series><sb:date>1951</sb:date></sb:issue><sb:pages><sb:first-page>169</sb:first-page><sb:last-page>185</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0010"><ce:label>[2]</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.A.</ce:given-name><ce:surname>Robinson</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A method of measuring eye movement using a scleral search coil in a magnetic field</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Biomed Eng.</sb:maintitle></sb:title><sb:volume-nr>10</sb:volume-nr></sb:series><sb:date>1963</sb:date></sb:issue><sb:pages><sb:first-page>137</sb:first-page><sb:last-page>145</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0015"><ce:label>[3]</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Traisk</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Bolzani</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Ygge</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A comparison between the magnetic scleral search coil and infrared reflection methods for saccadic eye movement analysis</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Graefe's Arch Clin Exp Ophthalmol</sb:maintitle></sb:title><sb:volume-nr>243</sb:volume-nr></sb:series><sb:date>2005</sb:date></sb:issue><sb:pages><sb:first-page>791</sb:first-page><sb:last-page>797</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0020"><ce:label>[4]</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>H.D.</ce:given-name><ce:surname>Schworm</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Ygge</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Pansell</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Lennerstrand</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Assessment of Ocular Counterroll during Head Tilt Using Binocular Video Oculography</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Investigative Ophthalmology &amp; Visual Science</sb:maintitle></sb:title><sb:volume-nr>43</sb:volume-nr></sb:series><sb:date>2002</sb:date></sb:issue><sb:pages><sb:first-page>662</sb:first-page><sb:last-page>667</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0025"><ce:label>[5]</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.N.</ce:given-name><ce:surname>van der Geest</ce:surname></sb:author><sb:author><ce:given-name>M.A.</ce:given-name><ce:surname>Frens</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Recording eye movements with video-oculography and scleral search coils: a direct comparison of two methods</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of Neuroscience Methods</sb:maintitle></sb:title><sb:volume-nr>114</sb:volume-nr></sb:series><sb:date>2002</sb:date></sb:issue><sb:pages><sb:first-page>185</sb:first-page><sb:last-page>195</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0030"><ce:label>[6]</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>C.S.</ce:given-name><ce:surname>Lin</ce:surname></sb:author><sb:author><ce:given-name>L.W.</ce:given-name><ce:surname>Lue</ce:surname></sb:author><sb:author><ce:given-name>M.S.</ce:given-name><ce:surname>Yeh</ce:surname></sb:author><sb:author><ce:given-name>T.S.</ce:given-name><ce:surname>Hwang</ce:surname></sb:author><sb:author><ce:given-name>S.H.</ce:given-name><ce:surname>Lee</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A new image processing method for evaluating the pupillary responses in a HMD-type eye-tracking device</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Optics &amp; Laser Technology</sb:maintitle></sb:title><sb:volume-nr>35</sb:volume-nr></sb:series><sb:date>2003</sb:date></sb:issue><sb:pages><sb:first-page>505</sb:first-page><sb:last-page>515</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0035"><ce:label>[7]</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.S.J.</ce:given-name><ce:surname>Lee</ce:surname></sb:author><sb:author><ce:given-name>R.M.</ce:given-name><ce:surname>Haralick</ce:surname></sb:author><sb:author><ce:given-name>L.G.</ce:given-name><ce:surname>Shapiro</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Morphological Edge Detection</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE J. Robot. Automat</sb:maintitle></sb:title><sb:volume-nr>3</sb:volume-nr></sb:series><sb:date>1987</sb:date></sb:issue><sb:pages><sb:first-page>142</sb:first-page><sb:last-page>157</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0040"><ce:label>[8]</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Canny</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A Computational Approach to Edge Detection</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Pattern Analysis and Machine Intelligence</sb:maintitle></sb:title><sb:volume-nr>8</sb:volume-nr></sb:series><sb:date>1986</sb:date></sb:issue><sb:pages><sb:first-page>679</sb:first-page><sb:last-page>698</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0045"><ce:label>[9]</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>W.</ce:given-name><ce:surname>McIlhagga</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The Canny Edge Detector Revisited</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Int J Comput Vis</sb:maintitle></sb:title><sb:volume-nr>91</sb:volume-nr></sb:series><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>251</sb:first-page><sb:last-page>261</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0050"><ce:label>[10]</ce:label><sb:reference><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Bao</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Zhang</ce:surname></sb:author><sb:author><ce:given-name>X.</ce:given-name><ce:surname>Wu</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Canny Edge Detection Enhancement by Scale Multiplication</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Pattern Analysis and Machine Intelligence</sb:maintitle></sb:title><sb:volume-nr>27</sb:volume-nr></sb:series><sb:date>2005</sb:date></sb:issue><sb:pages><sb:first-page>1485</sb:first-page><sb:last-page>1490</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference></ce:bibliography-sec></ce:bibliography></tail></article></xocs:serial-item></xocs:doc></originalText></full-text-retrieval-response>