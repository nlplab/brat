<full-text-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/article/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:bk="http://www.elsevier.com/xml/bk/dtd"><coredata><prism:url>http://api.elsevier.com/content/article/pii/S088523081530036X</prism:url><dc:identifier>doi:10.1016/j.csl.2016.03.001</dc:identifier><eid>1-s2.0-S088523081530036X</eid><prism:doi>10.1016/j.csl.2016.03.001</prism:doi><pii>S0885-2308(15)30036-X</pii><dc:title>On the use of deep feedforward neural networks for automatic language identification </dc:title><prism:publicationName>Computer Speech &amp; Language</prism:publicationName><prism:aggregationType>Journal</prism:aggregationType><prism:issn>08852308</prism:issn><prism:volume>40</prism:volume><prism:startingPage>46</prism:startingPage><prism:endingPage>59</prism:endingPage><prism:pageRange>46-59</prism:pageRange><dc:format>text/xml</dc:format><prism:coverDate>2016-11-30</prism:coverDate><prism:coverDisplayDate>November 2016</prism:coverDisplayDate><prism:copyright>© 2016 The Authors. Published by Elsevier Ltd.</prism:copyright><prism:publisher>The Authors. Published by Elsevier Ltd.</prism:publisher><dc:creator>Lopez-Moreno, Ignacio</dc:creator><dc:creator>Gonzalez-Dominguez, Javier</dc:creator><dc:creator>Martinez, David</dc:creator><dc:creator>Plchot, Oldřich</dc:creator><dc:creator>Gonzalez-Rodriguez, Joaquin</dc:creator><dc:creator>Moreno, Pedro J.</dc:creator><dc:description>AbstractIn this work, we present a comprehensive study on the use of deep neural networks (DNNs) for automatic language identification (LID). Motivated by the recent success of using DNNs in acoustic modeling for speech recognition, we adapt DNNs to the problem of identifying the language in a given utterance from its short-term acoustic features. We propose two different DNN-based approaches. In the first one, the DNN acts as an end-to-end LID classifier, receiving as input the speech features and providing as output the estimated probabilities of the target languages. In the second approach, the DNN is used to extract bottleneck features that are then used as inputs for a state-of-the-art i-vector system. Experiments are conducted in two different scenarios: the complete NIST Language Recognition Evaluation dataset 2009 (LRE'09) and a subset of the Voice of America (VOA) data from LRE'09, in which all languages have the same amount of training data. Results for both datasets demonstrate that the DNN-based systems significantly outperform a state-of-art i-vector system when dealing with short-duration utterances. Furthermore, the combination of the DNN-based and the classical i-vector system leads to additional performance improvements (up to 45% of relative improvement in both EER and Cavg on 3s and 10s conditions, respectively).</dc:description><openaccess>1</openaccess><openaccessArticle>true</openaccessArticle><openaccessType>Full</openaccessType><openArchiveArticle>false</openArchiveArticle><openaccessSponsorName/><openaccessSponsorType>Author</openaccessSponsorType><openaccessUserLicense>http://creativecommons.org/licenses/by/4.0/</openaccessUserLicense><dcterms:subject>LID</dcterms:subject><dcterms:subject>DNN</dcterms:subject><dcterms:subject>Bottleneck</dcterms:subject><dcterms:subject>i-vectors</dcterms:subject><link rel="self" href="http://api.elsevier.com/content/article/pii/S088523081530036X"/><link rel="scidir" href="http://www.sciencedirect.com/science/article/pii/S088523081530036X"/></coredata><objects><object ref="ycsla768-fig-0001" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="78" size="14221">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0001.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0002" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="94" size="13821">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0002.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0003" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="127" size="12169">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0003.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0004" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="85" size="12382">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0004.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0005" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="127" size="13541">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0005.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0006" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="85" size="15340">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0006.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0007" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="85" size="16097">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0007.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0001" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="732" height="262" size="82903">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0001.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0002" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="565" height="243" size="55141">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0002.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0003" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="335" height="194" size="39186">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0003.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0004" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="731" height="283" size="58142">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0004.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0005" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="335" height="194" size="41866">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0005.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0006" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="732" height="284" size="73740">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0006.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla768-fig-0007" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="732" height="283" size="76895">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-ycsla768-fig-0007.jpg?httpAccept=%2A%2F%2A</object><object ref="si1" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="115" height="27" size="535">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si1.gif?httpAccept=%2A%2F%2A</object><object ref="si10" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="177" height="63" size="1421">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si10.gif?httpAccept=%2A%2F%2A</object><object ref="si12" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="37" height="29" size="321">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si12.gif?httpAccept=%2A%2F%2A</object><object ref="si13" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="344" height="31" size="1618">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si13.gif?httpAccept=%2A%2F%2A</object><object ref="si14" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="98" height="31" size="672">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si14.gif?httpAccept=%2A%2F%2A</object><object ref="si15" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="65" height="31" size="474">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si15.gif?httpAccept=%2A%2F%2A</object><object ref="si16" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="108" height="31" size="497">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si16.gif?httpAccept=%2A%2F%2A</object><object ref="si2" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="248" height="35" size="1309">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si2.gif?httpAccept=%2A%2F%2A</object><object ref="si21" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="77" height="29" size="484">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si21.gif?httpAccept=%2A%2F%2A</object><object ref="si22" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="98" height="23" size="632">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si22.gif?httpAccept=%2A%2F%2A</object><object ref="si23" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="35" height="29" size="277">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si23.gif?httpAccept=%2A%2F%2A</object><object ref="si25" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="108" height="23" size="653">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si25.gif?httpAccept=%2A%2F%2A</object><object ref="si3" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="150" height="50" size="842">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si3.gif?httpAccept=%2A%2F%2A</object><object ref="si4" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="148" height="69" size="1325">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si4.gif?httpAccept=%2A%2F%2A</object><object ref="si5" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="148" height="54" size="881">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si5.gif?httpAccept=%2A%2F%2A</object><object ref="si6" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="208" height="63" size="1442">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si6.gif?httpAccept=%2A%2F%2A</object><object ref="si7" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="221" height="63" size="1468">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si7.gif?httpAccept=%2A%2F%2A</object><object ref="si8" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="254" height="67" size="2186">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si8.gif?httpAccept=%2A%2F%2A</object><object ref="si9" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="387" height="63" size="2408">http://api.elsevier.com/content/object/eid/1-s2.0-S088523081530036X-si9.gif?httpAccept=%2A%2F%2A</object></objects><scopus-id>84971260739</scopus-id><scopus-eid>2-s2.0-84971260739</scopus-eid><link rel="abstract" href="http://api.elsevier.com/content/abstract/scopus_id/84971260739"/><originalText><xocs:doc xmlns:xoe="http://www.elsevier.com/xml/xoe/dtd" xsi:schemaLocation="http://www.elsevier.com/xml/xocs/dtd http://be-prod3a/schema/dtds/document/fulltext/xcr/xocs-article.xsd"><xocs:meta><xocs:content-family>serial</xocs:content-family><xocs:content-type>JL</xocs:content-type><xocs:cid>272453</xocs:cid><xocs:ssids><xocs:ssid type="alllist">291210</xocs:ssid><xocs:ssid type="subj">291718</xocs:ssid><xocs:ssid type="subj">291723</xocs:ssid><xocs:ssid type="subj">291743</xocs:ssid><xocs:ssid type="subj">291782</xocs:ssid><xocs:ssid type="subj">291874</xocs:ssid><xocs:ssid type="content">31</xocs:ssid><xocs:ssid type="oa">90</xocs:ssid></xocs:ssids><xocs:srctitle>Computer Speech &amp; Language</xocs:srctitle><xocs:normalized-srctitle>COMPUTERSPEECHLANGUAGE</xocs:normalized-srctitle><xocs:orig-load-date yyyymmdd="20160506">2016-05-06</xocs:orig-load-date><xocs:available-online-date yyyymmdd="20160506">2016-05-06</xocs:available-online-date><xocs:vor-load-date yyyymmdd="20160524">2016-05-24</xocs:vor-load-date><xocs:vor-available-online-date yyyymmdd="20160524">2016-05-24</xocs:vor-available-online-date><xocs:ew-transaction-id>2016-06-07T09:11:09</xocs:ew-transaction-id><xocs:eid>1-s2.0-S088523081530036X</xocs:eid><xocs:pii-formatted>S0885-2308(15)30036-X</xocs:pii-formatted><xocs:pii-unformatted>S088523081530036X</xocs:pii-unformatted><xocs:doi>10.1016/j.csl.2016.03.001</xocs:doi><xocs:item-stage>S300</xocs:item-stage><xocs:item-version-number>S300.1</xocs:item-version-number><xocs:item-weight>FULL-TEXT</xocs:item-weight><xocs:hub-eid>1-s2.0-S0885230816X00034</xocs:hub-eid><xocs:timestamp yyyymmdd="20160607">2016-06-07T04:19:19.162653-04:00</xocs:timestamp><xocs:dco>0</xocs:dco><xocs:tomb>0</xocs:tomb><xocs:date-search-begin>20161101</xocs:date-search-begin><xocs:date-search-end>20161130</xocs:date-search-end><xocs:year-nav>2016</xocs:year-nav><xocs:indexeddate epoch="1462532791">2016-05-06T11:06:31.853551Z</xocs:indexeddate><xocs:articleinfo>absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst primabst ref</xocs:articleinfo><xocs:issns><xocs:issn-primary-formatted>0885-2308</xocs:issn-primary-formatted><xocs:issn-primary-unformatted>08852308</xocs:issn-primary-unformatted></xocs:issns><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>NONE</xocs:funding-body-id><xocs:crossmark is-crossmark="1">true</xocs:crossmark><xocs:vol-first>40</xocs:vol-first><xocs:volume-list><xocs:volume>40</xocs:volume></xocs:volume-list><xocs:suppl>C</xocs:suppl><xocs:vol-iss-suppl-text>Volume 40</xocs:vol-iss-suppl-text><xocs:sort-order>3</xocs:sort-order><xocs:first-fp>46</xocs:first-fp><xocs:last-lp>59</xocs:last-lp><xocs:pages><xocs:first-page>46</xocs:first-page><xocs:last-page>59</xocs:last-page></xocs:pages><xocs:cover-date-orig><xocs:start-date>201611</xocs:start-date></xocs:cover-date-orig><xocs:cover-date-text>November 2016</xocs:cover-date-text><xocs:cover-date-start>2016-11-01</xocs:cover-date-start><xocs:cover-date-end>2016-11-30</xocs:cover-date-end><xocs:cover-date-year>2016</xocs:cover-date-year><xocs:hub-sec><xocs:hub-sec-title>Research Articles</xocs:hub-sec-title></xocs:hub-sec><xocs:document-type>article</xocs:document-type><xocs:document-subtype>fla</xocs:document-subtype><xocs:copyright-line>© 2016 The Authors. Published by Elsevier Ltd.</xocs:copyright-line><xocs:normalized-article-title>USEDEEPFEEDFORWARDNEURALNETWORKSFORAUTOMATICLANGUAGEIDENTIFICATION</xocs:normalized-article-title><xocs:normalized-first-auth-surname>LOPEZMORENO</xocs:normalized-first-auth-surname><xocs:normalized-first-auth-initial>I</xocs:normalized-first-auth-initial><xocs:item-toc><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>1</xocs:item-toc-label><xocs:item-toc-section-title>Introduction</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2</xocs:item-toc-label><xocs:item-toc-section-title>The baseline I-vector based system</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.1</xocs:item-toc-label><xocs:item-toc-section-title>Feature extraction</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.2</xocs:item-toc-label><xocs:item-toc-section-title>I-vector extraction</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.3</xocs:item-toc-label><xocs:item-toc-section-title>Classification backends</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3</xocs:item-toc-label><xocs:item-toc-section-title>The DNN-based LID system</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.1</xocs:item-toc-label><xocs:item-toc-section-title>Architecture</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.2</xocs:item-toc-label><xocs:item-toc-section-title>Implementing DNNs for language identification</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4</xocs:item-toc-label><xocs:item-toc-section-title>Bottleneck features: A hybrid approach</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>5</xocs:item-toc-label><xocs:item-toc-section-title>Fusion and calibration</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>6</xocs:item-toc-label><xocs:item-toc-section-title>Databases and evaluation metrics</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>6.1</xocs:item-toc-label><xocs:item-toc-section-title>Databases</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>6.2</xocs:item-toc-label><xocs:item-toc-section-title>Evaluation metrics</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>7</xocs:item-toc-label><xocs:item-toc-section-title>Results</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>7.1</xocs:item-toc-label><xocs:item-toc-section-title>Results using LRE09_BDS</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>7.1.1</xocs:item-toc-label><xocs:item-toc-section-title>DNN vs i-vector system</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>7.1.2</xocs:item-toc-label><xocs:item-toc-section-title>Bottlenecks</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>7.1.3</xocs:item-toc-label><xocs:item-toc-section-title>Temporal context and number of layers</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>7.1.4</xocs:item-toc-label><xocs:item-toc-section-title>Fusion and results per language</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>7.2</xocs:item-toc-label><xocs:item-toc-section-title>Results using LRE09_FULL</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>8</xocs:item-toc-label><xocs:item-toc-section-title>Summary</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:acknowledgment"><xocs:item-toc-section-title>Acknowledgment</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:bibliography"><xocs:item-toc-section-title>References</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc><xocs:references><xocs:ref-info refid="sr0010"><xocs:ref-normalized-surname>BRUMMER</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-normalized-initial>N</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>MEASURINGREFININGCALIBRATINGSPEAKERLANGUAGEINFORMATIONEXTRACTEDSPEECH</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0015"><xocs:ref-normalized-surname>BRUMMER</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>216</xocs:ref-first-fp><xocs:ref-last-lp>223</xocs:ref-last-lp><xocs:ref-normalized-initial>N</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSODYSSEY2012SPEAKERLANGUAGERECOGNITIONWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>DESCRIPTIONANALYSISBRNO276SYSTEMFORLRE2011</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0020"><xocs:ref-normalized-surname>BRUMMER</xocs:ref-normalized-surname><xocs:ref-normalized-initial>N</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0025"><xocs:ref-normalized-surname>BRUMMER</xocs:ref-normalized-surname><xocs:ref-pub-year>2006</xocs:ref-pub-year><xocs:ref-normalized-initial>N</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCODYSSEY</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>CALIBRATIONLANGUAGERECOGNITIONSCORES</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="or0010"/><xocs:ref-info refid="sr0030"><xocs:ref-normalized-surname>COLE</xocs:ref-normalized-surname><xocs:ref-pub-year>1989</xocs:ref-pub-year><xocs:ref-first-fp>525</xocs:ref-first-fp><xocs:ref-last-lp>529</xocs:ref-last-lp><xocs:ref-normalized-initial>R</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>COMMUNICATIONSCOMPUTERSSIGNALPROCESSING1989CONFERENCEPROCEEDINGIEEEPACIFICRIMCONFERENCE</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>LANGUAGEIDENTIFICATIONNEURALNETWORKSAFEASIBILITYSTUDY</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0035"><xocs:ref-normalized-surname>DAVIS</xocs:ref-normalized-surname><xocs:ref-pub-year>1980</xocs:ref-pub-year><xocs:ref-first-fp>357</xocs:ref-first-fp><xocs:ref-last-lp>366</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0040"><xocs:ref-normalized-surname>DEAN</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>1232</xocs:ref-first-fp><xocs:ref-last-lp>1240</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMS25</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>LARGESCALEDISTRIBUTEDDEEPNETWORKS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0045"><xocs:ref-normalized-surname>DEHAK</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>857</xocs:ref-first-fp><xocs:ref-last-lp>860</xocs:ref-last-lp><xocs:ref-normalized-initial>N</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>INTERSPEECHISCA</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>LANGUAGERECOGNITIONVIAIVECTORSDIMENSIONALITYREDUCTION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0050"><xocs:ref-normalized-surname>DEHAK</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>788</xocs:ref-first-fp><xocs:ref-last-lp>798</xocs:ref-last-lp><xocs:ref-normalized-initial>N</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0055"><xocs:ref-normalized-surname>FERRER</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>4414</xocs:ref-first-fp><xocs:ref-last-lp>4417</xocs:ref-last-lp><xocs:ref-normalized-initial>L</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>INTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSING</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>ACOMPARISONAPPROACHESFORMODELINGPROSODICFEATURESINSPEAKERRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0060"><xocs:ref-normalized-surname>FONTAINE</xocs:ref-normalized-surname><xocs:ref-pub-year>1997</xocs:ref-pub-year><xocs:ref-normalized-initial>V</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>FIFTHEUROPEANCONFERENCESPEECHCOMMUNICATIONTECHNOLOGYEUROSPEECH1997</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>NONLINEARDISCRIMINANTANALYSISFORIMPROVEDSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0065"><xocs:ref-normalized-surname>GONZALEZDOMINGUEZ</xocs:ref-normalized-surname><xocs:ref-pub-year>2014</xocs:ref-pub-year><xocs:ref-first-fp>1</xocs:ref-first-fp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0070"><xocs:ref-normalized-surname>GONZALEZDOMINGUEZ</xocs:ref-normalized-surname><xocs:ref-pub-year>2015</xocs:ref-pub-year><xocs:ref-first-fp>49</xocs:ref-first-fp><xocs:ref-last-lp>58</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0075"><xocs:ref-normalized-surname>GREZL</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-first-fp>757</xocs:ref-first-fp><xocs:ref-last-lp>760</xocs:ref-last-lp><xocs:ref-normalized-initial>F</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCIEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP2007</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>PROBABILISTICBOTTLENECKFEATURESFORLVCSRMEETINGS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0080"><xocs:ref-normalized-surname>GREZL</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-first-fp>2947</xocs:ref-first-fp><xocs:ref-last-lp>2950</xocs:ref-last-lp><xocs:ref-normalized-initial>F</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>INTERSPEECH2009INTERNATIONALSPEECHCOMMUNICATIONASSOCIATION</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>INVESTIGATIONBOTTLENECKFEATURESFORMEETINGSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0085"><xocs:ref-normalized-surname>HERMANSKY</xocs:ref-normalized-surname><xocs:ref-pub-year>1994</xocs:ref-pub-year><xocs:ref-first-fp>578</xocs:ref-first-fp><xocs:ref-last-lp>589</xocs:ref-last-lp><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0090"><xocs:ref-normalized-surname>HERMANSKY</xocs:ref-normalized-surname><xocs:ref-pub-year>2000</xocs:ref-pub-year><xocs:ref-first-fp>1635</xocs:ref-first-fp><xocs:ref-last-lp>1638</xocs:ref-last-lp><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROC</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>TANDEMCONNECTIONISTFEATUREEXTRACTIONFORCONVENTIONALHMMSYSTEMS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0095"><xocs:ref-normalized-surname>HINTON</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>82</xocs:ref-first-fp><xocs:ref-last-lp>97</xocs:ref-last-lp><xocs:ref-normalized-initial>G</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0100"><xocs:ref-normalized-surname>JIANG</xocs:ref-normalized-surname><xocs:ref-pub-year>2014</xocs:ref-pub-year><xocs:ref-first-fp>e100795</xocs:ref-first-fp><xocs:ref-normalized-initial>B</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0105"><xocs:ref-normalized-surname>KENNY</xocs:ref-normalized-surname><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0110"><xocs:ref-normalized-surname>KENNY</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>980</xocs:ref-first-fp><xocs:ref-last-lp>988</xocs:ref-last-lp><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0115"><xocs:ref-normalized-surname>LEENA</xocs:ref-normalized-surname><xocs:ref-pub-year>2005</xocs:ref-pub-year><xocs:ref-first-fp>404</xocs:ref-first-fp><xocs:ref-last-lp>408</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>INTELLIGENTSENSINGINFORMATIONPROCESSING2005PROCEEDINGS2005INTERNATIONALCONFERENCE</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>NEURALNETWORKCLASSIFIERSFORLANGUAGEIDENTIFICATIONUSINGPHONOTACTICPROSODICFEATURES</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0120"><xocs:ref-normalized-surname>LI</xocs:ref-normalized-surname><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-first-fp>1136</xocs:ref-first-fp><xocs:ref-last-lp>1159</xocs:ref-last-lp><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="or0015"/><xocs:ref-info refid="sr0125"><xocs:ref-normalized-surname>MARTINEZ</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>NIST2011LREWORKSHOPBOOKLET</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>I3ALANGUAGERECOGNITIONSYSTEMDESCRIPTIONFORNISTLRE2011</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0130"><xocs:ref-normalized-surname>MARTINEZ</xocs:ref-normalized-surname><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-first-fp>2133</xocs:ref-first-fp><xocs:ref-last-lp>2137</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>INTERSPEECH201314THANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATION</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>DYSARTHRIAINTELLIGIBILITYASSESSMENTINAFACTORANALYSISTOTALVARIABILITYSPACE</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0135"><xocs:ref-normalized-surname>MARTINEZ</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>861</xocs:ref-first-fp><xocs:ref-last-lp>864</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>INTERSPEECH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>LANGUAGERECOGNITIONINIVECTORSSPACE</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0140"><xocs:ref-normalized-surname>MATEJKA</xocs:ref-normalized-surname><xocs:ref-pub-year>2014</xocs:ref-pub-year><xocs:ref-first-fp>299</xocs:ref-first-fp><xocs:ref-last-lp>304</xocs:ref-last-lp><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>SPEAKERODYSSEY</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>NEURALNETWORKBOTTLENECKFEATURESFORLANGUAGEIDENTIFICATION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0145"><xocs:ref-normalized-surname>MCCREE</xocs:ref-normalized-surname><xocs:ref-pub-year>2014</xocs:ref-pub-year><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>IEEEODYSSEYSPEAKERLANGUAGERECOGNITIONWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>MULTICLASSDISCRIMINATIVETRAININGIVECTORLANGUAGERECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0150"><xocs:ref-normalized-surname>MOHAMED</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>14</xocs:ref-first-fp><xocs:ref-last-lp>22</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0155"><xocs:ref-normalized-surname>MOHAMED</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>4273</xocs:ref-first-fp><xocs:ref-last-lp>4276</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>ICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>UNDERSTANDINGHOWDEEPBELIEFNETWORKSPERFORMACOUSTICMODELLING</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0160"><xocs:ref-normalized-surname>MONTAVON</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-normalized-initial>G</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>NIPSWORKSHOPDEEPLEARNINGFORSPEECHRECOGNITIONRELATEDAPPLICATIONS</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>DEEPLEARNINGFORSPOKENLANGUAGEIDENTIFICATION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0165"><xocs:ref-normalized-surname>MUTHUSAMY</xocs:ref-normalized-surname><xocs:ref-pub-year>1994</xocs:ref-pub-year><xocs:ref-first-fp>33</xocs:ref-first-fp><xocs:ref-last-lp>41</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0170"><xocs:ref-normalized-surname>NIST</xocs:ref-normalized-surname></xocs:ref-info><xocs:ref-info refid="sr0175"><xocs:ref-normalized-surname>REYNOLDS</xocs:ref-normalized-surname><xocs:ref-pub-year>2000</xocs:ref-pub-year><xocs:ref-first-fp>19</xocs:ref-first-fp><xocs:ref-last-lp>41</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0180"><xocs:ref-normalized-surname>REYNOLDS</xocs:ref-normalized-surname><xocs:ref-pub-year>2003</xocs:ref-pub-year><xocs:ref-first-fp>784</xocs:ref-first-fp><xocs:ref-last-lp>787</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>IEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSING</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>SUPERSIDPROJECTEXPLOITINGHIGHLEVELINFORMATIONFORHIGHACCURACYSPEAKERRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0185"><xocs:ref-normalized-surname>RICHARDSON</xocs:ref-normalized-surname><xocs:ref-normalized-initial>F</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0190"><xocs:ref-normalized-surname>STURIM</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>5272</xocs:ref-first-fp><xocs:ref-last-lp>5275</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>ACOUSTICSSPEECHSIGNALPROCESSINGICASSP2011IEEEINTERNATIONALCONFERENCE</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>MITLL2010SPEAKERRECOGNITIONEVALUATIONSYSTEMSCALABLELANGUAGEINDEPENDENTSPEAKERRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0195"><xocs:ref-normalized-surname>TORRESCARRASQUILLO</xocs:ref-normalized-surname><xocs:ref-pub-year>2002</xocs:ref-pub-year><xocs:ref-first-fp>89</xocs:ref-first-fp><xocs:ref-last-lp>92</xocs:ref-last-lp><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>ICSLP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>APPROACHESLANGUAGEIDENTIFICATIONUSINGGAUSSIANMIXTUREMODELSSHIFTEDDELTACEPSTRALFEATURES</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0200"><xocs:ref-normalized-surname>WELLING</xocs:ref-normalized-surname><xocs:ref-pub-year>1999</xocs:ref-pub-year><xocs:ref-first-fp>761</xocs:ref-first-fp><xocs:ref-last-lp>764</xocs:ref-last-lp><xocs:ref-normalized-initial>L</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS1999IEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP99</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>IMPROVEDMETHODSFORVOCALTRACTNORMALIZATION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0205"><xocs:ref-normalized-surname>XIA</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>2230</xocs:ref-first-fp><xocs:ref-last-lp>2233</xocs:ref-last-lp><xocs:ref-normalized-initial>R</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>INTERSPEECH201213THANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATION</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>USINGIVECTORSPACEMODELFOREMOTIONRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0210"><xocs:ref-normalized-surname>YU</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>145</xocs:ref-first-fp><xocs:ref-last-lp>154</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0215"><xocs:ref-normalized-surname>ZEILER</xocs:ref-normalized-surname><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>38THINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>RECTIFIEDLINEARUNITSFORSPEECHPROCESSING</xocs:ref-normalized-article-title></xocs:ref-info></xocs:references><xocs:refkeys><xocs:refkey3>LOPEZMORENOX2016X46</xocs:refkey3><xocs:refkey4lp>LOPEZMORENOX2016X46X59</xocs:refkey4lp><xocs:refkey4ai>LOPEZMORENOX2016X46XI</xocs:refkey4ai><xocs:refkey5>LOPEZMORENOX2016X46X59XI</xocs:refkey5></xocs:refkeys><xocs:open-access><xocs:oa-article-status is-open-access="1" is-open-archive="0">Full</xocs:oa-article-status><xocs:oa-access-effective-date>2016-05-02T13:50:53Z</xocs:oa-access-effective-date><xocs:oa-sponsor><xocs:oa-sponsor-type>Author</xocs:oa-sponsor-type></xocs:oa-sponsor><xocs:oa-user-license>http://creativecommons.org/licenses/by/4.0/</xocs:oa-user-license></xocs:open-access><xocs:self-archiving><xocs:sa-start-date>2018-05-24T00:00:00Z</xocs:sa-start-date><xocs:sa-embargo-status>UnderEmbargo</xocs:sa-embargo-status><xocs:sa-user-license>http://creativecommons.org/licenses/by-nc-nd/4.0/</xocs:sa-user-license></xocs:self-archiving><xocs:copyright-info><xocs:cp-license-lines><xocs:cp-license-line lang="en">This is an open access article under the CC BY license.</xocs:cp-license-line></xocs:cp-license-lines><xocs:cp-notices><xocs:cp-notice lang="en">© 2016 The Authors. Published by Elsevier Ltd.</xocs:cp-notice></xocs:cp-notices></xocs:copyright-info><xocs:attachment-metadata-doc><xocs:attachment-set-type>item</xocs:attachment-set-type><xocs:pii-formatted>S0885-2308(15)30036-X</xocs:pii-formatted><xocs:pii-unformatted>S088523081530036X</xocs:pii-unformatted><xocs:eid>1-s2.0-S088523081530036X</xocs:eid><xocs:doi>10.1016/j.csl.2016.03.001</xocs:doi><xocs:cid>272453</xocs:cid><xocs:timestamp>2016-06-07T04:19:19.162653-04:00</xocs:timestamp><xocs:cover-date-start>2016-11-01</xocs:cover-date-start><xocs:cover-date-end>2016-11-30</xocs:cover-date-end><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>NONE</xocs:funding-body-id><xocs:attachments><xocs:web-pdf><xocs:attachment-eid>1-s2.0-S088523081530036X-main.pdf</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/MAIN/application/pdf/035c1a1a2528f1d9aab5fc070691bc64/main.pdf</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/MAIN/application/pdf/035c1a1a2528f1d9aab5fc070691bc64/main.pdf</xocs:ucs-locator><xocs:filename>main.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:pdf-optimized>true</xocs:pdf-optimized><xocs:filesize>1731078</xocs:filesize><xocs:web-pdf-purpose>MAIN</xocs:web-pdf-purpose><xocs:web-pdf-page-count>14</xocs:web-pdf-page-count><xocs:web-pdf-images><xocs:web-pdf-image><xocs:attachment-eid>1-s2.0-S088523081530036X-main_1.png</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/PREVIEW/image/png/b76f7f30895dfacdba6950c356e05bfc/main_1.png</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/PREVIEW/image/png/b76f7f30895dfacdba6950c356e05bfc/main_1.png</xocs:ucs-locator><xocs:filename>main_1.png</xocs:filename><xocs:extension>png</xocs:extension><xocs:filesize>45629</xocs:filesize><xocs:pixel-height>849</xocs:pixel-height><xocs:pixel-width>656</xocs:pixel-width><xocs:attachment-type>IMAGE-WEB-PDF</xocs:attachment-type><xocs:pdf-page-num>1</xocs:pdf-page-num></xocs:web-pdf-image></xocs:web-pdf-images></xocs:web-pdf><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0001.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0001/THUMBNAIL/image/gif/34a721f2bbfae23f184eb2c5cf51c395/ycsla768-fig-0001.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0001/THUMBNAIL/image/gif/34a721f2bbfae23f184eb2c5cf51c395/ycsla768-fig-0001.sml</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0001</xocs:file-basename><xocs:filename>ycsla768-fig-0001.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>14221</xocs:filesize><xocs:pixel-height>78</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0002.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0002/THUMBNAIL/image/gif/3834fe77539ea445006fdb0e93744480/ycsla768-fig-0002.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0002/THUMBNAIL/image/gif/3834fe77539ea445006fdb0e93744480/ycsla768-fig-0002.sml</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0002</xocs:file-basename><xocs:filename>ycsla768-fig-0002.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>13821</xocs:filesize><xocs:pixel-height>94</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0003.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0003/THUMBNAIL/image/gif/c8212f3eed5e982b84fb7210a0c9e304/ycsla768-fig-0003.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0003/THUMBNAIL/image/gif/c8212f3eed5e982b84fb7210a0c9e304/ycsla768-fig-0003.sml</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0003</xocs:file-basename><xocs:filename>ycsla768-fig-0003.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>12169</xocs:filesize><xocs:pixel-height>127</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0004.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0004/THUMBNAIL/image/gif/468c592fbf7cc973ff92b1b5bfa6f13a/ycsla768-fig-0004.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0004/THUMBNAIL/image/gif/468c592fbf7cc973ff92b1b5bfa6f13a/ycsla768-fig-0004.sml</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0004</xocs:file-basename><xocs:filename>ycsla768-fig-0004.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>12382</xocs:filesize><xocs:pixel-height>85</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0005.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0005/THUMBNAIL/image/gif/375cf9eb392c24fea41da7bcadc92d3b/ycsla768-fig-0005.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0005/THUMBNAIL/image/gif/375cf9eb392c24fea41da7bcadc92d3b/ycsla768-fig-0005.sml</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0005</xocs:file-basename><xocs:filename>ycsla768-fig-0005.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>13541</xocs:filesize><xocs:pixel-height>127</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0006.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0006/THUMBNAIL/image/gif/471e0dfa909c42b69fa941aecfa35fa4/ycsla768-fig-0006.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0006/THUMBNAIL/image/gif/471e0dfa909c42b69fa941aecfa35fa4/ycsla768-fig-0006.sml</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0006</xocs:file-basename><xocs:filename>ycsla768-fig-0006.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>15340</xocs:filesize><xocs:pixel-height>85</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0007.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0007/THUMBNAIL/image/gif/ba748b47dce5bb376b23d22591b4d6e1/ycsla768-fig-0007.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0007/THUMBNAIL/image/gif/ba748b47dce5bb376b23d22591b4d6e1/ycsla768-fig-0007.sml</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0007</xocs:file-basename><xocs:filename>ycsla768-fig-0007.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>16097</xocs:filesize><xocs:pixel-height>85</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0001.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0001/DOWNSAMPLED/image/jpeg/86643969ee2bf8d87441b0456bb06139/ycsla768-fig-0001.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0001/DOWNSAMPLED/image/jpeg/86643969ee2bf8d87441b0456bb06139/ycsla768-fig-0001.jpg</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0001</xocs:file-basename><xocs:filename>ycsla768-fig-0001.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>82903</xocs:filesize><xocs:pixel-height>262</xocs:pixel-height><xocs:pixel-width>732</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0002.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0002/DOWNSAMPLED/image/jpeg/7c1026ef0dbd84cfe7f8634a0536e5cf/ycsla768-fig-0002.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0002/DOWNSAMPLED/image/jpeg/7c1026ef0dbd84cfe7f8634a0536e5cf/ycsla768-fig-0002.jpg</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0002</xocs:file-basename><xocs:filename>ycsla768-fig-0002.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>55141</xocs:filesize><xocs:pixel-height>243</xocs:pixel-height><xocs:pixel-width>565</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0003.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0003/DOWNSAMPLED/image/jpeg/ab7209f9bddfff77ffb05dc6085e9e00/ycsla768-fig-0003.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0003/DOWNSAMPLED/image/jpeg/ab7209f9bddfff77ffb05dc6085e9e00/ycsla768-fig-0003.jpg</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0003</xocs:file-basename><xocs:filename>ycsla768-fig-0003.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>39186</xocs:filesize><xocs:pixel-height>194</xocs:pixel-height><xocs:pixel-width>335</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0004.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0004/DOWNSAMPLED/image/jpeg/b73a0857fb28e7cf90b031ca8502b9dd/ycsla768-fig-0004.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0004/DOWNSAMPLED/image/jpeg/b73a0857fb28e7cf90b031ca8502b9dd/ycsla768-fig-0004.jpg</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0004</xocs:file-basename><xocs:filename>ycsla768-fig-0004.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>58142</xocs:filesize><xocs:pixel-height>283</xocs:pixel-height><xocs:pixel-width>731</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0005.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0005/DOWNSAMPLED/image/jpeg/0209e967d1fb8857ac6aef06d1ad70a6/ycsla768-fig-0005.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0005/DOWNSAMPLED/image/jpeg/0209e967d1fb8857ac6aef06d1ad70a6/ycsla768-fig-0005.jpg</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0005</xocs:file-basename><xocs:filename>ycsla768-fig-0005.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>41866</xocs:filesize><xocs:pixel-height>194</xocs:pixel-height><xocs:pixel-width>335</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0006.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0006/DOWNSAMPLED/image/jpeg/2504f3260322172a37987e0520aa9c43/ycsla768-fig-0006.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0006/DOWNSAMPLED/image/jpeg/2504f3260322172a37987e0520aa9c43/ycsla768-fig-0006.jpg</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0006</xocs:file-basename><xocs:filename>ycsla768-fig-0006.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>73740</xocs:filesize><xocs:pixel-height>284</xocs:pixel-height><xocs:pixel-width>732</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-ycsla768-fig-0007.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/ycsla768-fig-0007/DOWNSAMPLED/image/jpeg/43857bc1deddc8bb924789d0b4bfcb73/ycsla768-fig-0007.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/ycsla768-fig-0007/DOWNSAMPLED/image/jpeg/43857bc1deddc8bb924789d0b4bfcb73/ycsla768-fig-0007.jpg</xocs:ucs-locator><xocs:file-basename>ycsla768-fig-0007</xocs:file-basename><xocs:filename>ycsla768-fig-0007.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>76895</xocs:filesize><xocs:pixel-height>283</xocs:pixel-height><xocs:pixel-width>732</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si1.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/033ba495d1515c28e7e0d5853f116696/si1.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/033ba495d1515c28e7e0d5853f116696/si1.gif</xocs:ucs-locator><xocs:file-basename>si1</xocs:file-basename><xocs:filename>si1.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>535</xocs:filesize><xocs:pixel-height>27</xocs:pixel-height><xocs:pixel-width>115</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si10.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/6995df24f60d951d0aed7f4284e86dee/si10.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/6995df24f60d951d0aed7f4284e86dee/si10.gif</xocs:ucs-locator><xocs:file-basename>si10</xocs:file-basename><xocs:filename>si10.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1421</xocs:filesize><xocs:pixel-height>63</xocs:pixel-height><xocs:pixel-width>177</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si12.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/49b8bdf561177551f15cfac3f60b9872/si12.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/49b8bdf561177551f15cfac3f60b9872/si12.gif</xocs:ucs-locator><xocs:file-basename>si12</xocs:file-basename><xocs:filename>si12.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>321</xocs:filesize><xocs:pixel-height>29</xocs:pixel-height><xocs:pixel-width>37</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si13.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/2f68cb6f3f20386f1f2fdafae7d01bcc/si13.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/2f68cb6f3f20386f1f2fdafae7d01bcc/si13.gif</xocs:ucs-locator><xocs:file-basename>si13</xocs:file-basename><xocs:filename>si13.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1618</xocs:filesize><xocs:pixel-height>31</xocs:pixel-height><xocs:pixel-width>344</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si14.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/ac5c6b0d2412d83a728631cbef176607/si14.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/ac5c6b0d2412d83a728631cbef176607/si14.gif</xocs:ucs-locator><xocs:file-basename>si14</xocs:file-basename><xocs:filename>si14.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>672</xocs:filesize><xocs:pixel-height>31</xocs:pixel-height><xocs:pixel-width>98</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si15.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/cf3163abc6f757fa4d14c79d6a6f20e6/si15.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/cf3163abc6f757fa4d14c79d6a6f20e6/si15.gif</xocs:ucs-locator><xocs:file-basename>si15</xocs:file-basename><xocs:filename>si15.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>474</xocs:filesize><xocs:pixel-height>31</xocs:pixel-height><xocs:pixel-width>65</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si16.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/82bea24c2faf77b21e212267090cf5a7/si16.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/82bea24c2faf77b21e212267090cf5a7/si16.gif</xocs:ucs-locator><xocs:file-basename>si16</xocs:file-basename><xocs:filename>si16.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>497</xocs:filesize><xocs:pixel-height>31</xocs:pixel-height><xocs:pixel-width>108</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si2.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/fb5c26f91c4cbee8104b693117943300/si2.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/fb5c26f91c4cbee8104b693117943300/si2.gif</xocs:ucs-locator><xocs:file-basename>si2</xocs:file-basename><xocs:filename>si2.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1309</xocs:filesize><xocs:pixel-height>35</xocs:pixel-height><xocs:pixel-width>248</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si21.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/e70c2f147031a64b7ca208ac65cbf833/si21.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/e70c2f147031a64b7ca208ac65cbf833/si21.gif</xocs:ucs-locator><xocs:file-basename>si21</xocs:file-basename><xocs:filename>si21.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>484</xocs:filesize><xocs:pixel-height>29</xocs:pixel-height><xocs:pixel-width>77</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si22.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/4886ae17c9b38366eed2c76cadd3130b/si22.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/4886ae17c9b38366eed2c76cadd3130b/si22.gif</xocs:ucs-locator><xocs:file-basename>si22</xocs:file-basename><xocs:filename>si22.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>632</xocs:filesize><xocs:pixel-height>23</xocs:pixel-height><xocs:pixel-width>98</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si23.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/ec24d7e253be54f9df35b5dff3f8e263/si23.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/ec24d7e253be54f9df35b5dff3f8e263/si23.gif</xocs:ucs-locator><xocs:file-basename>si23</xocs:file-basename><xocs:abstract-attachment>true</xocs:abstract-attachment><xocs:filename>si23.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>277</xocs:filesize><xocs:pixel-height>29</xocs:pixel-height><xocs:pixel-width>35</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si25.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/39b9337fa4ae9efd98811bb3e86c0614/si25.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/39b9337fa4ae9efd98811bb3e86c0614/si25.gif</xocs:ucs-locator><xocs:file-basename>si25</xocs:file-basename><xocs:filename>si25.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>653</xocs:filesize><xocs:pixel-height>23</xocs:pixel-height><xocs:pixel-width>108</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si3.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/deafde0b93965276aea3db894f4ae9a1/si3.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/deafde0b93965276aea3db894f4ae9a1/si3.gif</xocs:ucs-locator><xocs:file-basename>si3</xocs:file-basename><xocs:filename>si3.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>842</xocs:filesize><xocs:pixel-height>50</xocs:pixel-height><xocs:pixel-width>150</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si4.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/f29ac37e5e12c320b6a9308a08eea5ec/si4.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/f29ac37e5e12c320b6a9308a08eea5ec/si4.gif</xocs:ucs-locator><xocs:file-basename>si4</xocs:file-basename><xocs:filename>si4.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1325</xocs:filesize><xocs:pixel-height>69</xocs:pixel-height><xocs:pixel-width>148</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si5.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/ac7be8949444daef413825eacfe3aa66/si5.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/ac7be8949444daef413825eacfe3aa66/si5.gif</xocs:ucs-locator><xocs:file-basename>si5</xocs:file-basename><xocs:filename>si5.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>881</xocs:filesize><xocs:pixel-height>54</xocs:pixel-height><xocs:pixel-width>148</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si6.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/fd42ee515c9a5ab6cabbf330e37c2098/si6.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/fd42ee515c9a5ab6cabbf330e37c2098/si6.gif</xocs:ucs-locator><xocs:file-basename>si6</xocs:file-basename><xocs:filename>si6.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1442</xocs:filesize><xocs:pixel-height>63</xocs:pixel-height><xocs:pixel-width>208</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si7.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/8a7f6a660b2dcdb8f4dc2b2d0a9f69f2/si7.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/8a7f6a660b2dcdb8f4dc2b2d0a9f69f2/si7.gif</xocs:ucs-locator><xocs:file-basename>si7</xocs:file-basename><xocs:filename>si7.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1468</xocs:filesize><xocs:pixel-height>63</xocs:pixel-height><xocs:pixel-width>221</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si8.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/b901c5955aa87b2cbf73703ed9f2d663/si8.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/b901c5955aa87b2cbf73703ed9f2d663/si8.gif</xocs:ucs-locator><xocs:file-basename>si8</xocs:file-basename><xocs:filename>si8.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>2186</xocs:filesize><xocs:pixel-height>67</xocs:pixel-height><xocs:pixel-width>254</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S088523081530036X-si9.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S088523081530036X/STRIPIN/image/gif/f6eba5c88dcd2117740a49fa8effd5b3/si9.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S088523081530036X/STRIPIN/image/gif/f6eba5c88dcd2117740a49fa8effd5b3/si9.gif</xocs:ucs-locator><xocs:file-basename>si9</xocs:file-basename><xocs:filename>si9.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>2408</xocs:filesize><xocs:pixel-height>63</xocs:pixel-height><xocs:pixel-width>387</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment></xocs:attachments></xocs:attachment-metadata-doc></xocs:meta><xocs:serial-item><article xmlns="http://www.elsevier.com/xml/ja/dtd" docsubtype="fla" xml:lang="en" version="5.4"><item-info><jid>YCSLA</jid><aid>768</aid><ce:pii>S0885-2308(15)30036-X</ce:pii><ce:doi>10.1016/j.csl.2016.03.001</ce:doi><ce:copyright type="other" year="2016">The Authors</ce:copyright></item-info><ce:floats><ce:figure id="f0010"><ce:label>Fig. 1</ce:label><ce:caption id="ca0010"><ce:simple-para id="sp0020" view="all">Pipeline process from the waveform to the final score (left). DNN topology (middle). DNN description (right).</ce:simple-para></ce:caption><ce:link id="ln0050" locator="ycsla768-fig-0001"/></ce:figure><ce:figure id="f0015"><ce:label>Fig. 2</ce:label><ce:caption id="ca0015"><ce:simple-para id="sp0025" view="all">Frame level probabilities of a DNN-based LID system (8 languages selected) evaluated over an English-USA (4s) test utterance.</ce:simple-para></ce:caption><ce:link id="ln0055" locator="ycsla768-fig-0002"/></ce:figure><ce:figure id="f0020"><ce:label>Fig. 3</ce:label><ce:caption id="ca0025"><ce:simple-para id="sp0035" view="all">DNN versus i-vector system performance (average EER) in function of test utterance segment duration (LRE09_BDS corpus).</ce:simple-para></ce:caption><ce:link id="ln0110" locator="ycsla768-fig-0003"/></ce:figure><ce:figure id="f0025"><ce:label>Fig. 4</ce:label><ce:caption id="ca0040"><ce:simple-para id="sp0050" view="all">i-vector, BN and fusion system performance comparison (average EER) per language on LRE09_BDS dataset. Errors bars for 30s, 10s and 3s are superimposed, and therefore, representing the actual error for every condition.</ce:simple-para></ce:caption><ce:link id="ln0135" locator="ycsla768-fig-0004"/></ce:figure><ce:figure id="f0030"><ce:label>Fig. 5</ce:label><ce:caption id="ca0045"><ce:simple-para id="sp0055" view="all">DNN vs i-vector system performance (EER) in function of test utterance segment duration (LRE09_FULL corpus).</ce:simple-para></ce:caption><ce:link id="ln0140" locator="ycsla768-fig-0005"/></ce:figure><ce:figure id="f0035"><ce:label>Fig. 6</ce:label><ce:caption id="ca0050"><ce:simple-para id="sp0060" view="all">i-vector, BN and fusion system performance comparison (average EER) per language on LRE09_FULL dataset. The DNN used to extract bottleneck features was trained just with the 8 target languages of LRE09_BDS (on the left of the vertical dashed line). Errors bars for 30s, 10s and 3s are superimposed, and therefore, representing the actual error for every condition.</ce:simple-para></ce:caption><ce:link id="ln0145" locator="ycsla768-fig-0006"/></ce:figure><ce:figure id="f0040"><ce:label>Fig. 7</ce:label><ce:caption id="ca0055"><ce:simple-para id="sp0065" view="all">i-vector, BN and fusion systems performance comparison (average EER) per language on LRE09_FULL dataset. The DNN used to extract bottleneck features was trained with all the 23 target languages. Errors bars for 30s, 10s and 3s are superimposed, and therefore, representing the actual error for every condition.</ce:simple-para></ce:caption><ce:link id="ln0150" locator="ycsla768-fig-0007"/></ce:figure><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0010"><ce:label>Table 1</ce:label><ce:caption id="ca0020"><ce:simple-para id="sp0030" view="all">Distribution of training hours per languages and eval files in used datasets LRE09_BDS and LRE09_FULL.</ce:simple-para></ce:caption><tgroup cols="9"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><colspec colname="col7" colnum="7"/><colspec colname="col8" colnum="8"/><colspec colname="col9" colnum="9"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="2" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col5" namest="col2" align="left">LRE09_BDS</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col9" namest="col6" align="left">LRE09_FULL</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Train (#hours)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col5" namest="col3" align="left">Eval (#files)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Train (#hours)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col9" namest="col7" align="left">Eval (#files)</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">03s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">10s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">03s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">10s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30s</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">amha</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">6h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">411</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">391</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">379</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">bosn</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">371</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">359</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">331</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">cant</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">39h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">349</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">347</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">375</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">creo</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">6h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">347</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">312</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">307</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">croa</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">1.5h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">390</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">369</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">364</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">dari</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">6h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">397</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">382</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">369</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">engi</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">18h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">511</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">533</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">580</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">engl</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">200h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">383</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">369</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">373</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">127h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">866</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">836</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">913</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">fars</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">200h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">338</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">338</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">338</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">38h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">385</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">383</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">391</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">fren</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">200h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">395</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">395</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">395</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">35.5h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">401</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">394</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">388</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">geor</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">403</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">396</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">398</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">haus</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">6h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">430</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">382</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">345</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">hind</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">69h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">640</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">614</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">668</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">kore</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">77.5h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">460</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">450</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">453</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">mand</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">200h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">404</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">390</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">387</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">244h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">977</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">971</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1028</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">pash</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">200h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">406</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">391</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">388</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">6h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">404</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">391</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">388</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">port</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">6h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">457</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">377</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">339</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">russ</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">200h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">257</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">253</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">254</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">66h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">484</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">479</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">523</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">span</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">200h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">402</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">383</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">370</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">114h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">398</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">383</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">370</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">turk</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">6h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">396</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">394</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">392</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">ukra</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">1h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">403</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">383</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">375</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">urdu</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">200h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">358</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">344</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">339</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">13h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">386</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">372</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">371</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">viet</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">56h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">282</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">279</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">315</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">OOS</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">200h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2510h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">–</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">–</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">TOTAL</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">1800h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2943</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2863</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2844</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3458.5h</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">10548</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">10177</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">10362</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0015"><ce:label>Table 2</ce:label><ce:caption id="ca0030"><ce:simple-para id="sp0040" view="all">Performance for individual and fusion systems – average EER in % and <mml:math altimg="si21.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math> – on the balanced <mml:math altimg="si22.gif" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mn>09</mml:mn><mml:mtext>_</mml:mtext><mml:mi>B</mml:mi><mml:mi>D</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:math> dataset by test duration. All DNN family systems {DNN, DNN_BN and BN} come from a DNN with 4 layers and context of ±10 frames.</ce:simple-para></ce:caption><tgroup cols="4"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="2" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col4" namest="col2" align="left">Equal error rate (%)/<mml:math altimg="si23.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math> (×100)</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col4" namest="col2" align="left">LRE09_BDS</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">10s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30s</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">i-vector</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">10.20/10.39</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">1.45/1.54</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">0.21/0.28</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">DNN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">6.42/6.79</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">1.62/1.72</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">0.94/0.94</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">DNN_BN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">6.43/6.61</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">1.65/1.68</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">0.97/0.98</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">BN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">6.73/7.03</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">1.16/1.20</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">0.43/0.56</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">i-vector + DNN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">5.86/6.17</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">0.92/1.03</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">0.20/0.28</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">i-vector + DNN_BN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">5.87/6.02</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">0.91/1.06</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/">0.19/0.27</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">i-vector + BN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/"><bold>5.82/6.16</bold></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/"><bold>0.81/0.85</bold></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="/"><bold>0.18/0.20</bold></entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0020"><ce:label>Table 3</ce:label><ce:caption id="ca0035"><ce:simple-para id="sp0045" view="all">Performance of DNN-based systems as a function of the number of layers and temporal context used. Results on LRE09_BDS are reported as average EER on the 8 languages (%).</ce:simple-para></ce:caption><tgroup cols="19"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><colspec colname="col7" colnum="7"/><colspec colname="col8" colnum="8"/><colspec colname="col9" colnum="9"/><colspec colname="col10" colnum="10"/><colspec colname="col11" colnum="11"/><colspec colname="col12" colnum="12"/><colspec colname="col13" colnum="13"/><colspec colname="col14" colnum="14"/><colspec colname="col15" colnum="15"/><colspec colname="col16" colnum="16"/><colspec colname="col17" colnum="17"/><colspec colname="col18" colnum="18"/><colspec colname="col19" colnum="19"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="3" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col19" namest="col2" align="left">Equal error rate (%)</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col10" namest="col2" align="left">4 Layers</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col19" namest="col11" align="left">8 Layers</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col4" namest="col2" align="left">0C-0C</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col7" namest="col5" align="left">5C-5C</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col10" namest="col8" align="left">10C-10C</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col13" namest="col11" align="left">0C-0C</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col16" namest="col14" align="left">5C-5C</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col19" namest="col17" align="left">10C-10C</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">10s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">10s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">10s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">10s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">10s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">10s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">DNN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2.00</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.07</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.66</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.64</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.95</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.42</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.62</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.94</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.90</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.94</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.98</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.40</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.72</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.92</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.23</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.61</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.94</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">DNN_BN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.21</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2.17</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.16</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.77</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.83</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.04</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.43</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.65</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.97</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.87</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.91</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.96</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><bold>6.17</bold></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.71</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.98</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.36</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.58</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.85</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">BN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.43</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.32</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.41</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">7.07</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><bold>1.13</bold></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char="."><bold>0.38</bold></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.73</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.16</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.43</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">9.36</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2.01</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.68</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">8.74</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.43</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.59</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">9.17</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.84</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">0.76</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0025"><ce:label>Table 4</ce:label><ce:caption id="ca0060"><ce:simple-para id="sp0070" view="all">Performance for individual and fusion systems – average EER in % and <mml:math altimg="si23.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math> × 100 – on full <mml:math altimg="si25.gif" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mn>09</mml:mn><mml:mtext>_</mml:mtext><mml:mi mathvariant="italic">FULL</mml:mi></mml:mrow></mml:math> dataset by test duration. All DNN family systems {DNN, DNN_BN and BN} come from a DNN with 4 layers and context of ±10 frames.</ce:simple-para></ce:caption><tgroup cols="4"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="2" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col4" namest="col2" align="left">Equal error rate (%)/<mml:math altimg="si23.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math> (×100)</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col4" namest="col2" align="left">LRE09_FULL</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">03s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">10s</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30s</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">i-vector</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">15.74/16.37</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5.30/6.24</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.33/2.90</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">DNN_BN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">13.49/14.21</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">6.38/7.11</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">4.37/4.88</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">BN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">13.52/14.19</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5.58/6.21</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3.24/3.77</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">i-vector + DNN_BN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">11.93/12.76</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3.80/4.59</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">1.94/2.28</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">i-vector + BN</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><bold>11.19/11.87</bold></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><bold>3.61/4.13</bold></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><bold>1.85/2.21</bold></entry></row></tbody></tgroup></ce:table></ce:floats><head><ce:title id="tit0010">On the use of deep feedforward neural networks for automatic language identification</ce:title><ce:author-group id="aug0010"><ce:author id="au0010"><ce:given-name>Ignacio</ce:given-name><ce:surname>Lopez-Moreno</ce:surname><ce:cross-ref id="crf0010" refid="af0010"><ce:sup loc="post">a</ce:sup></ce:cross-ref><ce:cross-ref id="crf0015" refid="co0010">*</ce:cross-ref><ce:e-address id="eadd0010" type="email">elnota@google.com</ce:e-address></ce:author><ce:author id="au0015"><ce:given-name>Javier</ce:given-name><ce:surname>Gonzalez-Dominguez</ce:surname><ce:cross-ref id="crf0020" refid="af0015"><ce:sup loc="post">b</ce:sup></ce:cross-ref></ce:author><ce:author id="au0020"><ce:given-name>David</ce:given-name><ce:surname>Martinez</ce:surname><ce:cross-ref id="crf0025" refid="af0020"><ce:sup loc="post">c</ce:sup></ce:cross-ref></ce:author><ce:author id="au0025"><ce:given-name>Oldřich</ce:given-name><ce:surname>Plchot</ce:surname><ce:cross-ref id="crf0030" refid="af0025"><ce:sup loc="post">d</ce:sup></ce:cross-ref></ce:author><ce:author id="au0030"><ce:given-name>Joaquin</ce:given-name><ce:surname>Gonzalez-Rodriguez</ce:surname><ce:cross-ref id="crf0035" refid="af0015"><ce:sup loc="post">b</ce:sup></ce:cross-ref></ce:author><ce:author id="au0035"><ce:given-name>Pedro J.</ce:given-name><ce:surname>Moreno</ce:surname><ce:cross-ref id="crf0040" refid="af0010"><ce:sup loc="post">a</ce:sup></ce:cross-ref></ce:author><ce:affiliation id="af0010"><ce:label>a</ce:label><ce:textfn id="tx0010">Google Inc., New York, USA</ce:textfn><sa:affiliation><sa:organization>Google Inc.</sa:organization><sa:state>New York</sa:state><sa:country>USA</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0015"><ce:label>b</ce:label><ce:textfn id="tx0015">ATVS-Biometric Recognition Group, Universidad Autonoma de Madrid, Madrid, Spain</ce:textfn><sa:affiliation><sa:organization>ATVS-Biometric Recognition Group</sa:organization><sa:organization>Universidad Autonoma de Madrid</sa:organization><sa:state>Madrid</sa:state><sa:country>Spain</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0020"><ce:label>c</ce:label><ce:textfn id="tx0020">I3A, Zaragoza, Spain</ce:textfn><sa:affiliation><sa:organization>I3A</sa:organization><sa:city>Zaragoza</sa:city><sa:country>Spain</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0025"><ce:label>d</ce:label><ce:textfn id="tx0025">Brno University of Technology, Brno, Czech Republic</ce:textfn><sa:affiliation><sa:organization>Brno University of Technology</sa:organization><sa:city>Brno</sa:city><sa:country>Czech Republic</sa:country></sa:affiliation></ce:affiliation><ce:correspondence id="co0010"><ce:label>*</ce:label><ce:text id="te0010">Corresponding author at: Google Inc, 76 Ninth Ave. P.C. 10011, New York, NY. Tel.: 9174054991.</ce:text><sa:affiliation><sa:organization>Google Inc</sa:organization><sa:address-line>76 Ninth Ave. P.C.</sa:address-line><sa:city>New York</sa:city><sa:state>NY</sa:state><sa:postal-code>10011</sa:postal-code></sa:affiliation></ce:correspondence></ce:author-group><ce:date-received day="30" month="10" year="2015"/><ce:date-accepted day="18" month="3" year="2016"/><ce:abstract class="author-highlights" id="ab0010" xml:lang="en" view="all"><ce:section-title id="st0010">Highlights</ce:section-title><ce:abstract-sec id="abs0010" view="all"><ce:simple-para id="sp0010" view="all"><ce:list id="ulist0010"><ce:list-item id="u0010"><ce:label>•</ce:label><ce:para id="p0010" view="all">This work presents a comprehensive study on the use of deep neural networks for automatic language identification.</ce:para></ce:list-item><ce:list-item id="u0015"><ce:label>•</ce:label><ce:para id="p0015" view="all">It includes a detailed performance analysis for different data selection strategies and DNN architectures.</ce:para></ce:list-item><ce:list-item id="u0020"><ce:label>•</ce:label><ce:para id="p0020" view="all">Proposed systems are tested on the NIST Language Recognition Evaluation 2009, against an state-of-the-art i-vector baseline.</ce:para></ce:list-item><ce:list-item id="u0025"><ce:label>•</ce:label><ce:para id="p0025" view="all">It also presents a novel approach that combines DNN and i-vector systems by using bottleneck features.</ce:para></ce:list-item><ce:list-item id="u0030"><ce:label>•</ce:label><ce:para id="p0030" view="all">The combination of i-vector and bottleneck systems outperforms our baseline system by 45% in EER and Cavg, on 3s and 10s.</ce:para></ce:list-item></ce:list></ce:simple-para></ce:abstract-sec></ce:abstract><ce:abstract class="author" id="ab0015" xml:lang="en" view="all"><ce:section-title id="st0015">Abstract</ce:section-title><ce:abstract-sec id="abs0015" view="all"><ce:simple-para id="sp0015" view="all">In this work, we present a comprehensive study on the use of deep neural networks (DNNs) for automatic language identification (LID). Motivated by the recent success of using DNNs in acoustic modeling for speech recognition, we adapt DNNs to the problem of identifying the language in a given utterance from its short-term acoustic features. We propose two different DNN-based approaches. In the first one, the DNN acts as an end-to-end LID classifier, receiving as input the speech features and providing as output the estimated probabilities of the target languages. In the second approach, the DNN is used to extract bottleneck features that are then used as inputs for a state-of-the-art i-vector system. Experiments are conducted in two different scenarios: the complete NIST Language Recognition Evaluation dataset 2009 (LRE'09) and a subset of the Voice of America (VOA) data from LRE'09, in which all languages have the same amount of training data. Results for both datasets demonstrate that the DNN-based systems significantly outperform a state-of-art i-vector system when dealing with short-duration utterances. Furthermore, the combination of the DNN-based and the classical i-vector system leads to additional performance improvements (up to 45% of relative improvement in both EER and <mml:math altimg="si23.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math> on 3s and 10s conditions, respectively).</ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords class="keyword" id="kwd0010" xml:lang="en" view="all"><ce:section-title id="st0020">Keywords</ce:section-title><ce:keyword id="kw0010"><ce:text id="te0015">LID</ce:text></ce:keyword><ce:keyword id="kw0015"><ce:text id="te0020">DNN</ce:text></ce:keyword><ce:keyword id="kw0020"><ce:text id="te0025">Bottleneck</ce:text></ce:keyword><ce:keyword id="kw0025"><ce:text id="te0030">i-vectors</ce:text></ce:keyword></ce:keywords></head><body view="all"><ce:sections><ce:section id="s0010" role="introduction" view="all"><ce:label>1</ce:label><ce:section-title id="st0025">Introduction</ce:section-title><ce:para id="p0035" view="all">Automatic language identification (LID) refers to the process of automatically determining the language of a given speech sample (<ce:cross-ref id="crf0045" refid="bib0175">Muthusamy et al., 1994</ce:cross-ref>). The need for reliable LID is continuously growing due to a number of factors, including the technological trend toward increased human interaction using hands-free, voice-operated devices and the need to facilitate the coexistence of multiple different languages in an increasingly globalized world (<ce:cross-ref id="crf0050" refid="bib0070">Gonzalez-Dominguez et al., 2014</ce:cross-ref>).</ce:para><ce:para id="p0040" view="all">Driven by recent developments in speaker verification, current state-of-the-art technology in acoustic LID systems involves using i-vector front-end features followed by diverse classification mechanisms that compensate for speaker and session variabilities (<ce:cross-refs id="crfs0010" refid="bib0015 bib0125 bib0200">Brummer et al., 2012; Li et al., 2013; Sturim et al., 2011</ce:cross-refs>). An i-vector is a compact representation (typically from 400 to 600 dimensions) of a whole utterance, derived as a point estimate of the latent variable in a factor analysis model (<ce:cross-refs id="crfs0015" refid="bib0050 bib0115">Dehak et al., 2011; Kenny et al., 2008</ce:cross-refs>). While proven to be successful in a variety of scenarios, i-vector based approaches have two major drawbacks. First, i-vectors are point estimates and their robustness quickly degrades as the duration of the utterance decreases. Note that the shorter the utterance, the larger the variance of the posterior probability distribution of the latent variable; and thus, the larger the i-vector <ce:italic>uncertainty</ce:italic>. Second, in real-time applications, most of the costs associated with i-vector computation occur after completion of the utterance, which introduces an undesirable latency.</ce:para><ce:para id="p0045" view="all">Motivated by the prominence of deep neural networks (DNNs), which surpass the performance of the previous dominant paradigm, Gaussian mixture models (GMMs), in diverse and challenging machine learning applications – including acoustic modeling (<ce:cross-refs id="crfs0020" refid="bib0100 bib0160">Hinton et al., 2012; Mohamed et al., 2012</ce:cross-refs>), visual object recognition (<ce:cross-ref id="crf0055" refid="bib0030">Ciresan et al.</ce:cross-ref>), and many others (<ce:cross-ref id="crf0060" refid="bib0220">Yu and Deng, 2011</ce:cross-ref>) – we previously introduced a successful LID system based on DNNs in <ce:cross-ref id="crf0065" refid="bib0130">Lopez-Moreno et al.</ce:cross-ref>. Unlike previous works on using neural networks for LID (<ce:cross-refs id="crfs0025" refid="bib0035 bib0120 bib0170">Cole et al., 1989; Leena et al., 2005; Montavon, 2009</ce:cross-refs>), this paper represented, to the best of our knowledge, the first time a DNN scheme was applied at large scale for LID and was benchmarked against alternative state-of-the-art approaches. Evaluated using two different datasets – the NIST LRE'09 (3s task) and Google 5M LID – this scheme demonstrated significantly improved performance compared to several i-vector-based state-of-the-art systems (<ce:cross-ref id="crf0070" refid="bib0130">Lopez-Moreno et al.</ce:cross-ref>). This scheme has also been successfully applied as a front-end stage for real-time multilingual speech recognition, as described in (<ce:cross-ref id="crf0075" refid="bib0070">Gonzalez-Dominguez et al., 2014</ce:cross-ref>).</ce:para><ce:para id="p0050" view="all">This article builds on our previous work by extensively evaluating and comparing the use of DNNs for LID with an i-vector baseline system in different scenarios. We explore the influence of several factors on the DNN architecture configuration, such as the number of layers, the importance of including the temporal context and the duration of test segments. Further, we present a hybrid approach between the DNN and the i-vector system – the <ce:italic>bottleneck</ce:italic> system – in an attempt to take the best from both approaches. In this hybrid system, a DNN with a bottleneck hidden layer (40 dimensions) acts as a new step in the feature extraction before the i-vector modeling strategy is implemented. Bottleneck features have recently been used in the context of LID (<ce:cross-refs id="crfs0030" refid="bib0105 bib0150 bib0195">Jiang et al., 2014; Matĕjka et al., 2014; Richardson et al.</ce:cross-refs>). In these previous works, the DNN models were optimized to classify the phonetic units of a specific language, following the standard approach of an acoustic model for automatic speech recognition. Unlike in these previous works, here we propose using the bottleneck features from a DNN directly optimized for language recognition. In this new approach, <ce:italic>i</ce:italic>) the DNN optimization criterion is coherent with the LID evaluation criterion, and <ce:italic>ii</ce:italic>) the DNN training process does not require using transcribed audio, which is typically much harder to acquire than language labels. Note that the transcription process involves handwork from experts that are familiarized with specific guidelines (e.g. transcriptions provided in the written domain, or the spoken domain); it is slow, as each utterance typically contains about 2 words/sec and moreover, word level transcriptions needs to be mapped into frame level alignments before a DNN such as the one used in previous works can be trained. That requires bootstrapping from another pre-existing ASR system, typically a GMM-based acoustic model iteratively trained from scratch. Instead, in the process of training lang-id networks, no previous alignments are needed, only one label per utterance is required and annotation guidelines are significantly simpler. Overall, that facilitates the adoption of a bottleneck lang-id system, which has the additional advantage that targets language discrimination in all its intermediate stages.</ce:para><ce:para id="p0055" view="all">For this study, we conducted experiments using two different datasets: i) a subset of LRE'09 (8 languages) that comprises equal quantities of data for each target language, and ii) the full LRE'09 evaluation dataset (23 languages), which contains significantly different amounts of available data for each target language. This approach enabled us to assess the performance of all the proposed systems in cases of both controlled and uncontrolled conditions.</ce:para><ce:para id="p0060" view="all">The rest of this paper is organized as follows: <ce:cross-ref id="crf0080" refid="s0015">Sections 2</ce:cross-ref> and <ce:cross-ref id="crf0085" refid="s0035">3</ce:cross-ref> present the i-vector baseline system and the architecture of the DNN-based system. In <ce:cross-ref id="crf0090" refid="s0050">Section 4</ce:cross-ref>, we describe the proposed bottleneck scheme. In <ce:cross-ref id="crf0095" refid="s0055">Sections 5</ce:cross-ref> and <ce:cross-ref id="crf0100" refid="s0060">6</ce:cross-ref>, we outline fusion and calibration, and the datasets used during experimentation. Results are then presented in <ce:cross-ref id="crf0105" refid="s0075">Section 7</ce:cross-ref>. Finally, <ce:cross-ref id="crf0110" refid="s0110">Section 8</ce:cross-ref> summarizes final conclusions and potential future lines of this work.</ce:para></ce:section><ce:section id="s0015" view="all"><ce:label>2</ce:label><ce:section-title id="st0030">The baseline I-vector based system</ce:section-title><ce:section id="s0020" view="all"><ce:label>2.1</ce:label><ce:section-title id="st0035">Feature extraction</ce:section-title><ce:para id="p0065" view="all">The input audio to our system is segmented into windows of 25ms with 10ms overlap. 7 Mel-frequency cepstral coefficients (MFCCs), including <ce:italic>C</ce:italic><ce:inf loc="post">0</ce:inf>, are computed on each frame (<ce:cross-ref id="crf0115" refid="bib0040">Davis and Mermelstein, 1980</ce:cross-ref>). Vocal tract length normalization (VTLN) (<ce:cross-ref id="crf0120" refid="bib0210">Welling et al., 1999</ce:cross-ref>), cepstral mean and variance normalization, and RASTA filtering (<ce:cross-ref id="crf0125" refid="bib0090">Hermansky and Morgan, 1994</ce:cross-ref>) are applied on the MFCCs. Finally, shifted delta cepstra (SDC) features are computed in a 7-1-3-7 configuration (<ce:cross-ref id="crf0130" refid="bib0205">Torres-Carrasquillo et al., 2002</ce:cross-ref>), and a 56-dimensional vector is obtained every 10 ms by stacking the MFCCs and the SDC of the current frame. The feature sequence of each utterance is converted into a single i-vector with the i-vector system described next.</ce:para></ce:section><ce:section id="s0025" view="all"><ce:label>2.2</ce:label><ce:section-title id="st0040">I-vector extraction</ce:section-title><ce:para id="p0070" view="all">I–vectors (<ce:cross-ref id="crf0135" refid="bib0055">Dehak et al., 2011</ce:cross-ref>) have become a standard approach for speaker identification, and have grown in popularity also for language recognition (<ce:cross-refs id="crfs0035" refid="bib0015 bib0050 bib0145 bib0155">Brummer et al., 2012; Dehak et al., 2011; Martinez et al., 2011; McCree, 2014</ce:cross-refs>). Apart from language and speaker identification, i–vectors have been shown to be useful also for several different classification problems including emotion recognition (<ce:cross-ref id="crf0140" refid="bib0215">Xia and Liu, 2012</ce:cross-ref>), and intelligibility assessment (<ce:cross-ref id="crf0145" refid="bib0140">Martínez et al., 2013</ce:cross-ref>). An i–vector is a compact representation of a Gaussian Mixture Model (GMM) supervector (<ce:cross-ref id="crf0150" refid="bib0185">Reynolds et al., 2000</ce:cross-ref>), which captures most of the GMM supervectors variability. It is obtained by a Maximum–A–Posteriori (MAP) estimate of the mean of a posterior distribution (<ce:cross-ref id="crf0155" refid="bib0110">Kenny, 2007</ce:cross-ref>). In the i–vector framework, we model the utterance-specific supervector <ce:bold>m</ce:bold> as:<ce:display><ce:formula id="e0010"><ce:label>(1)</ce:label><mml:math altimg="si1.gif" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">m</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold">T</mml:mi><mml:mi mathvariant="bold">w</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></ce:formula></ce:display>where <ce:bold>u</ce:bold> is the UBM GMM mean supervector and <ce:bold>T</ce:bold> is a low-rank rectangular matrix representing the bases spanning the sub-space, which contains most of the variability in the supervector space. The i–vector is then a MAP estimate of the low-dimensional latent variable <ce:bold>w</ce:bold>. In our experiments, we have used a GMM containing 2048 Gaussian components with diagonal covariance matrices and the dimensionality of i-vectors was set to 600.</ce:para></ce:section><ce:section id="s0030" view="all"><ce:label>2.3</ce:label><ce:section-title id="st0045">Classification backends</ce:section-title><ce:para id="p0075" view="all">For classification, the i-vectors of each language are used to estimate a single Gaussian distribution via maximum likelihood, where the covariance matrix is shared among languages and is equal to the within-class covariance matrix of the training data. During evaluation, every new utterance is evaluated against the models of all the languages. Further details can be found in (<ce:cross-ref id="crf0160" refid="bib0145">Martinez et al., 2011</ce:cross-ref>).</ce:para></ce:section></ce:section><ce:section id="s0035" view="all"><ce:label>3</ce:label><ce:section-title id="st0050">The DNN-based LID system</ce:section-title><ce:para id="p0080" view="all">Recent findings in the field of speech recognition have shown that significant accuracy improvements over classical GMM schemes can be achieved through the use of DNNs. DNNs can be used to generate new feature representations or as final classifiers that directly estimate class posterior scores. Among the most important advantages of DNNs is their multilevel distributed representation of the model's input data (<ce:cross-ref id="crf0165" refid="bib0100">Hinton et al., 2012</ce:cross-ref>). This fact makes the DNN an exponentially more compact model than GMMs. Further, DNNs do not impose assumptions on the input data distribution (<ce:cross-ref id="crf0170" refid="bib0165">Mohamed et al., 2012</ce:cross-ref>) and have proven successful in exploiting large amounts of data, achieving more robust models without lapsing into overtraining. All of these factors motivate the use of DNNs in language identification. The rest of this section describes the architecture and practical application of our DNN system.</ce:para><ce:section id="s0040" view="all"><ce:label>3.1</ce:label><ce:section-title id="st0055">Architecture</ce:section-title><ce:para id="p0085" view="all">The DNN system used in this work is a fully-connected feed-forward neural network with rectified linear units (ReLU) (<ce:cross-ref id="crf0175" refid="bib0225">Zeiler et al., 2013</ce:cross-ref>). Thus, an input at level <ce:italic>j</ce:italic>, <ce:italic>x<ce:inf loc="post">j</ce:inf></ce:italic>, is mapped to its corresponding activation <ce:italic>y<ce:inf loc="post">j</ce:inf></ce:italic> (input of the layer above) as:<ce:display><ce:formula id="e0015"><ce:label>(2)</ce:label><mml:math altimg="si2.gif" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">ReLU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></ce:formula></ce:display><ce:display><ce:formula id="e0020"><ce:label>(3)</ce:label><mml:math altimg="si3.gif" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>i</ce:italic> is an index over the units of the layer below and <ce:italic>b<ce:inf loc="post">j</ce:inf></ce:italic> is the bias of the unit <ce:italic>j</ce:italic>.</ce:para><ce:para id="p0090" view="all">The output layer is then configured as a <ce:italic>softmax</ce:italic>, where hidden units map input <ce:italic>y<ce:inf loc="post">j</ce:inf></ce:italic> to a class probability <ce:italic>p<ce:inf loc="post">j</ce:inf></ce:italic> in the form:<ce:display><ce:formula id="e0025"><ce:label>(4)</ce:label><mml:math altimg="si4.gif" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>l</ce:italic> is an index over all of the target classes (languages, <ce:cross-ref id="crf0190" refid="f0015">Fig. 2</ce:cross-ref>).</ce:para><ce:para id="p0095" view="all">As a cost function for backpropagating gradients in the training stage, we use the cross-entropy function defined as:<ce:display><ce:formula id="e0030"><ce:label>(5)</ce:label><mml:math altimg="si5.gif" display="block" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>log</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>t<ce:inf loc="post">j</ce:inf></ce:italic> represents the target probability of the class <ce:italic>j</ce:italic> for the current evaluated example, taking a value of either 1 (true class) or 0 (false class).</ce:para></ce:section><ce:section id="s0045" view="all"><ce:label>3.2</ce:label><ce:section-title id="st0060">Implementing DNNs for language identification</ce:section-title><ce:para id="p0100" view="all">From the conceptual architecture explained above, we built a language identification system to work at the frame level as follows:</ce:para><ce:para id="p0105" view="all">As the input of the net, we used the same features as the i-vector baseline system (56 MFCC-SDC). Specifically, the input layer was fed with 21 frames formed by stacking the current processed frame and its ±10 left/right neighbors. Thus, the input layer comprised a total number of 1176 (21 × 56) visible units, <ce:italic>v</ce:italic>.</ce:para><ce:para id="p0110" view="all">On top of the input layer, we stacked a total number of <mml:math altimg="si12.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math> (4) hidden layers, each containing <ce:italic>h</ce:italic> (2560) units. Then, we added the softmax layer, whose dimension (<ce:italic>s</ce:italic>) corresponds to the number of target languages (<ce:italic>N<ce:inf loc="post">L</ce:inf></ce:italic>), plus one extra output for the out-of-set (<ce:italic>OOS</ce:italic>) languages. This OOS class, devoted to unknown test languages, could later allow us to use the system in open-set identification scenarios.</ce:para><ce:para id="p0115" view="all">Overall, the net was defined by a total of <ce:italic>w</ce:italic> free parameters (weights + bias), <mml:math altimg="si13.gif" display="inline" overflow="scroll"><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math> (<ce:italic>~</ce:italic>23M). The complete topology of the network is depicted in <ce:cross-ref id="crf0180" refid="f0010">Fig. 1</ce:cross-ref><ce:float-anchor refid="f0010"/><ce:float-anchor refid="f0015"/>.</ce:para><ce:para id="p0120" view="all">In terms of the training procedure, we used asynchronous stochastic gradient descent within the DistBelief framework (<ce:cross-ref id="crf0185" refid="bib0045">Dean et al., 2012</ce:cross-ref>), which uses computing clusters with thousands of machines to train large models. The learning rate and minibatch size were fixed to 0.001 and 200 samples.<ce:cross-ref id="crf0195" refid="fn0010"><ce:sup loc="post">1</ce:sup></ce:cross-ref><ce:footnote id="fn0010"><ce:label>1</ce:label><ce:note-para id="np0010" view="all">We define sample as the input of the DNN: the feature representation of a single frame besides those from its adjacent frames forming the context.</ce:note-para></ce:footnote></ce:para><ce:para id="p0125" view="all">Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages. This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame. Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages. A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer. The score <ce:italic>s<ce:inf loc="post">l</ce:inf></ce:italic> for language <ce:italic>l</ce:italic> of a given test utterance is computed by multiplying the output probabilities <ce:italic>p<ce:inf loc="post">l</ce:inf></ce:italic> obtained for all of its frames; or equivalently, accumulating the logs as:<ce:display><ce:formula id="e0035"><ce:label>(6)</ce:label><mml:math altimg="si6.gif" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi>log</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mtext>​</mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></ce:formula></ce:display>where <mml:math altimg="si14.gif" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mtext>​</mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math> represents the class probability output for the language <ce:italic>l</ce:italic> corresponding to the input example at time <ce:italic>t</ce:italic>, <ce:italic>x<ce:inf loc="post">t</ce:inf></ce:italic> by using the DNN defined by parameters <ce:italic>θ</ce:italic>.</ce:para></ce:section></ce:section><ce:section id="s0050" view="all"><ce:label>4</ce:label><ce:section-title id="st0065">Bottleneck features: A hybrid approach</ce:section-title><ce:para id="p0130" view="all">Another interesting way to leverage the discriminative power of a DNN is through the use of <ce:italic>bottleneck features</ce:italic> (<ce:cross-refs id="crfs0040" refid="bib0065 bib0085">Fontaine et al., 1997; Grézl et al., 2009</ce:cross-refs>). Typically, in speech recognition, bottleneck features are extracted from a DNN trained to predict phonetic targets, by either using the estimated output probabilities (<ce:cross-ref id="crf0205" refid="bib0095">Hermansky et al., 2000</ce:cross-ref>) or the activations of a narrow hidden layer (<ce:cross-ref id="crf0210" refid="bib0080">Grézl et al., 2007</ce:cross-ref>), the so-called bottleneck layer. The bottleneck features represent a low-dimensional non-linear transformation of the input features, ready to use for further classification.</ce:para><ce:para id="p0135" view="all">Utilizing this approach, we extracted bottleneck features from the DNN directly trained for LID, as explained in <ce:cross-ref id="crf0215" refid="s0035">Section 3</ce:cross-ref>, and replaced the last complete hidden layer with a bottleneck layer of 40 dimensions. Then, we modeled those new bottleneck features by using an i-vector strategy. That is, we replaced the standard MFCC-SDC features with bottleneck features as the input of our i-vector baseline system.</ce:para><ce:para id="p0140" view="all">The underlying motivation of this hybrid architecture is to take the best from both the DNN and the i-vector system approaches. On one hand, we make use of the discriminative power of the DNN model and its capability to learn better feature representations; on the other, we are still able to leverage the generative modeling introduced by the i-vector system.</ce:para></ce:section><ce:section id="s0055" view="all"><ce:label>5</ce:label><ce:section-title id="st0070">Fusion and calibration</ce:section-title><ce:para id="p0145" view="all">We used multiclass logistic regression in order to combine and calibrate the outputs of individual LID systems (<ce:cross-ref id="crf0220" refid="bib0025">Brümmer and van Leeuwen, 2006</ce:cross-ref>). Let <mml:math altimg="si15.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math> be the log-likelihood score for the recognizer <ce:italic>k</ce:italic> and language <ce:italic>L</ce:italic> for utterance <ce:italic>x<ce:inf loc="post">i</ce:inf></ce:italic>. We derive combined scores as<ce:display><ce:formula id="e0040"><ce:label>(7)</ce:label><mml:math altimg="si7.gif" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>L</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></ce:formula></ce:display></ce:para><ce:para id="p0150" view="all">Note that this is just a generic version of the product rule combination, parameterized by <ce:italic>α</ce:italic> and <ce:italic>β</ce:italic>. Defining a multiclass logistic regression model for the class posterior as<ce:display><ce:formula id="e0045"><ce:label>(8)</ce:label><mml:math altimg="si8.gif" display="block" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>L</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>L</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math></ce:formula></ce:display>we found <ce:italic>α</ce:italic> and <ce:italic>β</ce:italic> to maximize the global log-posterior in a held-out dataset of <ce:italic>I</ce:italic> utterances<ce:display><ce:formula id="e0050"><ce:label>(9)</ce:label><mml:math altimg="si9.gif" display="block" overflow="scroll"><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mtext>​</mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo>…</mml:mo><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>α</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mtext>​</mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mtext>​</mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo>…</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>I</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></ce:formula></ce:display>being<ce:display><ce:formula id="e0055"><ce:label>(10)</ce:label><mml:math altimg="si10.gif" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mtext>​</mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mtext> </mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>w<ce:inf loc="post">l</ce:inf></ce:italic> (<mml:math altimg="si16.gif" display="inline" overflow="scroll"><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo>…</mml:mo><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>N</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math>) is a weight vector that normalizes the number of samples for every language in the development set (typically, <ce:italic>w<ce:inf loc="post">L</ce:inf></ce:italic> = 1 if an equal number of samples per language is used). This fusion and calibration procedure was conducted using the FoCal (Multi-class) toolkit (<ce:cross-ref id="crf0225" refid="bib0020">Brümmer</ce:cross-ref>).</ce:para></ce:section><ce:section id="s0060" view="all"><ce:label>6</ce:label><ce:section-title id="st0075">Databases and evaluation metrics</ce:section-title><ce:section id="s0065" view="all"><ce:label>6.1</ce:label><ce:section-title id="st0080">Databases</ce:section-title><ce:para id="p0155" view="all">We evaluate all proposed systems in the framework of the NIST LRE 2009 (LRE'09) evaluation. The LRE'09 includes data from two different audio sources: Conversational Telephone Speech (CTS) and, unlike previous LRE evaluations, telephone speech from broadcast news, which was used for both training and test purposes. Broadcast data were obtained via an automatic acquisition system from “Voice of America” news (VOA) that mixed telephone and non-telephone speech. Up to 2TB of 8kHz raw data containing radio broadcast speech, with corresponding language and audio source labels, were distributed to participants, and a total of 40 languages (23 target and 17 out of set) were included. While the VOA corpus contains over 2000 hours of labeled audio, only the labels from a fraction of about 200 hours were manually verified by the Linguistic Data Consortium (LDC).</ce:para><ce:para id="p0160" view="all">Due to the large disparity in the amounts of available training material by language and type of audio source, we created two different evaluation sets from LRE'09: LRE09_FULL and LRE09_BDS. LRE09_FULL corresponds to the original LRE'09 evaluation, which includes the original test files and all development training files for each language.<ce:cross-ref id="crf0230" refid="fn0015"><ce:sup loc="post">2</ce:sup></ce:cross-ref><ce:footnote id="fn0015"><ce:label>2</ce:label><ce:note-para id="np0015" view="all">We used the training dataset defined by the I3A research group (University of Zaragoza) in its participation in the LRE'11 evaluation (<ce:cross-ref id="crf0235" refid="bib0135">Martínez et al., 2011</ce:cross-ref>).</ce:note-para></ce:footnote> LRE09_BDS, on the other hand, is a balanced subset of 8 languages from automatically labeled VOA audio data. While the LRE09_FULL set uses data from the manually annotated part of the VOA corpus, the LRE09_BDS contains audio from both automatically and manually annotated parts. This dual evaluation approach served two purposes: i) LRE09_FULL, which is a standard benchmark, allowed us to generate results that could be compared with those of other research groups, and ii) LRE09_BDS allowed us to conduct new experiments using a controlled and balanced dataset with more hours of data for each target language. This approach may also help identify a potentially detrimental effect on the LRE09_FULL DNN-based systems due to the lack of data in some target languages. This is important because we previously found that the relative performance of a DNN versus an i-vector system is largely dependent on the amount of available data (<ce:cross-ref id="crf0240" refid="bib0130">Lopez-Moreno et al.</ce:cross-ref>).</ce:para><ce:para id="p0165" view="all"><ce:cross-ref id="crf0200" refid="t0010">Table 1</ce:cross-ref><ce:float-anchor refid="t0010"/> summarizes the specific training and evaluation data per language used in each dataset.</ce:para></ce:section><ce:section id="s0070" view="all"><ce:label>6.2</ce:label><ce:section-title id="st0085">Evaluation metrics</ce:section-title><ce:para id="p0170" view="all">Two different metrics were used to assess the performance of the proposed techniques. As the main error measure to evaluate the capabilities of one-vs.-all language detection, we used <mml:math altimg="si23.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math> (average detection cost), as defined in the LRE 2009 (<ce:cross-refs id="crfs0045" refid="bib0010 bib0180">Brummer, 2010; NIST, 2009</ce:cross-refs>) evaluation plan. <mml:math altimg="si23.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math> is a measure of the cost of making incorrect decisions and, therefore, considers not only the discrimination capabilities of the system, but also the ability of setting optimal thresholds (i. e., calibration). Further, the well-known metric Equal Error Rate (EER) is a calibration-insensitive metric that indicates the error rate at the operating point where the number of false alarms and the number of false rejections are equal. Since our problem is a detection task where a binary classification is performed for each language, the final EER is the average of the EERs obtained for each language.</ce:para></ce:section></ce:section><ce:section id="s0075" role="results" view="all"><ce:label>7</ce:label><ce:section-title id="st0090">Results</ce:section-title><ce:para id="p0175" view="all">In this section, we present a comprehensive set of experiments that compare and assess the two systems of interest, as well as a combined version of the two. Besides the i-vector-based baseline system, we evaluate the following three family of systems:<ce:list id="ulist0015"><ce:list-item id="u0035"><ce:label>•</ce:label><ce:para id="p0180" view="all"><ce:bold>DNN</ce:bold> refers to the end-to-end deep neural network based system presented in <ce:cross-ref id="crf0245" refid="s0035">Section 3</ce:cross-ref>, which is used as a final classifier to predict language posteriors.</ce:para></ce:list-item><ce:list-item id="u0040"><ce:label>•</ce:label><ce:para id="p0185" view="all"><ce:bold>DNN_BN</ce:bold>: refers to an end-to-end DNN system where the last hidden layer is replaced by a bottleneck layer. This DNN is used as a final classifier to predict language posteriors.</ce:para></ce:list-item><ce:list-item id="u0045"><ce:label>•</ce:label><ce:para id="p0190" view="all"><ce:bold>BN</ce:bold> refers to the i-vector system where the inputs are bottleneck features, as explained in <ce:cross-ref id="crf0250" refid="s0050">Section 4</ce:cross-ref>.</ce:para></ce:list-item></ce:list></ce:para><ce:para id="p0195" view="all">Individual systems vary in the number of layers used (4 or 8 layers) and the size of their input context (0, 5 or 10 left/right frames). Hereafter, we will use the family name to refer a specific system {DNN, DNN_BN, BN}, followed by a set of suffixes {4L, 8L} and the {0-0C, 5-5C, 10-10C} to denote the number of layers and input context, respectively. For instance, the system name DNN_BN_4L_5-5C refers to a DNN system with 4 layers where the last hidden layer is a bottleneck layer, which uses an input of 11 concatenated frames (5 to the left and 5 to the right of the central frame). Note that the difference between DNN_BN and BN is that in the first, the DNN with a bottleneck layer is used directly as an end-to-end classifier, while in the second the DNN is used to extract bottleneck features which are used as input to an i-vector system.</ce:para><ce:section id="s0080" role="results" view="all"><ce:label>7.1</ce:label><ce:section-title id="st0095">Results using LRE09_BDS</ce:section-title><ce:section id="s0085" view="all"><ce:label>7.1.1</ce:label><ce:section-title id="st0100">DNN vs i-vector system</ce:section-title><ce:para id="p0200" view="all">As the starting point of this study, we compare the performance of the proposed DNN architecture (with 4 layers and input context of ±10 frames) and the i-vector baseline system. <ce:cross-ref id="crf0255" refid="f0020">Fig. 3</ce:cross-ref><ce:float-anchor refid="f0020"/> shows the difference in performance using test segments with a duration of 3s, 10s and 30s. The trend of the lines in the figure illustrates one of the main conclusions of this work: the DNN system significantly outperforms the i-vector system for short duration utterances (3s), while the i-vector system is more robust for test utterance over 10s.</ce:para><ce:para id="p0205" view="all">Unlike i-vectors, the DNN system does not process the complete test utterance at once. Instead, posterior scores are computed at each individual frame and combined as if each frame was independent (Eq. <ce:cross-ref id="crf0260" refid="e0035">6</ce:cross-ref>). This is a frame-by-frame strategy that allows for providing continuous labels for a data stream, which may be beneficial in real time applications (<ce:cross-ref id="crf0265" refid="bib0070">Gonzalez-Dominguez et al., 2014</ce:cross-ref>).</ce:para></ce:section><ce:section id="s0090" view="all"><ce:label>7.1.2</ce:label><ce:section-title id="st0105">Bottlenecks</ce:section-title><ce:para id="p0210" view="all">Next, we explore the use of bottleneck features in the bottleneck system. As previously stated, it is a hybrid DNN/i-vector system where the DNN model acts as a feature extractor, whose features are used by the i-vector model instead of the standard MFCC-SDC. Specifically, we present the results of a bottleneck system that uses a DNN model with 4 layers, where the last hidden layer was replaced by a 40 dimensional bottleneck layer. <ce:cross-ref id="crf0270" refid="t0015">Table 2</ce:cross-ref><ce:float-anchor refid="t0015"/> compares the results from the hybrid bottleneck system with those of the standalone alternative approaches presented previously. Results show significantly improved performance for 10s and 30s utterances when using the bottleneck system (BN), as compared with the DNN system without bottleneck (DNN) (28% and 54% relative improvement in EER, respectively), while results for 3s utterances are similar. With respect to the i-vector, results obtained with the BN system are better in 3s and 10s (20% and 34% relative improvement in EER, respectively), whereas in 30s i-vectors still obtain better performance. Again, these results demonstrate the robustness of the i-vector system when evaluating longer test segments. They also suggest that further research into this area could lead to improved results when combining the strengths of DNN and i-vector systems.</ce:para><ce:para id="p0215" view="all">We also analyze the loss in performance of our standalone neural network system when reducing the number of nodes in the last hidden layer from 2560 (DNN system) to 40 nodes used by the DNN_BN system. That is, the DNN_BN system uses the same network that extracts the BN features, but is used as an end-to-end classifier. Results collected in <ce:cross-ref id="crf0275" refid="t0015">Table 2</ce:cross-ref> show that there is not a significant difference in performance when reducing the number of nodes in the last hidden layer. This result demonstrates that bottleneck features are an accurate representation of the frame-level information; at least, comparable to that presented in the complete last hidden layer of the conventional DNN architecture.</ce:para></ce:section><ce:section id="s0095" view="all"><ce:label>7.1.3</ce:label><ce:section-title id="st0110">Temporal context and number of layers</ce:section-title><ce:para id="p0220" view="all">Another important aspect in the DNN system configuration is the temporal context of the spectral features used as the input to the DNN. Until now, we have used a fixed right/left context of ±10 frames respectively. That is, the input of our network, as mentioned in <ce:cross-ref id="crf0280" refid="s0035">Section 3</ce:cross-ref>, is formed by stacking the features of every frame with its corresponding 10 adjacent frames to the left and right. The motivation behind using temporal contexts with a large number of frames lies in the idea of incorporating additional high-level information into our system (i.e. phonetic, phonotactic and prosodic information). This idea has been widely and successfully implemented in language identification in the past, using long-term phonotactic/prosodic tokenizations (<ce:cross-refs id="crfs0050" refid="bib0060 bib0190">Ferrer et al., 2010; Reynolds et al., 2003</ce:cross-refs>) or, in acoustic approaches, by using shifted-delta-cepstral features (<ce:cross-ref id="crf0285" refid="bib0205">Torres-Carrasquillo et al., 2002</ce:cross-ref>).</ce:para><ce:para id="p0225" view="all"><ce:cross-ref id="crf0290" refid="t0020">Table 3</ce:cross-ref><ce:float-anchor refid="t0020"/> presents the performance for contextual windows of size 0, ± 5 and ± 10 frames. Unlike the results we presented in <ce:cross-ref id="crf0295" refid="bib0075">Gonzalez-Dominguez et al. (2015</ce:cross-ref>), where we found that the window size was critical to model the contextual information, here just small and non-uniform gains were found. This result can be explained by the fact that, unlike the PLP features used in <ce:cross-ref id="crf0300" refid="bib0075">Gonzalez-Dominguez et al. (2015</ce:cross-ref>), the MFCC-SDC features in this paper already include some degree of temporal information.</ce:para><ce:para id="p0230" view="all">In addition, we evaluate the effect of increasing the number of layers to eight, doubling the number of weights in the network from 22.7M to 48.9M. The results of this evaluation are summarized in <ce:cross-ref id="crf9015" refid="t0020">Table 3</ce:cross-ref>. The 8 layers topology achieved only small gains for DNN and DNN_BN in 3s segments, so we opted to keep the original 4-layer DNN as our reference DNN system.</ce:para></ce:section><ce:section id="s0100" view="all"><ce:label>7.1.4</ce:label><ce:section-title id="st0115">Fusion and results per language</ce:section-title><ce:para id="p0235" view="all">The different optimization strategies of DNN and i-vector systems (discriminative vs. generative) and the different results observed in the evaluated tasks have demonstrated the complementarity of these two approaches. Suchcomplementarity suggests that further gains may be achieved through a score level combination of the systems presented above, the results of which are presented in the last three rows of <ce:cross-ref id="crf0305" refid="t0015">Table 2</ce:cross-ref>. We performed a score-level combination of the baseline i-vector system and various neural network-based systems by means of multiclass logistic regression (<ce:cross-ref id="crf0310" refid="s0055">Section 5</ce:cross-ref>). The fusion of the i-vector and bottleneck systems achieves the best performance, with relative improvements over the standalone i-vector system of 42%/40%, 44%/44% and 14%/28% in terms of EER/<mml:math altimg="si23.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math> for the 3s, 10s and 30s evaluated conditions, respectively. Moreover, results are consistent across all the languages and test duration conditions as shown in <ce:cross-ref id="crf0315" refid="f0025">Fig. 4</ce:cross-ref><ce:float-anchor refid="f0025"/>. These results confirm that when bottleneck and i-vector systems are combined, they consistently outperform the baseline i-vector system, although the relative improvement diminishes as the test duration increases (<ce:cross-ref id="crf0320" refid="f0030">Fig. 5</ce:cross-ref><ce:float-anchor refid="f0030"/>).</ce:para></ce:section></ce:section><ce:section id="s0105" role="results" view="all"><ce:label>7.2</ce:label><ce:section-title id="st0120">Results using LRE09_FULL</ce:section-title><ce:para id="p0240" view="all">To properly train a DNN system, we ideally need large and balanced amounts of data for each language. In this section, we evaluate the implications of having an unbalanced training dataset. Specifically, we mirror the experiments in the above section, instead using the entire LRE09_FULL dataset (see <ce:cross-ref id="crf9010" refid="t0010">Table 1</ce:cross-ref> for the distribution of this dataset). One of the possible approaches for dealing with an unbalanced dataset is to build a bottleneck system in the following way: First, generate a balanced subset of utterances from the most represented languages to train a network that includes a bottleneck layer. This network may or may not contain all of the target languages. Then, using the previous network, compute the bottleneck features from the original unbalanced dataset to optimize the remaining stages involved in the i-vector system. We simulated the unbalanced data scenario by using the eight-language DNN from <ce:cross-ref id="crf0325" refid="s0080">Section 7.1</ce:cross-ref> to compute bottleneck features over our LRE09_FULL training set. While one could consider using non-overlapping sets for the DNN and i-vector optimization to avoid overfitting, we opted to use the entire unbalanced dataset due to data scarcity in our training material for LRE09_FULL.</ce:para><ce:para id="p0245" view="all"><ce:cross-ref id="crf0330" refid="f0035">Fig. 6</ce:cross-ref><ce:float-anchor refid="f0035"/> depicts the performance of the bottleneck system trained as explained above, the i-vector system, and their fusion, for each of the 3 conditions (3s, 10s, 30s) and the 23 target languages. The vertical line separates the performance for the languages included (left) and excluded (right) during the DNN optimization process. The results show that, despite overall good performance, the bottleneck system performs much better for the languages involved in the DNN training.</ce:para><ce:para id="p0250" view="all">The second approach used was to train a new DNN model using all the target languages in the LRE09_FULL evaluation. Note that, in this case, unequal amounts of training data were used to optimize each of the 23 DNN outputs. The results of this second approach are shown in <ce:cross-ref id="crf0335" refid="f0040">Fig. 7</ce:cross-ref><ce:float-anchor refid="f0040"/>. By comparing <ce:cross-refs id="crfs0055" refid="f0035 f0040">Figs. 7 and 6</ce:cross-refs>, in which the only underlying difference is the training data used for the DNN model, we see that data imbalance may be an issue in the stand-alone DNN system, but it is not an issue when using the bottleneck system. Moreover, the bottleneck system seems to benefit from matching the target classes of the underlying DNN model with the target languages in the language recognition evaluation (see, for instance, the performance improvements on Georgian, Korean or Ukranian).</ce:para><ce:para id="p0255" view="all">Finally, the results for the LRE09_FULL dataset for all the individual systems proposed, including fusions with the baseline i-vector system, are summarized in <ce:cross-ref id="crf0340" refid="t0025">Table 4</ce:cross-ref><ce:float-anchor refid="t0025"/>. Note that all DNNs shown in this table were trained using data of the 23 target languages. The conclusion remains the same as in the case of the LRE09_BDS dataset (<ce:cross-ref id="crf0345" refid="t0015">Table 2</ce:cross-ref>), but with modest performance improvements. Specifically, when fusing the i-vector and the bottleneck systems, we achieved improvements of 29%/27%, 32%/34% and 21%/24% in terms of EER/<mml:math altimg="si23.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math> for the 3s, 10s and 30s evaluated conditions.</ce:para></ce:section></ce:section><ce:section id="s0110" view="all"><ce:label>8</ce:label><ce:section-title id="st0125">Summary</ce:section-title><ce:para id="p0260" view="all">In this work, we presented an extensive study of the use of deep neural networks for LID. Guided by the success of DNNs for acoustic modeling, we explored their capability to learn discriminative language information from speech signals.</ce:para><ce:para id="p0265" view="all">First, we showed how a DNN directly trained to discern languages obtains significantly improved results with respect to our best i-vector system when dealing with short-duration utterances. This proposed DNN architecture is able to generate a local decision about the language spoken in every single frame. These local decisions can then be combined into a final decision at any point during the utterance, which makes this approach particularly suitable for real-time applications.</ce:para><ce:para id="p0270" view="all">Next, we introduced the LID optimized bottleneck system as a hybrid approach between the proposed DNN and i-vector systems. Here, a DNN optimized to classify languages is seen as a front-end that extracts a more suitable representation (in terms of discrimination) of feature vectors. On contrary to previous bottleneck approaches for LID, where the DNN was trained to recognize the phonetic units of a given language, in this work, the DNN optimization criterion is coherent with the LID objective. Moreover, the DNN model requires only language labels which are much easier to obtain than the speech transcriptions.</ce:para><ce:para id="p0275" view="all">We observed that the most desirable scenario is to train the DNN with a balanced dataset that includes all the target languages. In the case of not being able to fulfill this requirement, it is preferable to include data from all target languages during the DNN optimization stage, even if some languages contain more training hours than others. In addition, fusion results show that DNN-based systems provide complementary information to the baseline i-vector system. In particular, the combination of the i-vector and bottleneck systems result in a relative improvement of up to 42%/40%, 44%/44% and 14%/28% and 29%/27%, 32%/34% and 21%/24% for the balanced dataset LRE09_BDS and the whole LRE'09 evaluation respectively, in terms of EER/<ce:italic>C</ce:italic>_<ce:italic>avg</ce:italic> and for the 3s, 10s, and 30s test conditions.</ce:para><ce:para id="p0280" view="all">We believe that the performance of the DNN could be improved further. In the future, we plan to experiment with other topologies/activation functions and other input features, such as filterbank energies. Further, for the sake of comparison, in this work we chose i-vector modeling as the strategy to model the bottleneck features. It is a future line of this work to experiment with different modeling schemes that might fit better for those bottleneck features .</ce:para></ce:section></ce:sections><ce:acknowledgment id="ac0010" view="all"><ce:section-title id="st0130">Acknowledgment</ce:section-title><ce:para id="p0285" view="all">The authors would like to thank Daniel Garcia Romero for helpful suggestions and valuable discussions. Javier Gonzalez-Dominguez worked in this project during his stay is Google supported by the Google Visitor Faculty program. This work was partly supported by the <ce:grant-sponsor id="gsp0010" xlink:type="simple" xlink:role="http://www.elsevier.com/xml/linking-roles/grant-sponsor">Spanish government</ce:grant-sponsor> through project <ce:grant-number id="gnum0010" refid="gsp0010">TIN2011-28169-C05-02</ce:grant-number> and by the <ce:grant-sponsor id="gsp0015" xlink:type="simple" xlink:role="http://www.elsevier.com/xml/linking-roles/grant-sponsor">Czech Ministry of Interior</ce:grant-sponsor> project No. <ce:grant-number id="gnum0015" refid="gsp0015">VG20132015129</ce:grant-number> “ZAOM” and by the <ce:grant-sponsor id="gsp0020" xlink:type="simple" xlink:role="http://www.elsevier.com/xml/linking-roles/grant-sponsor">Czech Ministry of Education, Youth and Sports</ce:grant-sponsor> from the National Programme of Sustainability (NPU II) project “IT4Innovations excellence in science – <ce:grant-number id="gnum0020" refid="gsp0020">LQ1602</ce:grant-number>”.</ce:para></ce:acknowledgment></body><tail view="all"><ce:bibliography id="bb0010" view="all"><ce:section-title id="st0135">References</ce:section-title><ce:bibliography-sec id="bs0010" view="all"><ce:bib-reference id="bib0010"><ce:label>Brummer, 2010</ce:label><sb:reference id="sr0010"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Brummer</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Measuring, Refining and Calibrating Speaker and Language Information Extracted from Speech</sb:maintitle></sb:title></sb:contribution><sb:comment>Ph.D. thesis; Department of Electrical and Electronic Engineering, University of Stellenbosch</sb:comment><sb:host><sb:book><sb:date>2010</sb:date></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0015"><ce:label>Brummer et al, 2012</ce:label><sb:reference id="sr0015"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Brummer</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Cumani</ce:surname></sb:author><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Glembek</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Karafiát</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Matejka</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Pesan</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Description and analysis of the Brno276 System for LRE2011</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of Odyssey 2012: The Speaker and Language Recognition Workshop</sb:maintitle></sb:title><sb:date>2012</sb:date><sb:publisher><sb:name>International Speech Communication Association</sb:name></sb:publisher></sb:edited-book><sb:pages><sb:first-page>216</sb:first-page><sb:last-page>223</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0020"><ce:label>Brümmer,</ce:label><sb:reference id="sr0020"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Brümmer</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Fusion and calibration toolkit [software package]</sb:maintitle></sb:title></sb:contribution><sb:host><sb:e-host><ce:inter-ref id="iw0010" xlink:href="http://sites.google.com/site/nikobrummer/focal" xlink:type="simple">http://sites.google.com/site/nikobrummer/focal</ce:inter-ref><sb:date>2010</sb:date></sb:e-host></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0025"><ce:label>Brümmer, van Leeuwen, 2006</ce:label><sb:reference id="sr0025"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Brümmer</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>van Leeuwen</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>On calibration of language recognition scores</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proc. of Odyssey</sb:maintitle></sb:title><sb:date>2006</sb:date><sb:publisher><sb:name>San Juan, Puerto Rico</sb:name></sb:publisher></sb:edited-book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0030"><ce:label>Ciresan et al,</ce:label><ce:other-ref id="or0010"><ce:textref id="tr0010">Ciresan, D., Meier, U., Gambardella, L., Schmidhuber, J., 2010. Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition, CoRR abs/1003.0358.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0035"><ce:label>Cole et al, 1989</ce:label><sb:reference id="sr0030"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Cole</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Inouye</ce:surname></sb:author><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Muthusamy</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Gopalakrishnan</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Language identification with neural networks: a feasibility study</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Communications, Computers and Signal Processing, 1989. Conference Proceeding, IEEE Pacific Rim Conference on</sb:maintitle></sb:title><sb:date>1989</sb:date></sb:edited-book><sb:pages><sb:first-page>525</sb:first-page><sb:last-page>529</sb:last-page></sb:pages><ce:doi>10.1109/PACRIM.1989.48417</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0040"><ce:label>Davis, Mermelstein, 1980</ce:label><sb:reference id="sr0035"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Davis</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Mermelstein</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Acoust. Speech Signal Process</sb:maintitle></sb:title><sb:volume-nr>28</sb:volume-nr></sb:series><sb:date>1980</sb:date></sb:issue><sb:pages><sb:first-page>357</sb:first-page><sb:last-page>366</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0045"><ce:label>Dean et al, 2012</ce:label><sb:reference id="sr0040"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Dean</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Corrado</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Monga</ce:surname></sb:author><sb:author><ce:surname>Chen</ce:surname><ce:given-name>K.</ce:given-name></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Devin</ce:surname></sb:author><sb:author><ce:given-name>Q.</ce:given-name><ce:surname>Le</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Large scale distributed deep networks</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:editors><sb:editor><ce:given-name>P.</ce:given-name><ce:surname>Bartlett</ce:surname></sb:editor><sb:editor><ce:given-name>F.</ce:given-name><ce:surname>Pereira</ce:surname></sb:editor><sb:editor><ce:given-name>C.</ce:given-name><ce:surname>Burges</ce:surname></sb:editor><sb:editor><ce:given-name>L.</ce:given-name><ce:surname>Bottou</ce:surname></sb:editor><sb:editor><ce:given-name>K.</ce:given-name><ce:surname>Weinberger</ce:surname></sb:editor></sb:editors><sb:title><sb:maintitle>Advances in Neural Information Processing Systems 25</sb:maintitle></sb:title><sb:date>2012</sb:date></sb:edited-book><sb:pages><sb:first-page>1232</sb:first-page><sb:last-page>1240</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0050"><ce:label>Dehak et al, 2011</ce:label><sb:reference id="sr0045"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Dehak</ce:surname></sb:author><sb:author><ce:given-name>P.A.</ce:given-name><ce:surname>Torres-Carrasquillo</ce:surname></sb:author><sb:author><ce:given-name>D.A.</ce:given-name><ce:surname>Reynolds</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Dehak</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Language recognition via i-vectors and dimensionality reduction</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>INTERSPEECH, ISCA</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book><sb:pages><sb:first-page>857</sb:first-page><sb:last-page>860</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0055"><ce:label>Dehak et al, 2011</ce:label><sb:reference id="sr0050"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Dehak</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Kenny</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Dehak</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Dumouchel</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Ouellet</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Front-end factor analysis for speaker verification</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Audio Speech Lang. Process</sb:maintitle></sb:title><sb:volume-nr>19</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>788</sb:first-page><sb:last-page>798</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0060"><ce:label>Ferrer et al, 2010</ce:label><sb:reference id="sr0055"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Ferrer</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Scheffer</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Shriberg</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A comparison of approaches for modeling prosodic features in speaker recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>International Conference on Acoustics, Speech, and Signal Processing</sb:maintitle></sb:title><sb:date>2010</sb:date></sb:edited-book><sb:pages><sb:first-page>4414</sb:first-page><sb:last-page>4417</sb:last-page></sb:pages><ce:doi>10.1109/ICASSP.2010.5495632</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0065"><ce:label>Fontaine et al, 1997</ce:label><sb:reference id="sr0060"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>V.</ce:given-name><ce:surname>Fontaine</ce:surname></sb:author><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Ris</ce:surname></sb:author><sb:author><ce:given-name>J.-M.</ce:given-name><ce:surname>Boite</ce:surname></sb:author><sb:author><ce:given-name>M.S.</ce:given-name><ce:surname>Initialis</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Nonlinear discriminant analysis for improved speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Fifth European Conference on Speech Communication and Technology, EUROSPEECH 1997</sb:maintitle></sb:title><sb:date>1997</sb:date></sb:edited-book></sb:host><sb:comment>Rhodes, Greece</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0070"><ce:label>Gonzalez-Dominguez et al, 2014</ce:label><sb:reference id="sr0065"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Gonzalez-Dominguez</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Eustis</ce:surname></sb:author><sb:author><ce:given-name>I.</ce:given-name><ce:surname>Lopez Moreno</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Senior</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Beaufays</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Moreno</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A real-time end-to-end multilingual speech recognition architecture</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE J. Sel. Top. Signal Process</sb:maintitle></sb:title></sb:series><sb:issue-nr>99</sb:issue-nr><sb:date>2014</sb:date></sb:issue><sb:pages><sb:first-page>1</sb:first-page></sb:pages><ce:doi>10.1109/JSTSP.2014.2364559</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0075"><ce:label>Gonzalez-Dominguez et al, 2015</ce:label><sb:reference id="sr0070"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Gonzalez-Dominguez</ce:surname></sb:author><sb:author><ce:given-name>I.</ce:given-name><ce:surname>Lopez-Moreno</ce:surname></sb:author><sb:author><ce:given-name>P.J.</ce:given-name><ce:surname>Moreno</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Gonzalez-Rodriguez</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Frame-by-frame language identification in short utterances using deep neural networks</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Neural Netw</sb:maintitle></sb:title><sb:volume-nr>64</sb:volume-nr></sb:series><sb:date>2015</sb:date></sb:issue><sb:pages><sb:first-page>49</sb:first-page><sb:last-page>58</sb:last-page></sb:pages><ce:doi>10.1016/j.neunet.2014.08.006</ce:doi></sb:host><sb:comment>Special Issue on Deep Learning of Representations.</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0080"><ce:label>Grézl et al, 2007</ce:label><sb:reference id="sr0075"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Grézl</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Karafiát</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Kontár</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Černocký</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Probabilistic and bottle-neck features for LVCSR of meetings</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2007)</sb:maintitle></sb:title><sb:date>2007</sb:date><sb:publisher><sb:name>IEEE Signal Processing Society</sb:name></sb:publisher></sb:edited-book><sb:pages><sb:first-page>757</sb:first-page><sb:last-page>760</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0085"><ce:label>Grézl et al, 2009</ce:label><sb:reference id="sr0080"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Grézl</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Karafiát</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Burget</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Investigation into bottle-neck features for meeting speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>INTERSPEECH 2009, International Speech Communication Association</sb:maintitle></sb:title><sb:date>2009</sb:date></sb:edited-book><sb:pages><sb:first-page>2947</sb:first-page><sb:last-page>2950</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0090"><ce:label>Hermansky, Morgan, 1994</ce:label><sb:reference id="sr0085"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Hermansky</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Morgan</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>RASTA processing of speech</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Speech Audio Process</sb:maintitle></sb:title><sb:volume-nr>2</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>1994</sb:date></sb:issue><sb:pages><sb:first-page>578</sb:first-page><sb:last-page>589</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0095"><ce:label>Hermansky et al, 2000</ce:label><sb:reference id="sr0090"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Hermansky</ce:surname></sb:author><sb:author><ce:given-name>D.P.W.</ce:given-name><ce:surname>Ellis</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Sharma</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Tandem connectionist feature extraction for conventional HMM systems</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>PROC</sb:maintitle></sb:title><sb:date>2000</sb:date><sb:publisher><sb:name>ICASSP</sb:name></sb:publisher></sb:edited-book><sb:pages><sb:first-page>1635</sb:first-page><sb:last-page>1638</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0100"><ce:label>Hinton et al, 2012</ce:label><sb:reference id="sr0095"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Hinton</ce:surname></sb:author><sb:author><ce:surname>Deng</ce:surname><ce:given-name>L.</ce:given-name></sb:author><sb:author><ce:surname>Yu</ce:surname><ce:given-name>D.</ce:given-name></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Dahl</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Mohamed</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Jaitly</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Signal Process. Mag</sb:maintitle></sb:title><sb:volume-nr>29</sb:volume-nr></sb:series><sb:issue-nr>6</sb:issue-nr><sb:date>2012</sb:date></sb:issue><sb:pages><sb:first-page>82</sb:first-page><sb:last-page>97</sb:last-page></sb:pages><ce:doi>10.1109/MSP.2012.2205597</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0105"><ce:label>Jiang et al, 2014</ce:label><sb:reference id="sr0100"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Jiang</ce:surname><ce:given-name>B.</ce:given-name></sb:author><sb:author><ce:surname>Song</ce:surname><ce:given-name>Y.</ce:given-name></sb:author><sb:author><ce:surname>Wei</ce:surname><ce:given-name>S.</ce:given-name></sb:author><sb:author><ce:surname>Liu</ce:surname><ce:given-name>J.H.</ce:given-name></sb:author><sb:author><ce:given-name>I.V.</ce:given-name><ce:surname>McLoughlin</ce:surname></sb:author><sb:author><ce:surname>Dai</ce:surname><ce:given-name>L.-R.</ce:given-name></sb:author></sb:authors><sb:title><sb:maintitle>Deep bottleneck features for spoken language identification</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>PLoS ONE</sb:maintitle></sb:title><sb:volume-nr>9</sb:volume-nr></sb:series><sb:issue-nr>7</sb:issue-nr><sb:date>2014</sb:date></sb:issue><sb:pages><sb:first-page>e100795</sb:first-page></sb:pages><ce:doi>10.1371/journal.pone.0100795</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0110"><ce:label>Kenny, 2007</ce:label><sb:reference id="sr0105"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Kenny</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Joint factor analysis of speaker and session variability: theory and algorithms</sb:maintitle></sb:title></sb:contribution><sb:host><sb:e-host><ce:inter-ref id="iw0015" xlink:href="http://www.crim.ca/perso/patrick.kenny/FAtheory.pdf" xlink:type="simple">http://www.crim.ca/perso/patrick.kenny/FAtheory.pdf</ce:inter-ref><sb:date>2007</sb:date></sb:e-host></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0115"><ce:label>Kenny et al, 2008</ce:label><sb:reference id="sr0110"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Kenny</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Oullet</ce:surname></sb:author><sb:author><ce:given-name>V.</ce:given-name><ce:surname>Dehak</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Gupta</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Dumouchel</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A study of interspeaker variability in speaker verification</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Audio Speech Lang. Process</sb:maintitle></sb:title><sb:volume-nr>16</sb:volume-nr></sb:series><sb:issue-nr>5</sb:issue-nr><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>980</sb:first-page><sb:last-page>988</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0120"><ce:label>Leena et al, 2005</ce:label><sb:reference id="sr0115"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Leena</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Srinivasa Rao</ce:surname></sb:author><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Yegnanarayana</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Neural network classifiers for language identification using phonotactic and prosodic features</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Intelligent Sensing and Information Processing, 2005. Proceedings of 2005 International Conference on</sb:maintitle></sb:title><sb:date>2005</sb:date></sb:edited-book><sb:pages><sb:first-page>404</sb:first-page><sb:last-page>408</sb:last-page></sb:pages><ce:doi>10.1109/ICISIP.2005.1529486</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0125"><ce:label>Li et al, 2013</ce:label><sb:reference id="sr0120"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Li</ce:surname><ce:given-name>H.</ce:given-name></sb:author><sb:author><ce:surname>Ma</ce:surname><ce:given-name>B.</ce:given-name></sb:author><sb:author><ce:given-name>K.A.</ce:given-name><ce:surname>Lee</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Spoken language recognition: from fundamentals to practice</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>P. IEEE</sb:maintitle></sb:title><sb:volume-nr>101</sb:volume-nr></sb:series><sb:issue-nr>5</sb:issue-nr><sb:date>2013</sb:date></sb:issue><sb:pages><sb:first-page>1136</sb:first-page><sb:last-page>1159</sb:last-page></sb:pages><ce:doi>10.1109/JPROC.2012.2237151</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0130"><ce:label>Lopez-Moreno et al,</ce:label><ce:other-ref id="or0015"><ce:textref id="tr0015">Lopez-Moreno, I., Gonzalez-Dominguez, J., Plchot, O., Martinez, D., Gonzalez-Rodriguez, J., Moreno, P., 2014. Automatic language identification using deep neural networks. Acoustics, Speech, and Signal Processing, IEEE International Conference on.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0135"><ce:label>Martínez et al, 2011</ce:label><sb:reference id="sr0125"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Martínez</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Villalba</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Ortega</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Lleida</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>I3A language recognition system description for NIST LRE 2011</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>NIST 2011 LRE Workshop Booklet</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book></sb:host><sb:comment>Atlanta, Georgia, USA</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0140"><ce:label>Martínez et al, 2013</ce:label><sb:reference id="sr0130"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Martínez</ce:surname></sb:author><sb:author><ce:given-name>P.D.</ce:given-name><ce:surname>Green</ce:surname></sb:author><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Christensen</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Dysarthria intelligibility assessment in a factor analysis total variability space</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association</sb:maintitle></sb:title><sb:date>2013</sb:date></sb:edited-book><sb:pages><sb:first-page>2133</sb:first-page><sb:last-page>2137</sb:last-page></sb:pages></sb:host><sb:comment>Lyon, France; August 25–29</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0145"><ce:label>Martinez et al, 2011</ce:label><sb:reference id="sr0135"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Martinez</ce:surname></sb:author><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Plchot</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Burget</ce:surname></sb:author><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Glembek</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Matejka</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Language recognition in iVectors space</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>INTERSPEECH</sb:maintitle></sb:title><sb:date>2011</sb:date><sb:publisher><sb:name>ISCA</sb:name></sb:publisher></sb:edited-book><sb:pages><sb:first-page>861</sb:first-page><sb:last-page>864</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0150"><ce:label>Matĕjka et al, 2014</ce:label><sb:reference id="sr0140"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Matĕjka</ce:surname></sb:author><sb:author><ce:surname>Zhang</ce:surname><ce:given-name>L.</ce:given-name></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Ng</ce:surname></sb:author><sb:author><ce:given-name>H.S.</ce:given-name><ce:surname>Mallidi</ce:surname></sb:author><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Glembek</ce:surname></sb:author><sb:author><ce:surname>Ma</ce:surname><ce:given-name>J.</ce:given-name></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Neural network bottleneck features for language identification</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Speaker Odyssey</sb:maintitle></sb:title><sb:date>2014</sb:date></sb:edited-book><sb:pages><sb:first-page>299</sb:first-page><sb:last-page>304</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0155"><ce:label>McCree, 2014</ce:label><sb:reference id="sr0145"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>McCree</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Multiclass discriminative training of i-vector language recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>IEEE Odyssey: The Speaker and Language Recognition Workshop</sb:maintitle></sb:title><sb:date>2014</sb:date><sb:publisher><sb:name>Joensu, Finland</sb:name></sb:publisher></sb:edited-book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0160"><ce:label>Mohamed et al, 2012</ce:label><sb:reference id="sr0150"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Mohamed</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Dahl</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Hinton</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Acoustic modeling using deep belief networks</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Audio Speech Lang. Process</sb:maintitle></sb:title><sb:volume-nr>20</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2012</sb:date></sb:issue><sb:pages><sb:first-page>14</sb:first-page><sb:last-page>22</sb:last-page></sb:pages><ce:doi>10.1109/TASL.2011.2109382</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0165"><ce:label>Mohamed et al, 2012</ce:label><sb:reference id="sr0155"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.-R.</ce:given-name><ce:surname>Mohamed</ce:surname></sb:author><sb:author><ce:given-name>G.E.</ce:given-name><ce:surname>Hinton</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Penn</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Understanding how deep belief networks perform acoustic modelling</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>ICASSP</sb:maintitle></sb:title><sb:date>2012</sb:date><sb:publisher><sb:name>IEEE</sb:name></sb:publisher></sb:edited-book><sb:pages><sb:first-page>4273</sb:first-page><sb:last-page>4276</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0170"><ce:label>Montavon, 2009</ce:label><sb:reference id="sr0160"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Montavon</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Deep learning for spoken language identification</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>NIPS Workshop on Deep Learning for Speech Recognition and Related Applications</sb:maintitle></sb:title><sb:date>2009</sb:date></sb:edited-book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0175"><ce:label>Muthusamy et al, 1994</ce:label><sb:reference id="sr0165"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Muthusamy</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Barnard</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Cole</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Reviewing automatic language identification</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Signal Process. Mag</sb:maintitle></sb:title><sb:volume-nr>11</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>1994</sb:date></sb:issue><sb:pages><sb:first-page>33</sb:first-page><sb:last-page>41</sb:last-page></sb:pages><ce:doi>10.1109/79.317925</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0180"><ce:label>NIST, 2009</ce:label><sb:reference id="sr0170"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>NIST</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The 2009 NIST SLR Evaluation Plan</sb:maintitle></sb:title></sb:contribution><sb:host><sb:e-host><ce:inter-ref id="iw0020" xlink:href="http://www.itl.nist.gov/iad/mig/tests/lre/2009/LRE09_EvalPlan_v6.pdf" xlink:type="simple">www.itl.nist.gov/iad/mig/tests/lre/2009/LRE09_EvalPlan_v6.pdf</ce:inter-ref><sb:date>2009</sb:date></sb:e-host></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0185"><ce:label>Reynolds et al, 2000</ce:label><sb:reference id="sr0175"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Reynolds</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Quatieri</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Dunn</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Speaker verification using adapted Gaussian mixture models</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Dig. Sig. Process</sb:maintitle></sb:title><sb:volume-nr>10</sb:volume-nr></sb:series><sb:issue-nr>1/2/3</sb:issue-nr><sb:date>2000</sb:date></sb:issue><sb:pages><sb:first-page>19</sb:first-page><sb:last-page>41</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0190"><ce:label>Reynolds et al, 2003</ce:label><sb:reference id="sr0180"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Reynolds</ce:surname></sb:author><sb:author><ce:given-name>W.</ce:given-name><ce:surname>Andrews</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Campbell</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Navratil</ce:surname></sb:author><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Peskin</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Adami</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>The SuperSID project: exploiting high-level information for high-accuracy speaker recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:book-series><sb:series><sb:title><sb:maintitle>IEEE International Conference on Acoustics, Speech, and Signal Processing</sb:maintitle></sb:title><sb:volume-nr>vol. 4</sb:volume-nr></sb:series></sb:book-series><sb:date>2003</sb:date></sb:edited-book><sb:pages><sb:first-page>784</sb:first-page><sb:last-page>787</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0195"><ce:label>Richardson et al,</ce:label><sb:reference id="sr0185"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Richardson</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Reynolds</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Dehak</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A Unified Deep Neural Network for Speaker and Language Recognition arXiv:1504.00923</sb:maintitle></sb:title></sb:contribution><sb:host><sb:e-host><ce:inter-ref id="iw0025" xlink:href="http://arxiv.org/abs/1504.00923" xlink:type="simple">http://arxiv.org/abs/1504.00923</ce:inter-ref><sb:date>2015</sb:date></sb:e-host></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0200"><ce:label>Sturim et al, 2011</ce:label><sb:reference id="sr0190"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Sturim</ce:surname></sb:author><sb:author><ce:given-name>W.</ce:given-name><ce:surname>Campbell</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Dehak</ce:surname></sb:author><sb:author><ce:given-name>Z.</ce:given-name><ce:surname>Karam</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>McCree</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Reynolds</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>The MIT LL 2010 speaker recognition evaluation system: scalable language-independent speaker recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book><sb:pages><sb:first-page>5272</sb:first-page><sb:last-page>5275</sb:last-page></sb:pages><ce:doi>10.1109/ICASSP.2011.5947547</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0205"><ce:label>Torres-Carrasquillo et al, 2002</ce:label><sb:reference id="sr0195"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.A.</ce:given-name><ce:surname>Torres-Carrasquillo</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Singer</ce:surname></sb:author><sb:author><ce:given-name>M.A.</ce:given-name><ce:surname>Kohler</ce:surname></sb:author><sb:author><ce:given-name>J.R.</ce:given-name><ce:surname>Deller</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Approaches to language identification using Gaussian mixture models and shifted delta cepstral features</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:book-series><sb:series><sb:title><sb:maintitle>ICSLP</sb:maintitle></sb:title><sb:volume-nr>vol. 1</sb:volume-nr></sb:series></sb:book-series><sb:date>2002</sb:date></sb:edited-book><sb:pages><sb:first-page>89</sb:first-page><sb:last-page>92</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0210"><ce:label>Welling et al, 1999</ce:label><sb:reference id="sr0200"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Welling</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Kanthak</ce:surname></sb:author><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Ney</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Improved methods for vocal tract normalization</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP ‘99</sb:maintitle></sb:title><sb:date>1999</sb:date></sb:edited-book><sb:pages><sb:first-page>761</sb:first-page><sb:last-page>764</sb:last-page></sb:pages></sb:host><sb:comment>Phoenix, Arizona, USA; March 15–19</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0215"><ce:label>Xia, Liu, 2012</ce:label><sb:reference id="sr0205"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Xia</ce:surname><ce:given-name>R.</ce:given-name></sb:author><sb:author><ce:surname>Liu</ce:surname><ce:given-name>Y.</ce:given-name></sb:author></sb:authors><sb:title><sb:maintitle>Using i-vector space model for emotion recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication Association</sb:maintitle></sb:title><sb:date>2012</sb:date></sb:edited-book><sb:pages><sb:first-page>2230</sb:first-page><sb:last-page>2233</sb:last-page></sb:pages></sb:host><sb:comment>Portland, Oregon, USA; September 9–13</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0220"><ce:label>Yu, Deng, 2011</ce:label><sb:reference id="sr0210"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Yu</ce:surname><ce:given-name>D.</ce:given-name></sb:author><sb:author><ce:surname>Deng</ce:surname><ce:given-name>L.</ce:given-name></sb:author></sb:authors><sb:title><sb:maintitle>Deep learning and its applications to signal and information processing [exploratory DSP]</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Signal Process. Mag</sb:maintitle></sb:title><sb:volume-nr>28</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>145</sb:first-page><sb:last-page>154</sb:last-page></sb:pages><ce:doi>10.1109/MSP.2010.939038</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0225"><ce:label>Zeiler et al, 2013</ce:label><sb:reference id="sr0215"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Zeiler</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Ranzato</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Monga</ce:surname></sb:author><sb:author><ce:surname>Mao</ce:surname><ce:given-name>M.</ce:given-name></sb:author><sb:author><ce:surname>Yang</ce:surname><ce:given-name>K.</ce:given-name></sb:author><sb:author><ce:given-name>Q.</ce:given-name><ce:surname>Le</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>On rectified linear units for speech processing</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>38th International Conference on Acoustics, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>2013</sb:date></sb:edited-book></sb:host><sb:comment>Vancouver</sb:comment></sb:reference></ce:bib-reference></ce:bibliography-sec></ce:bibliography></tail></article></xocs:serial-item></xocs:doc></originalText></full-text-retrieval-response>