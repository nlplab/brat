The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure. This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model. However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute. This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show. Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested. This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation.
