<full-text-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/article/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:bk="http://www.elsevier.com/xml/bk/dtd"><coredata><prism:url>http://api.elsevier.com/content/article/pii/S2212671612000698</prism:url><dc:identifier>doi:10.1016/j.aasri.2012.06.068</dc:identifier><eid>1-s2.0-S2212671612000698</eid><prism:doi>10.1016/j.aasri.2012.06.068</prism:doi><pii>S2212-6716(12)00069-8</pii><dc:title>Obstacle Detection and Classification in Dynamical Background </dc:title><prism:publicationName>AASRI Procedia</prism:publicationName><prism:aggregationType>Journal</prism:aggregationType><prism:issn>22126716</prism:issn><prism:volume>1</prism:volume><prism:startingPage>435</prism:startingPage><prism:endingPage>440</prism:endingPage><prism:pageRange>435-440</prism:pageRange><dc:format>text/xml</dc:format><prism:coverDate>2012-12-31</prism:coverDate><prism:coverDisplayDate>2012</prism:coverDisplayDate><prism:copyright>Copyright © 2012 Published by Elsevier B.V.</prism:copyright><prism:publisher>Published by Elsevier B.V.</prism:publisher><prism:issueName>AASRI Conference on Computational Intelligence and Bioinformatics</prism:issueName><dc:creator>Liu, Lizhuang</dc:creator><dc:creator>Cui, Jianzhu</dc:creator><dc:creator>Li, Jing</dc:creator><dc:description>AbstractIn Obstacle detection is based on inverse perspective mapping and homography. Obstacle classification is based on fuzzy neural network. The estimation of the vanishing point relies on feature extraction strategy. The method exploits the geometrical relations between the elements in the scene so that obstacle can be detected. The estimated homography of the road plane between successive images is used for image alignment. A new fuzzy decision fusion method with fuzzy attribution for obstacle detection and classification application is described The fuzzy decision function modifies parameters with auto-adapted algorithm to get better classification probability It is shown that the method can achieve better classification result</dc:description><openaccess>1</openaccess><openaccessArticle>true</openaccessArticle><openaccessType>Full</openaccessType><openArchiveArticle>false</openArchiveArticle><openaccessSponsorName/><openaccessSponsorType/><openaccessUserLicense>http://creativecommons.org/licenses/by-nc-nd/3.0/</openaccessUserLicense><dcterms:subject>object detection</dcterms:subject><dcterms:subject>image alignment</dcterms:subject><dcterms:subject>fuzzy neural network</dcterms:subject><link rel="self" href="http://api.elsevier.com/content/article/pii/S2212671612000698"/><link rel="scidir" href="http://www.sciencedirect.com/science/article/pii/S2212671612000698"/></coredata><scopus-id>84862104263</scopus-id><scopus-eid>2-s2.0-84862104263</scopus-eid><link rel="abstract" href="http://api.elsevier.com/content/abstract/scopus_id/84862104263"/><originalText><xocs:doc xmlns:xoe="http://www.elsevier.com/xml/xoe/dtd" xsi:schemaLocation="http://www.elsevier.com/xml/xocs/dtd http://be-prod3a/schema/dtds/document/fulltext/xcr/xocs-article.xsd"><xocs:meta><xocs:content-family>serial</xocs:content-family><xocs:content-type>JL</xocs:content-type><xocs:cid>282179</xocs:cid><xocs:ssids><xocs:ssid type="alllist">291210</xocs:ssid><xocs:ssid type="subj">291791</xocs:ssid><xocs:ssid type="subj">291877</xocs:ssid><xocs:ssid type="subj">291882</xocs:ssid><xocs:ssid type="subj">291883</xocs:ssid><xocs:ssid type="content">31</xocs:ssid><xocs:ssid type="oa">90</xocs:ssid></xocs:ssids><xocs:srctitle>AASRI Procedia</xocs:srctitle><xocs:normalized-srctitle>AASRIPROCEDIA</xocs:normalized-srctitle><xocs:orig-load-date yyyymmdd="20120821">2012-08-21</xocs:orig-load-date><xocs:available-online-date yyyymmdd="20120823">2012-08-23</xocs:available-online-date><xocs:ew-transaction-id>2014-11-18T16:43:20</xocs:ew-transaction-id><xocs:eid>1-s2.0-S2212671612000698</xocs:eid><xocs:pii-formatted>S2212-6716(12)00069-8</xocs:pii-formatted><xocs:pii-unformatted>S2212671612000698</xocs:pii-unformatted><xocs:doi>10.1016/j.aasri.2012.06.068</xocs:doi><xocs:item-stage>S300</xocs:item-stage><xocs:item-version-number>S300.3</xocs:item-version-number><xocs:item-weight>HEAD-AND-TAIL</xocs:item-weight><xocs:hub-eid>1-s2.0-S2212671612X00027</xocs:hub-eid><xocs:timestamp yyyymmdd="20150515">2015-05-15T07:34:50.698394-04:00</xocs:timestamp><xocs:dco>0</xocs:dco><xocs:tomb>0</xocs:tomb><xocs:date-search-begin>20120101</xocs:date-search-begin><xocs:date-search-end>20121231</xocs:date-search-end><xocs:year-nav>2012</xocs:year-nav><xocs:indexeddate epoch="1345507200">2012-08-21T00:00:00Z</xocs:indexeddate><xocs:articleinfo>rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist webpdf webpdfpagecount yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref</xocs:articleinfo><xocs:issns><xocs:issn-primary-formatted>2212-6716</xocs:issn-primary-formatted><xocs:issn-primary-unformatted>22126716</xocs:issn-primary-unformatted></xocs:issns><xocs:crossmark is-crossmark="0">false</xocs:crossmark><xocs:vol-first>1</xocs:vol-first><xocs:volume-list><xocs:volume>1</xocs:volume></xocs:volume-list><xocs:suppl>C</xocs:suppl><xocs:vol-iss-suppl-text>Volume 1</xocs:vol-iss-suppl-text><xocs:sort-order>68</xocs:sort-order><xocs:first-fp>435</xocs:first-fp><xocs:last-lp>440</xocs:last-lp><xocs:pages><xocs:first-page>435</xocs:first-page><xocs:last-page>440</xocs:last-page></xocs:pages><xocs:cover-date-orig><xocs:start-date>2012</xocs:start-date></xocs:cover-date-orig><xocs:cover-date-text>2012</xocs:cover-date-text><xocs:cover-date-start>2012-01-01</xocs:cover-date-start><xocs:cover-date-end>2012-12-31</xocs:cover-date-end><xocs:cover-date-year>2012</xocs:cover-date-year><xocs:title-editors-groups><xocs:title-editors-group><ce:title>AASRI Conference on Computational Intelligence and Bioinformatics</ce:title><ce:editors><ce:author-group><ce:author><ce:degrees>Dr.</ce:degrees><ce:given-name>Wei</ce:given-name><ce:surname>Deng</ce:surname></ce:author><ce:affiliation><ce:textfn>American Applied Sciences Research Institute, Suit C, 637 Vineland Ave, La Puente, CA 91746, United States of America</ce:textfn></ce:affiliation></ce:author-group></ce:editors></xocs:title-editors-group></xocs:title-editors-groups><xocs:document-type>article</xocs:document-type><xocs:document-subtype>fla</xocs:document-subtype><xocs:copyright-line>Copyright © 2012 Published by Elsevier B.V.</xocs:copyright-line><xocs:normalized-article-title>OBSTACLEDETECTIONCLASSIFICATIONINDYNAMICALBACKGROUND</xocs:normalized-article-title><xocs:normalized-first-auth-surname>LIU</xocs:normalized-first-auth-surname><xocs:normalized-first-auth-initial>L</xocs:normalized-first-auth-initial><xocs:references><xocs:ref-info refid="bib0005"/><xocs:ref-info refid="bib0010"/><xocs:ref-info refid="bib0015"/><xocs:ref-info refid="bib0020"/><xocs:ref-info refid="bib0025"/><xocs:ref-info refid="bib0030"/><xocs:ref-info refid="bib0035"/><xocs:ref-info refid="bib0040"/><xocs:ref-info refid="bib0045"/><xocs:ref-info refid="bib0050"/><xocs:ref-info refid="bib0055"/><xocs:ref-info refid="bib0060"/><xocs:ref-info refid="bib0065"/></xocs:references><xocs:refkeys><xocs:refkey3>LIUX2012X435</xocs:refkey3><xocs:refkey4lp>LIUX2012X435X440</xocs:refkey4lp><xocs:refkey4ai>LIUX2012X435XL</xocs:refkey4ai><xocs:refkey5>LIUX2012X435X440XL</xocs:refkey5></xocs:refkeys><xocs:open-access><xocs:oa-article-status is-open-access="1" is-open-archive="0">Full</xocs:oa-article-status><xocs:oa-access-effective-date>2013-07-15T12:00:47Z</xocs:oa-access-effective-date><xocs:oa-user-license>http://creativecommons.org/licenses/by-nc-nd/3.0/</xocs:oa-user-license><xocs:oa-access-inherited-from winid="http://vtw.elsevier.com/content/oaw/GEN_UNBOUNDED_ESWaived">OA-Window</xocs:oa-access-inherited-from></xocs:open-access><xocs:attachment-metadata-doc><xocs:attachment-set-type>item</xocs:attachment-set-type><xocs:pii-formatted>S2212-6716(12)00069-8</xocs:pii-formatted><xocs:pii-unformatted>S2212671612000698</xocs:pii-unformatted><xocs:eid>1-s2.0-S2212671612000698</xocs:eid><xocs:doi>10.1016/j.aasri.2012.06.068</xocs:doi><xocs:cid>282179</xocs:cid><xocs:timestamp>2014-11-19T14:21:05.813819-05:00</xocs:timestamp><xocs:cover-date-start>2012-01-01</xocs:cover-date-start><xocs:cover-date-end>2012-12-31</xocs:cover-date-end><xocs:attachments><xocs:web-pdf><xocs:attachment-eid>1-s2.0-S2212671612000698-main.pdf</xocs:attachment-eid><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2212671612000698/MAIN/application/pdf/d5b808fd067ad98e9fbcb0f9448719fd/main.pdf</xocs:ucs-locator><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2212671612000698/MAIN/application/pdf/d5b808fd067ad98e9fbcb0f9448719fd/main.pdf</xocs:ucs-locator><xocs:filename>main.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:pdf-optimized>true</xocs:pdf-optimized><xocs:filesize>244665</xocs:filesize><xocs:web-pdf-purpose>MAIN</xocs:web-pdf-purpose><xocs:web-pdf-page-count>6</xocs:web-pdf-page-count><xocs:web-pdf-images><xocs:web-pdf-image><xocs:attachment-eid>1-s2.0-S2212671612000698-main_1.png</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2212671612000698/PREVIEW/image/png/37cf5ad054c7c070ed1c35a16e665100/main_1.png</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2212671612000698/PREVIEW/image/png/37cf5ad054c7c070ed1c35a16e665100/main_1.png</xocs:ucs-locator><xocs:filename>main_1.png</xocs:filename><xocs:extension>png</xocs:extension><xocs:filesize>44488</xocs:filesize><xocs:pixel-height>849</xocs:pixel-height><xocs:pixel-width>656</xocs:pixel-width><xocs:attachment-type>IMAGE-WEB-PDF</xocs:attachment-type><xocs:pdf-page-num>1</xocs:pdf-page-num></xocs:web-pdf-image></xocs:web-pdf-images></xocs:web-pdf></xocs:attachments></xocs:attachment-metadata-doc></xocs:meta><xocs:rawtext> AASRI Procedia   1  ( 2012 )  435 â€“ 440  2212-6716  2012 Published by Elsevier Ltd.  doi: 10.1016/j.aasri.2012.06.068  2012 AASRI Conference on Computational Intelligence and Bioinformatics  Obstacle Detection and Classification in Dynamical Background  Lizhuang Liu  a,b , Jianzhu Cui b , Jing Li c * a School of Communication and Information Engineering,Shanghai University, no.99 Shangda Road, Shanghai 200444 ,China  b Shanghai Advanced Research Institute, Chinese Academy of Sciences, No.99 Haike Road, Shanghai, 201203,China c College of Information Technology, Shanghai Ocean University, No.999 huhuanchen Road, Shanghai 201303, China  Abstract  In this work, a new obstacle detection and classification technique in dynamical background is proposed. Obstacle  detection is based on inverse perspective mapping and homography. Obstacle classification is based on fuzzy neural  network. The estimation of the vanishing point relies on feature extraction strategy. The method exploits the geometrical  relations between the elements in the scene so that obstacle can be detected. The estimated homography of the road plane  between successive images is used for image alignment. A new fuzzy decision fusion method with fuzzy attribution for  obstacle detection and classification application is describedËŠThe fuzzy decision function modifies parameters with auto- adapted algorithm to get better classification probabilityËŠ It is shown that the method can achieve better classification  resultËŠ 2012 Published by Elsevier B.V. Selection and/or peer review under responsibility of American Applied  Science Research Institute  Keywords: object detection; image alignment; fuzzy neural network  1. Introduction  One of the major challenges of the next generation of road transportation vehicles is to increase the safety  of the passengers. A precrash system is an automobile safety system designed to reduce the severity of an  * Corresponding author. Tel.: +86 021 20350822; fax: +86 021 20350822.  E-mail address: liulz@sari.ac.cnËˆcuijianlou@163.comËˆj_li@shou.edu.cn  . AASRI Procedia www.elsevier.com/locate/procedia Available online at www.sciencedirect.com Open access under CC BY-NC-ND license. Open access under CC BY-NC-ND license. 436   Lizhuang Liu et al. /  AASRI Procedia  1 ( 2012 )  435 â€“ 440  accident. Also known as forward collision warning systems they use radar and sometimes laser sensors to  detect an imminent crash. Depending on the system they may warn the driver, precharge the brakes, inflate  seats for extra support, move the passenger seat, position head rests to avoid whip lash, tension seat belts and  automatically apply partial or full braking to minimize impact. In 2009 the U.S. National Highway Traffic  Safety Administration (NHTSA) began studying whether to make frontal collision warning systems and lane  departure warning systems mandatory[1]. In 2011 a question has been submitted to the European Commission  regarding stimulation of these "collision mitigation by braking" systems [2]. The mandatory fitting of  advanced emergency braking systems in commercial vehicles will be implemented on 1 November 2013 for  new vehicle types and on 1 November 2015 for all new vehicles in the European Union [3].   Some vehicles are already fitted with systems which employ sensors to monitor the proximity of the  vehicle in front and detect situations where the relative speed and distance between the two vehicles suggest  that a collision is imminent. In such a situation, emergency braking can be automatically applied and the  effects of the collision are either mitigated or avoided altogether. The capability of such systems could be  expanded in the future to cover other types of accident (for example, pedestrian accidents or even head-on  collisions). Preliminary studies suggest that such systems could ultimately save around 5000 fatalities and  50,000 serious injuries per year across the EU. It is likely that due to the technical challenges involved, these  systems will only be ready for installation on the whole range of new vehicles in a few years time. However, it  is already possible to provide estimates of the likely costs and benefits of such systems.  A supplement to the LDW system is the lane change assistant (LCA) system. This assists drivers intending  to change lanes. The lane change assistant monitors the adjacent lanes and warns the driver if another vehicle  is likely to come within colliding distance during the lane change. This occurs for example, if the other vehicle  is located in the LCA equipped vehicle's blind spot. Presently the system would warn the driver of such a  problem with e.g. a red flashing side mirror. Later on, a system with feedback in the steering wheel could be  introduced. The lane change assistant needs predictive sensors to scan the surrounding vehicles. The sensors  might possibly be integrated with the sensors used on a AEBS system.    The full version of the system (Pre-Sense Plus) works in four phases. In the first phase, the system provides  warning of impending accident, while the hazard warning lights are activated, the side windows and sunroof  are closed and the front seat belts are tensioned. In the second phase, the warning is followed by light braking,  strong enough to win the driver's attention. The third phase initiates autonomous partial braking at a rate of 3  m/sÂ². The fourth phase decelerates the car at 5 m/sÂ² followed by automatic deceleration at full braking power,  roughly half a second before projected impact. A second system called (Pre-Sense Rear) is designed to reduce  consequences of rear end collisions. Sunroof and windows are closed, seat belts prepared for impact. The  optional memory seats are moved forward to protect the car occupants. The system uses radar and video  sensors[5] and was introduced in 2010 on the 2011 Audi A8.[6] Ford's Collision Warning with Brake Support  was introduced in 2009 on the Lincoln MKS and MKT and the Ford Taurus.[4] This system provides a  warning through a Head Up Display that visually resembles brake lamps. If the driver does not react, the  system pre-charges the brakes and increases the brake assist sensitivity to maximize driver braking  performance. Nissan's luxury brand in North America and Europe, Infiniti, offers a laser-based system in the  US market, which pre-pressurizes the braking system so maximum force can be applied early. Nissan is  reportedly developing a new "magic bumper" system which raises the accelerator pedal if it senses an  impending collision. Once the driver lifts off the pedal, the system then automatically applies the brakes.[12]  Front Assist on the 2011 Volkswagen Touareg can brake to a stop in case of an emergency and tension the  seatbelts as a precautionary measure.[13] Volvo's Collision Warning with Auto Brake (CWAB)[7] developed  in cooperation with Mobileye N.V. was introduced on the 2007 Volvo S80. This system is powered by a  radar/camera fusion and provides a warning through a Head Up Display that visually resembles brake lamps.  437 Lizhuang Liu et al. /  AASRI Procedia  1 ( 2012 )  435 â€“ 440  If the driver does not react, the system pre-charges the brakes and increases the brake assist sensitivity to  maximize driver braking performance.  Lots of sensors mounted on the vehicle could provide a practical solution to this problem. However, the  prices of the traditional systems and their limited performance have prevented such systems from entering the  market. In this context, solutions based on video processing have played an important role for the last decade.  Obstacle detection becomes the main focus in most Forward Collision Warning (FCW) systems.   2. System overview  The complete system overview is shown in Fig 1. The input to the system is a sequence of images captured    by the camera installed inside the vehicle. A video camera has been used to acquire the video sequences. The   input interface records the images delivered by the camera, and performs the necessary operations to  accommodate them to the format expected by the video processing module, i.e., decompresses the image, and  rescales it to a 760x320 format.   The system is composed of two blocks, as shown in Fig.1. The contribution of this paper is: we propose a  novel obstacle detection and classification system. Obstacle detection is based on inverse perspective  mapping and homography. Obstacle classification is based on fuzzy neural network. The rest of the paper is  organized as follows: Section 2 gives a brief description of obstacle detection and classification system. Then  in Section 3, we describe the algorithm in detail. Section 4 is experiment.  3. Obstacle detection and classification  Fig.1. Obstacle classification based on fuzzy neural network  A new Inverse Perspective Mapping (IPM) technique is proposed based on a robust estimation of the  vanishing point, which provides bird-view images of the road, so that facilitating the tasks of road modelling  and obstacle detection and tracking. This new approach has been design to cope with the instability that  cameras mounted on a moving obstacle suffer. The estimation of the vanishing point relies on a novel and  efficient feature extraction strategy, which segment the lane markings of the images by combining a  histogram-based segmentation with temporal and frequency filtering. Then, the vanishing point of each image  is stabilized by means of a temporal filtering along the estimates of previous images. In a last step, the IPM  438   Lizhuang Liu et al. /  AASRI Procedia  1 ( 2012 )  435 â€“ 440  image is computed based on the stabilized vanishing point. Tests have been carried out on several long video  sequences captured from cameras inside an obstacle being driven along highways and local roads, with  different illumination and weather conditions, presence of shadows, occluding obstacles, and slope changes.  Results have shown a significant improvement in terms of lane width constancy and parallelism between lane  markings over non-stabilized IPM algorithms.  3 ~| 1 1 X u Y vKRIC Z ÂªÂº ÂªÂº Â«Â» Â«Â» Â«Â» ÂªÂº   Â¬Â¼ Â«Â» Â«Â» Â«Â» Â«Â» Â¬Â¼ Â¬Â¼                                                                                                                                 (1)  Once the pitch and yaw angle values are obtained from the estimation of the vanishing point, we can build  stabilized IPM images by computing the transform matrix. This matrix describes how points of the real world  X = (X, Y, Z) are projected into points in the image, with coordinates, in pixels, u = (u, v). Three different  coordinate systems will be considered: i) the world coordinate system, assumed to be on the road, ii) the  camera coordinate system, and iii) the image coordinate system. A point in the real world, for instance  belonging to the road plane, may be expressed as X with respect to the world coordinate system, or x = (x, y, z) with respect to the camera coordinate system. The transform that links these expressions is shown in equation  (1)Ëˆwhere R is the rotation matrix. This latter matrix is denominated K, or the camera calibration matrix.  The transform between image points and world coordinates is immediately obtained by multiplying the  expressions in below, where I3 is the 3Ã—3 identity matrix.   This paper presents a full system for obstacle detection and classification in non-stationary settings based  on computer vision. The method proposed for obstacle detection exploits the geometrical relations between  the elements in the scene so that obstacles can be detected by analyzing motion parallax. Namely, the  homography of the road plane between successive images is computed. The estimated homography is used for  image alignment, which in turn allows to detect the obstacles in the image.   ' 11 11 12 13 ' 2212232 ' 31 32 33 3 x x hhh x hhh x hhh x x Â§Â· Â§Â· ÂªÂº Â¨Â¸ Â¨Â¸ Â«Â»   Â¨Â¸ Â¨Â¸ Â¨Â¸ Â¨Â¸ Â«Â» Â¬Â¼ Â©Â¹ Â©Â¹                                                                                                                   (2)  ' X HX                                                                                                                                                       (3)                                                                                                                                                    k1 X k HX                                                                                                                                                     (4)  A fuzzy decision fusion method with fuzzy attribution for obstacle detection and classification application  is proposedËŠThe fuzzy decision function modifies parameters with auto-adapted algorithm to get better  classification probabilityËŠ It is shown that the method can achieve better classification resultËŠ A new information fusion decision method about fusing two signal based on fuzzy neural network was  proposed in this paperËŠFuzziness is one of the general characteristics of human thinking and objective things.   Fuzzy logic is a form of many-valued logic or probabilistic logic; it deals with reasoning that is  approximate rather than fixed and exact. In contrast with traditional logic theory, where binary sets havetwo- valued logic: true or false, fuzzy logic variables may have a truth value that ranges in degree between 0 and 1.  Fuzzy logic has been extended to handle the concept of partial truth, where the truth value may range between  completely true and completely false.[11] Furthermore, when linguistic variables are used, these degrees may  439 Lizhuang Liu et al. /  AASRI Procedia  1 ( 2012 )  435 â€“ 440  be managed by specific functions. Fuzzy logic began with the 1965 proposal of fuzzy set theory by Lotfi  Zadeh.[8][9] Fuzzy logic has been applied to many fields, from control theory  to artificial intelligence. The  reasoning in fuzzy logic is similar to human reasoning. It allows for approximate values and inferences as  well as incomplete or ambiguous data (fuzzy data) as opposed to only relying on crisp data (binary yes/no  choices). Fuzzy logic is able to process incomplete data and provide approximate solutions to problems other  methods find difficult to solve. Terminology used in fuzzy logic not used in other methods are: very high,  increasing, somewhat decreased, reasonable and very low.[10] An artificial neural network (ANN), usually  called neural network (NN), is a mathematical model or computational model that is inspired by the structure  and/or functional aspects of biological neural networks. A neural network consists of an interconnected group  of artificial neurons, and it processes information using a connectionist approach to computation. In most  cases an ANN is an adaptive system that changes its structure based on external or internal information that  flows through the network during the learning phase. Modern neural networks are non-linear statistical data  modeling tools. They are usually used to model complex relationships between inputs and outputs or to find  patterns in data.  The decision function of the fuzzy neural network is :     9 1 * ii i Sma     Â–                                                                                                                                           (5)  4. Experiment Fig 2 show our algorithm can work in complex environment. Tests have been made on images belonging to  video sequences captured from cameras inside a car being driven along different types of road, including  highways and local roads. The proposed strategy has been efficiently implemented in a general purpose PC,  working in real time for different frame rates according to the size of the images: 15 fps. Overall, an average  detection rate of above 92% is obtained for a set of scenarios, including different illumination, weather and  traffic conditions.             Fig 2 detection result  References  [1]E. Dagan, O. Mano, G.P. Stein and A. Shashua, Ä€Forward Collision Warning with a Single CameraÄ� IEEE Intelligent vehicles Symposium (IV2004) June 2004, Parma, ItalyËˆp201-205  440   Lizhuang Liu et al. /  AASRI Procedia  1 ( 2012 )  435 â€“ 440  [2] Yong ZhouËˆRong XuËˆXiaofeng HuËˆQingtai YeËŠA robust lane detection and tracking method based  on computer vision. Measurement Science and Technology. 2006,21 (17): p736-745ËŠ [3] Y. Wang, E. K. Teoh, D. Shen. Lane detection and tracking using B-snake. Image Vision Compute.,  2004,22(1): p269Ì¢280ËŠ [4] J. C. McCall, M. M. Trivedi. An integrated, robust approach to lane marking detection and lane tracking.  In Proceedings of IEEE Intelligent vehicles, Parma, Italy, 2004: 533Ì¢537ËŠ [5] N. Matthews, P. An, D. Charnley, and C. Harris, â€œvehicle detection and recognition in greyscale imagery,â€�  Control Engineering Practice, vol. 4, p473â€“479, 1996.  [6]Bo Wu, Nevatia R. Detection of multiple, partially occluded humans in a single image by Bayesian  combination of edgelet part detectors[C]. In: Proceedings of IEEE International Conference on Computer  Vision, 2005, 1:p90-97.  [7]Masoud O, Papanikolopoulos N. A novel method for tracking and counting pedestrians in real-time using a  single camera[J]. IEEE Transactions on Vehicular Technology, 2001, 50(5): p1267-1278.  [8] http://www.mobileye.info/en/index.html  [9] C. Goerick, N. Detlev and M.Werner, â€œArtificial neural networks in real-time vehicle detection and  tracking applications,â€� Pattern Recognition Letters, vol. 17, p335â€“343, 1996.  [10] Z. Sun, R. Miller, G. Bebis and D. Dimeo, â€œA real-time precrash vehicle detection,â€� IEEE Intelligent  vehicles Symposium 2000. Dearborn, MI, USA.p35â€“43.  [11]S. Baker and I. Matthews. Lucas-Kanade 20 years on: A unifying framework: Part 1: The quantity  approximated, the warp update rule, and the gradient descent approximation. International Journal of  Computer Vision, 2003, p38â€“45.  [12] Bertsekas, D.P., Tsitsiklis, J.N. (1996). Neuro-dynamic programming. Athena Scientific.  pp. 512. ISBN 1-886529-10-8  [13] DANN:Genetic Wavelets. dANN project. Retrieved 12 July 2010   or ambiguous data (fuzzy data) as opposed to only relying on crisp data (binary yes/no  choices). Fuzzy logic is able to process incomplete data and provide approximate solutions to problems other  methods find difficult to solve. Terminology used in fuzzy logic not used in other methods are: very high,  increasing, somewhat decreased, reasonable and very low.[10] An artificial neural network (ANN), usually  called neural network (NN), is a mathematical model or computational model that is inspired by the structure  and/or functional aspects of biological neural networks. A neural network consists of an interconnected group  of artificial neurons, and it processes information using a connectionist approach to computation. In most  cases an ANN is an adaptive system that changes its structure based on external or internal information that  flows through the network during the learning phase. Modern neural networks are non-linear statistical data  modeling tools. They are usually used to model complex relationships between inputs and outputs or to find  patterns in data.  The decision function of the fuzzy neural network is :     9 1 * ii i Sma     Â–                                                                                                                                           (5)  4. Experiment Fig 2 show our algorithm can work in complex environment. Tests have been made on images belonging to  video sequences captured from cameras inside a car being driven along different types of road, including  highways and local roads. The proposed strategy has been efficiently implemented in a general purpose PC,  working in real time for different frame rates according to the size of the images: 15 fps. Overall, an average  detection rate of above 92% is obtained for a set of scenarios, including different illumination, weather and  traffic conditions.             Fig 2 detection result  References  [1]E. Dagan, O. Mano, G.P. Stein and A. Shashua, Ä€Forward Collision Warning with a Single CameraÄ� IEEE Intelligent vehicles Symposium (IV2004) June 2004, Parma, ItalyËˆp201-205  440   Lizhuang Liu et al. /  AASRI Procedia  1 ( 2012 )  435 â€“ 440  [2] Yong ZhouËˆRong XuËˆXiaofeng HuËˆQingtai YeËŠA robust lane detection and tracking method based  on comp</xocs:rawtext><xocs:serial-item><article xmlns="http://www.elsevier.com/xml/ja/dtd" version="5.1" xml:lang="en" docsubtype="fla"><item-info><jid>AASRI</jid><aid>68</aid><ce:pii>S2212-6716(12)00069-8</ce:pii><ce:doi>10.1016/j.aasri.2012.06.068</ce:doi><ce:copyright type="unknown" year="2012"/></item-info><head><ce:title>Obstacle Detection and Classification in Dynamical Background</ce:title><ce:author-group><ce:author><ce:given-name>Lizhuang</ce:given-name><ce:surname>Liu</ce:surname><ce:cross-ref refid="aff0005"><ce:sup loc="post">a</ce:sup></ce:cross-ref><ce:cross-ref refid="aff0010"><ce:sup loc="post">b</ce:sup></ce:cross-ref><ce:e-address type="email">liulz@sari.ac.cn</ce:e-address></ce:author><ce:author><ce:given-name>Jianzhu</ce:given-name><ce:surname>Cui</ce:surname><ce:cross-ref refid="aff0010"><ce:sup loc="post">b</ce:sup></ce:cross-ref><ce:e-address type="email">cuijianlou@163.com</ce:e-address></ce:author><ce:author><ce:given-name>Jing</ce:given-name><ce:surname>Li</ce:surname><ce:cross-ref refid="aff0015"><ce:sup loc="post">c</ce:sup></ce:cross-ref><ce:cross-ref refid="cor0005"><ce:sup loc="post">⁎</ce:sup></ce:cross-ref><ce:e-address type="email">j_li@shou.edu.cn</ce:e-address></ce:author><ce:affiliation id="aff0005"><ce:label>a</ce:label><ce:textfn>School of Communication and Information Engineering,Shanghai University, no.99 Shangda Road, Shanghai 200444,China</ce:textfn></ce:affiliation><ce:affiliation id="aff0010"><ce:label>b</ce:label><ce:textfn>Shanghai Advanced Research Institute, Chinese Academy of Sciences, No.99 Haike Road, Shanghai, 201203,China</ce:textfn></ce:affiliation><ce:affiliation id="aff0015"><ce:label>c</ce:label><ce:textfn>College of Information Technology, Shanghai Ocean University, No.999 huhuanchen Road, Shanghai 201303, China</ce:textfn></ce:affiliation><ce:correspondence id="cor0005"><ce:label>⁎</ce:label><ce:text>Corresponding author. Tel.: +86 021 20350822; fax: +86 021 20350822.</ce:text></ce:correspondence></ce:author-group><ce:abstract class="author"><ce:section-title>Abstract</ce:section-title><ce:abstract-sec><ce:simple-para id="spar0005" view="all">In Obstacle detection is based on inverse perspective mapping and homography. Obstacle classification is based on fuzzy neural network. The estimation of the vanishing point relies on feature extraction strategy. The method exploits the geometrical relations between the elements in the scene so that obstacle can be detected. The estimated homography of the road plane between successive images is used for image alignment. A new fuzzy decision fusion method with fuzzy attribution for obstacle detection and classification application is described The fuzzy decision function modifies parameters with auto-adapted algorithm to get better classification probability It is shown that the method can achieve better classification result</ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords class="keyword"><ce:section-title>Keywords</ce:section-title><ce:keyword><ce:text>object detection</ce:text></ce:keyword><ce:keyword><ce:text>image alignment</ce:text></ce:keyword><ce:keyword><ce:text>fuzzy neural network</ce:text></ce:keyword></ce:keywords></head><tail view="all"><ce:bibliography id="bibl0005" view="all"><ce:section-title>References</ce:section-title><ce:bibliography-sec id="bibs0005"><ce:bib-reference id="bib0005"><ce:label>[1]</ce:label><ce:other-ref><ce:textref>E. Dagan, O. Mano, G.P. Stein and A. Shashua, “Forward Collision Warning with a Single Camera”. IEEE Intelligent vehicles Symposium (IV2004) June 2004, Parma, Italy p201-205.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0010"><ce:label>[2]</ce:label><ce:other-ref><ce:textref>Yong Zhou, Rong Xu, Xiaofeng Hu, Qingtai Ye, A robust lane detection and tracking method based. on computer vision. Measurement Science and Technology. 2006,21 (17): p736-745.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0015"><ce:label>[3]</ce:label><ce:other-ref><ce:textref>Y. Wang, E.K. Teoh, D. Shen. Lane detection and tracking using B-snake. Image Vision Compute. 2004,22(1): p269-280.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0020"><ce:label>[4]</ce:label><ce:other-ref><ce:textref>J. C. McCall, M.M. Trivedi. An integrated, robust approach to lane marking detection and lane tracking. In Proceedings of IEEE Intelligent vehicles, Parma, Italy, 2004: 533-537.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0025"><ce:label>[5]</ce:label><ce:other-ref><ce:textref>N. Matthews, P. An, D. Charnley, and C. Harris, “vehicle detection and recognition in greyscale imagery,”. Control Engineering Practice, vol. 4, p473-479, 1996.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0030"><ce:label>[6]</ce:label><ce:other-ref><ce:textref>Bo Wu, Nevatia R. Detection of multiple, partially occluded humans in a single image by Bayesian. combination of edgelet part detectors[C]. In: Proceedings of IEEE International Conference on Computer. Vision, 2005, 1:p90-97.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0035"><ce:label>[7]</ce:label><ce:other-ref><ce:textref>Masoud O, Papanikolopoulos N. A novel method for tracking and counting pedestrians in real-time using a. single camera[J]. IEEE Transactions on Vehicular Technology, 2001, 50(5): p1267-1278.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0040"><ce:label>[8]</ce:label><ce:other-ref><ce:textref>http://www.mobileye.info/en/index.html.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0045"><ce:label>[9]</ce:label><ce:other-ref><ce:textref>C. Goerick, N. Detlev and M. Werner, “Artificial neural networks in real-time vehicle detection and. tracking applications,” Pattern Recognition Letters, vol. 17, p335-343, 1996.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0050"><ce:label>[10]</ce:label><ce:other-ref><ce:textref>Z. Sun, R. Miller, G. Bebis and D. Dimeo, “A real-time precrash vehicle detection,” IEEE Intelligent. vehicles Symposium 2000. Dearborn, MI, USA.p35-43.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0055"><ce:label>[11]</ce:label><ce:other-ref><ce:textref>S. Baker and I. Matthews. Lucas-Kanade 20 years on: A unifying framework: Part 1: The quantity. approximated, the warp update rule, and the gradient descent approximation. International Journal of. Computer Vision, 2003, p38-45.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0060"><ce:label>[12]</ce:label><ce:other-ref><ce:textref>Bertsekas, D.P., Tsitsiklis, J.N. (1996). Neuro-dynamic programming. Athena Scientific. pp. 512. ISBN 1-886529-10-8.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0065"><ce:label>[13]</ce:label><ce:other-ref><ce:textref>DANN:Genetic Wavelets. dANN project. Retrieved 12 July 2010.</ce:textref></ce:other-ref></ce:bib-reference></ce:bibliography-sec></ce:bibliography></tail></article></xocs:serial-item></xocs:doc></originalText></full-text-retrieval-response>