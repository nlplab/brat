<full-text-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/article/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:bk="http://www.elsevier.com/xml/bk/dtd"><coredata><prism:url>http://api.elsevier.com/content/article/pii/S2212667814001245</prism:url><dc:identifier>doi:10.1016/j.ieri.2014.09.076</dc:identifier><eid>1-s2.0-S2212667814001245</eid><prism:doi>10.1016/j.ieri.2014.09.076</prism:doi><pii>S2212-6678(14)00124-5</pii><dc:title>Improve Bayesian Network to Generating Vietnamese Sentence Reduction </dc:title><prism:publicationName>IERI Procedia</prism:publicationName><prism:aggregationType>Journal</prism:aggregationType><prism:issn>22126678</prism:issn><prism:volume>10</prism:volume><prism:startingPage>190</prism:startingPage><prism:endingPage>195</prism:endingPage><prism:pageRange>190-195</prism:pageRange><dc:format>text/xml</dc:format><prism:coverDate>2014-12-31</prism:coverDate><prism:coverDisplayDate>2014</prism:coverDisplayDate><prism:copyright>Copyright © 2014 The Authors. Published by Elsevier B.V.</prism:copyright><prism:publisher>The Authors. Published by Elsevier B.V.</prism:publisher><prism:issueName>International Conference on Future Information Engineering (FIE 2014)</prism:issueName><dc:creator>Thu, Ha Nguyen Thi</dc:creator><dc:creator>Ngoc, Dung Vu Thi</dc:creator><dc:description>AbstractSentence reduction is one of approaches for text summarization that has been attracted many researchers and scholars of natural language processing field. In this paper, we present a method that generates sentence reduction and applying in Vietnamese text summarization using Bayesian Network model. Bayesian network model is used to find the best likelihood short sentence through compare difference of probability. Experimental results with 980 sentences, show that our method really effectively in generating sentence reduction that understandable, readable and exactly grammar.</dc:description><openaccess>1</openaccess><openaccessArticle>true</openaccessArticle><openaccessType>Full</openaccessType><openArchiveArticle>false</openArchiveArticle><openaccessSponsorName/><openaccessSponsorType>ElsevierWaived</openaccessSponsorType><openaccessUserLicense>http://creativecommons.org/licenses/by-nc-nd/3.0/</openaccessUserLicense><dcterms:subject>Sentence reduction</dcterms:subject><dcterms:subject>natural language processing</dcterms:subject><dcterms:subject>text summarization</dcterms:subject><dcterms:subject>Bayesian network</dcterms:subject><dcterms:subject>probability;</dcterms:subject><link rel="self" href="http://api.elsevier.com/content/article/pii/S2212667814001245"/><link rel="scidir" href="http://www.sciencedirect.com/science/article/pii/S2212667814001245"/></coredata><scopus-id>84964728249</scopus-id><scopus-eid>2-s2.0-84964728249</scopus-eid><link rel="abstract" href="http://api.elsevier.com/content/abstract/scopus_id/84964728249"/><originalText><xocs:doc xmlns:xoe="http://www.elsevier.com/xml/xoe/dtd" xsi:schemaLocation="http://www.elsevier.com/xml/xocs/dtd http://be-prod3a/schema/dtds/document/fulltext/xcr/xocs-article.xsd"><xocs:meta><xocs:content-family>serial</xocs:content-family><xocs:content-type>JL</xocs:content-type><xocs:cid>282178</xocs:cid><xocs:ssids><xocs:ssid type="alllist">291210</xocs:ssid><xocs:ssid type="subj">291773</xocs:ssid><xocs:ssid type="subj">291800</xocs:ssid><xocs:ssid type="subj">291880</xocs:ssid><xocs:ssid type="subj">291882</xocs:ssid><xocs:ssid type="content">31</xocs:ssid><xocs:ssid type="oa">90</xocs:ssid></xocs:ssids><xocs:srctitle>IERI Procedia</xocs:srctitle><xocs:normalized-srctitle>IERIPROCEDIA</xocs:normalized-srctitle><xocs:orig-load-date yyyymmdd="20141001">2014-10-01</xocs:orig-load-date><xocs:available-online-date yyyymmdd="20141001">2014-10-01</xocs:available-online-date><xocs:ew-transaction-id>2014-10-10T04:56:58</xocs:ew-transaction-id><xocs:eid>1-s2.0-S2212667814001245</xocs:eid><xocs:pii-formatted>S2212-6678(14)00124-5</xocs:pii-formatted><xocs:pii-unformatted>S2212667814001245</xocs:pii-unformatted><xocs:doi>10.1016/j.ieri.2014.09.076</xocs:doi><xocs:item-stage>S300</xocs:item-stage><xocs:item-version-number>S300.2</xocs:item-version-number><xocs:item-weight>HEAD-AND-TAIL</xocs:item-weight><xocs:hub-eid>1-s2.0-S2212667814X00067</xocs:hub-eid><xocs:timestamp yyyymmdd="20150515">2015-05-15T07:34:50.756359-04:00</xocs:timestamp><xocs:dco>0</xocs:dco><xocs:tomb>0</xocs:tomb><xocs:date-search-begin>20140101</xocs:date-search-begin><xocs:date-search-end>20141231</xocs:date-search-end><xocs:year-nav>2014</xocs:year-nav><xocs:indexeddate epoch="1412186304">2014-10-01T17:58:24.848272Z</xocs:indexeddate><xocs:articleinfo>rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist webpdf webpdfpagecount yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref</xocs:articleinfo><xocs:issns><xocs:issn-primary-formatted>2212-6678</xocs:issn-primary-formatted><xocs:issn-primary-unformatted>22126678</xocs:issn-primary-unformatted></xocs:issns><xocs:crossmark is-crossmark="1">true</xocs:crossmark><xocs:vol-first>10</xocs:vol-first><xocs:volume-list><xocs:volume>10</xocs:volume></xocs:volume-list><xocs:suppl>C</xocs:suppl><xocs:vol-iss-suppl-text>Volume 10</xocs:vol-iss-suppl-text><xocs:sort-order>28</xocs:sort-order><xocs:first-fp>190</xocs:first-fp><xocs:last-lp>195</xocs:last-lp><xocs:pages><xocs:first-page>190</xocs:first-page><xocs:last-page>195</xocs:last-page></xocs:pages><xocs:cover-date-orig><xocs:start-date>2014</xocs:start-date></xocs:cover-date-orig><xocs:cover-date-text>2014</xocs:cover-date-text><xocs:cover-date-start>2014-01-01</xocs:cover-date-start><xocs:cover-date-end>2014-12-31</xocs:cover-date-end><xocs:cover-date-year>2014</xocs:cover-date-year><xocs:title-editors-groups><xocs:title-editors-group><ce:title>International Conference on Future Information Engineering (FIE 2014)</ce:title><ce:editors><ce:author-group><ce:author><ce:given-name>Garry</ce:given-name><ce:surname>Lee</ce:surname></ce:author></ce:author-group></ce:editors></xocs:title-editors-group></xocs:title-editors-groups><xocs:hub-sec><xocs:hub-sec-title>Management Science and Technology</xocs:hub-sec-title></xocs:hub-sec><xocs:document-type>article</xocs:document-type><xocs:document-subtype>fla</xocs:document-subtype><xocs:copyright-line>Copyright © 2014 The Authors. Published by Elsevier B.V.</xocs:copyright-line><xocs:normalized-article-title>IMPROVEBAYESIANNETWORKGENERATINGVIETNAMESESENTENCEREDUCTION</xocs:normalized-article-title><xocs:normalized-first-auth-surname>THU</xocs:normalized-first-auth-surname><xocs:normalized-first-auth-initial>H</xocs:normalized-first-auth-initial><xocs:references><xocs:ref-info refid="oref0005"/><xocs:ref-info refid="oref0010"/><xocs:ref-info refid="oref0015"/><xocs:ref-info refid="oref0020"/><xocs:ref-info refid="oref0025"/><xocs:ref-info refid="oref0030"/><xocs:ref-info refid="oref0035"/><xocs:ref-info refid="oref0040"/><xocs:ref-info refid="sbref0045"><xocs:ref-normalized-surname>JENIETURNER</xocs:ref-normalized-surname><xocs:ref-pub-year>2005</xocs:ref-pub-year><xocs:ref-first-fp>290</xocs:ref-first-fp><xocs:ref-last-lp>297</xocs:ref-last-lp></xocs:ref-info><xocs:ref-info refid="sbref0050"><xocs:ref-normalized-surname>KNIGHT</xocs:ref-normalized-surname><xocs:ref-pub-year>2002</xocs:ref-pub-year><xocs:ref-first-fp>91</xocs:ref-first-fp><xocs:ref-last-lp>107</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="oref0055"/><xocs:ref-info refid="oref0060"/><xocs:ref-info refid="oref0065"/><xocs:ref-info refid="oref0070"/><xocs:ref-info refid="oref0075"/><xocs:ref-info refid="oref0080"/><xocs:ref-info refid="oref0085"/><xocs:ref-info refid="oref0090"/><xocs:ref-info refid="oref0095"/><xocs:ref-info refid="oref0100"/><xocs:ref-info refid="sbref0105"><xocs:ref-normalized-surname>AHO</xocs:ref-normalized-surname><xocs:ref-pub-year>1969</xocs:ref-pub-year><xocs:ref-first-fp>319</xocs:ref-first-fp><xocs:ref-last-lp>334</xocs:ref-last-lp><xocs:ref-normalized-initial>V</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0110"><xocs:ref-normalized-surname>STOCHASTIC</xocs:ref-normalized-surname><xocs:ref-pub-year>1997</xocs:ref-pub-year><xocs:ref-first-fp>377</xocs:ref-first-fp><xocs:ref-last-lp>404</xocs:ref-last-lp><xocs:ref-normalized-initial>W</xocs:ref-normalized-initial></xocs:ref-info></xocs:references><xocs:refkeys><xocs:refkey3>THUX2014X190</xocs:refkey3><xocs:refkey4lp>THUX2014X190X195</xocs:refkey4lp><xocs:refkey4ai>THUX2014X190XH</xocs:refkey4ai><xocs:refkey5>THUX2014X190X195XH</xocs:refkey5></xocs:refkeys><xocs:open-access><xocs:oa-article-status is-open-access="1" is-open-archive="0">Full</xocs:oa-article-status><xocs:oa-access-effective-date>2014-10-01T11:57:40Z</xocs:oa-access-effective-date><xocs:oa-sponsor><xocs:oa-sponsor-type>ElsevierWaived</xocs:oa-sponsor-type></xocs:oa-sponsor><xocs:oa-user-license>http://creativecommons.org/licenses/by-nc-nd/3.0/</xocs:oa-user-license><xocs:oa-access-inherited-from winid="http://vtw.elsevier.com/content/oaw/PROC_UNBOUNDED_ESWaived">OA-Window</xocs:oa-access-inherited-from></xocs:open-access><xocs:attachment-metadata-doc><xocs:attachment-set-type>item</xocs:attachment-set-type><xocs:pii-formatted>S2212-6678(14)00124-5</xocs:pii-formatted><xocs:pii-unformatted>S2212667814001245</xocs:pii-unformatted><xocs:eid>1-s2.0-S2212667814001245</xocs:eid><xocs:doi>10.1016/j.ieri.2014.09.076</xocs:doi><xocs:cid>282178</xocs:cid><xocs:timestamp>2014-10-10T10:10:04.751995-04:00</xocs:timestamp><xocs:cover-date-start>2014-01-01</xocs:cover-date-start><xocs:cover-date-end>2014-12-31</xocs:cover-date-end><xocs:attachments><xocs:web-pdf><xocs:attachment-eid>1-s2.0-S2212667814001245-main.pdf</xocs:attachment-eid><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2212667814001245/MAIN/application/pdf/19796c08e77a639b5af9c951f80cd1b3/main.pdf</xocs:ucs-locator><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2212667814001245/MAIN/application/pdf/19796c08e77a639b5af9c951f80cd1b3/main.pdf</xocs:ucs-locator><xocs:filename>main.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:pdf-optimized>true</xocs:pdf-optimized><xocs:filesize>307756</xocs:filesize><xocs:web-pdf-purpose>MAIN</xocs:web-pdf-purpose><xocs:web-pdf-page-count>6</xocs:web-pdf-page-count><xocs:web-pdf-images><xocs:web-pdf-image><xocs:attachment-eid>1-s2.0-S2212667814001245-main_1.png</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S2212667814001245/PREVIEW/image/png/8fd41cc8b182ce01619d29e5dffc9e91/main_1.png</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S2212667814001245/PREVIEW/image/png/8fd41cc8b182ce01619d29e5dffc9e91/main_1.png</xocs:ucs-locator><xocs:filename>main_1.png</xocs:filename><xocs:extension>png</xocs:extension><xocs:filesize>42654</xocs:filesize><xocs:pixel-height>849</xocs:pixel-height><xocs:pixel-width>656</xocs:pixel-width><xocs:attachment-type>IMAGE-WEB-PDF</xocs:attachment-type><xocs:pdf-page-num>1</xocs:pdf-page-num></xocs:web-pdf-image></xocs:web-pdf-images></xocs:web-pdf></xocs:attachments></xocs:attachment-metadata-doc></xocs:meta><xocs:rawtext> IERI Procedia   10  ( 2014 )  190 â€“ 195  Available online at www.sciencedirect.com 2212-6678  2014 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license  (http://creativecommons.org/licenses/by-nc-nd/3.0/). Selection and peer review under responsibility of Information Engineering Research Institute doi: 10.1016/j.ieri.2014.09.076  ScienceDirect 2014 International Conference on Future Information Engineering  Improve Bayesian Network to Generating Vietnamese Sentence  Reduction  Ha Nguyen Thi Thu a* , Dung Vu Thi Ngoc b   a Information Techonology Faculty, Vietnam Electric Power University, Hanoi, Vietnam  b HaiDuong Center for Continuing Education, HaiDuong, Vietnam    Abstract  Sentence reduction is one of approaches for text summarization that has been attracted many researchers and scholars of  natural language processing field. In this paper, we present a method that generates sentence reduction and applying in  Vietnamese text summarization using Bayesian Network model. Bayesian network model is used to find the best  likelihood short sentence through compare difference of probability. Experimental results with 980 sentences, show that  our method really effectively in generating sentence reduction that understandable, readable and exactly grammar.     Â© 2014. Published by Elsevier B.V.  Selection and peer review under responsibility of Information Engineering Research Institute    Keywords: Sentence reduction, natural language processing, text summarization, Bayesian network, probability;  1. Introduction  Today, most text summarization systems based on extracted sentences to generate a summary and we  called extracted approach [9], [12], [13], [14]. With this approach, the weight of sentence is calculated based  on some features that we think it is important like: term frequency, sentence position, sentence length... And      *  * Corresponding author. Tel.: +84906113373  E-mail address: hantt@epu.edu.vn.   2014 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license  (http://creativecommons.org/licenses/by-nc-nd/3.0/). Selection and peer review under responsibility of Information Engineering Research Institute 191 Ha Nguyen Thi Thu and Dung Vu Thi Ngoc /  IERI Procedia  10 ( 2014 )  190 â€“ 195  then, sentences will be sorted by its weight and extracted based on the rate (extraction rate). A text summary  is including sentences that has maximum weight from the original text. With this approach, text summary will  synthesis of the discrete sentences from original text, it can be:  xText summarization is seamlessly because of sentences are not linked by content in the text, specially,  when the extraction rate is smaller, it will be greater discrete.  xText summarization is confusing sometimes it can be loosen important information in original text by  some sentences that have been not extracted.  Therefore, we have chosen the sentence reduction approach for processing in sentence level, remove not  important words in the sentence and generate new sentence and creating summary. Target text will overcome  some disadvantages that have been analyzed above [5], [6], [7].   This paper present a Vietnamese sentence reduction method based on Bayesian network, each word in  original text is considered as a node of the network. Reduced sentence is generated by find a path that is the  shortest and greatest weight, we called it: the best likelihood path.  The next structure of this paper: In Section 2, is overview of related work, the methodology of sentence  reduction based on Bayesian network method in section 3, Section 4 is the experimental results and finally is  conclusion.  2. Related works  Almost related works focus in building lexical rules model or syntax parser tree. First Aho and Ullman  using synchronous context free grammars (SCFGs)[21]. Wu in 1997 has proposed a method that included  inversion transduction grammar [22] and some other relate research with CFG like head transducers by  Alshawi, Bangalore and Douglas in 2000.  Knight and Marcu proposed a noisy channel of sentence compression. They use two components: P(y) is  language model and P(x|y) is a channel model. P(x|y) capturing the probability of original sentence x and  target compression y. After that, using decoding algorithm searches for maximizes of P(x)P(x|y). This channel  model is a SCFG, parallel corpus is used to extracted rules, and weights estimated using maximum likelihood  [9].  With Vietnamese sentence reduction approach, most of the methods are applied from English method.  However, the performance of this method is not high when applied to Vietnamese language. Because of single  syllable language, In Vietnamese, words can not be determined based on space..... So that, they often use  extraction approach for building Vietnamese text summarization systems, there are some method that use  reduction approach but it is not high effectively.  3. Methodology of sentence reduction based on bayesian network  3.1.  Bayesian Network  Bayesian networks is the one of probabilistic graphical models. When represent the uncertain knowledge  can used graphical models. In Bayesian model, each node is a random variable, and the edges between the  nodes represent prbalilistic dependencies among the corresponding random variables. This probabilistic can  be computed from history data [2].   If B is a Bayesian network B, so that B is an annotated acyclic graph and B represents a joint probability  distribution over a set of random variables V. This network is defined:   B=&lt;G, Ô¦&gt;  In which:   192   Ha Nguyen Thi Thu and Dung Vu Thi Ngoc /  IERI Procedia  10 ( 2014 )  190 â€“ 195  xG is directed acyclic graph whose nodes X 1 ,X 2 ,â€¦,X n  represents random variables, each variable Xi is  independent of its non=descendants given its parents in G, denoted generically as i 3 .   xÔ¦, denotes the set of parameters of the network.  This set contains the parameter )|( | iiBx xP ii 3  3 T  for each realization of x i  of X i  conditioned on i 3 ,  the set of parents of X i  in G. So, B defines a unique joint probability distribution over V.                                          (1)  3.2. Methodology of sentence reduction based on bayesian network  Supposed that have a sentence S, each word w k  in S was generated by w k-1  or can be generated by w k-2  w k-n .  Then, we can build a improving Bayesian network that can find a reduced sentence based on probability of n- grams between word wk and word w k-n  with  )1(,1   kn  .  Need to find an initial state. Supposed that, the set of initial states with probability: Start -&gt; w 1 = 0.6, Start - &gt; w 2 =0.32, Start -&gt; w 3 = 0.47, Start -&gt; w 4 =0.56, Start -&gt; w 5 =0.2, Start -&gt; w 6 = 0.11.  So that, we choose w 1  is the initial state, so reduced sentence will be started by w 1 . Figure 1 illustrates the  structure of Bayesian network with six words in sentence S. Word w 1  is considered the first likely node to the  next words in sentence S. And word w 2  can be created a path to w 3 , w 4 , w 5 , w 6 .    Fig.1  Bayesian network model with 6 words  To overcome the computational complexity, in this proposed we use dynamic programming and donâ€™t need  to compute n-grams probability over all node. For example, have probability as: w1 -&gt; w2=0.3, w1 -&gt; w3 =  0.6, w1-&gt; w4 = 0.042, w1 -&gt; w5= 0.002, w1 -&gt; w6 =0. Choose one state that have maximum probabily. So  that, w3 will be choosen and use path that contain w3.  For ease of visualization, Bayesian networks is  described as a tree as Figure 2    0 . 2 4 0 . 0 0 4 0 . 3   Fig.2. Bayesian network with probability.  193 Ha Nguyen Thi Thu and Dung Vu Thi Ngoc /  IERI Procedia  10 ( 2014 )  190 â€“ 195  In this Bayesian network. Probability ability on the branch is calculated by n-grams. In this example, path  from word w 1  to word w 2  has probability is 0.3. The first point of the sentence S is w 1 . From word w 1   sentence reduced by:   xFrom w1 we have some likely paths to w2, w3, w4, w5, w6.   xSelecting the path is weighted the highest probability. For example in the figure 4 is w1-&gt; w3 = 0.6   xSave point in the highest path. For example w3 will be stored.   xFrom this point of highest path find some paths to the other words, choosing the most likelihood path. For  example w3-&gt; w4.   xContinue to repeat the last word of the sentence S.   Finish we have a sentence that has been reduced, and reduced sentence include four words w 1 w 3 w 5 w 6 .  In figure 3, we simulated an algorithm which called SRBBN (Sentence Reduction Based on Bayesian  Network) as  Sentence Reduction Based on Bayesian Network Algorithm  Input: S: original sentence;  Output: S : reduced sentence;  1. Initialization  ;I T ;' I T ;I N ;1 i ;0 j 2. Separate words from S  For i=1 to Length (S)   T(i)m Separate (S);  3.  Selecting word from original sentence  While (i&lt; Length(S) do     begin            for jm i+1 to Length(S) do                                 begin         N(j)mNgrams(wj,wi); point=argmax(N(j));  Tâ€™=Tâ€™ Â‰  w j                                          end;              i=j;  ;I N                         end;  4. Generating sentence reduction;  Sâ€™=Order(Tâ€™);  Fig. 3 Sentence Reduction Based on Bayesian Network Algorithm  In this algorithm, we use some functions: Separate() is used separate words in sentence. Length() returns  length of sentence N-grams use for calculating n-grams of word wi and word wj that learns from training data.  Argmax() takes the largest value in the set N. Order() use for sorting words in the original sentences to  generate reduced sentence.  4. Experimentals  There is not standard corpus for Vietnamese text summarization now. So that, in our experimental, we built  corpus by manual. Documents in this corpus had been downloaded from websitesâ€™s news as:  http://thongtincongnghe.com, http://echip.com, http://vnexpress.net, http://vietnamnet.vn, http://tin247.com...  194   Ha Nguyen Thi Thu and Dung Vu Thi Ngoc /  IERI Procedia  10 ( 2014 )  190 â€“ 195  Corpusâ€™s entitle is "information" and "technology". There are over 300 documents in it. We segmented from  300 documents into 16,117 sentences.  After that, We use a Vietnamese text segmentation tool that use for  word segmentation . We selected 814 sentences in the corpus for signed label.  There isnâ€™t a standard evaluation for Vietnamese sentence reduction methods. So that, we use the  evaluation way by Knight and Marcu to compare our proposed (called SRBBN) with some other methods  likes: Human, Syn.con (proposed by M.L Nguyen using syntax control) [9].  Below is the results of compare between our proposed method with two other methods (in Table 1)  Table 1 Experimental Results for Vietnamese Sentence Reduction  Method Compression Grammaticality Word significance  weight   Baseline X X X  SRBBN 65.82 84.2 78.4  Human 61.2209 83.33333 63.5  Syn.con 67 65. 7 6.11  5. Conclusion  This paper has proposed a novel method (we called SRBBN) to reducing Vietnamese sentence. The  method based on Bayesian Network to find the best sentence reduction. We used a corpus of 16,117 sentences  of Vietnamese text for training and test with 980 sentences shown that this method achieved acceptable results  when compared with human. Sentences after reduced is satisfied user requirement, readable, understandable  and well grammatically.    References  [1] Ann Arbor, et. al. Constraint based sentence compression: An integer programming 306 approach.  Proceedings of the COLING/ACL 2006, pp 144 â€“ 151, 2006  [2] Blanco, Roi; Lioma; Christina, Graph based term weighting for information retrieval, Information  Retrieval, pp. 54-92, 2012  [3] Courtney Napoles; Chris Callison-Burch; Juri Ganitkevitch, Benjamin Van Durme; Paraphrastic Sentence  Compression with a Character based Metric: Tightening without Deletion, Proceedings of the 49th Annual  Meeting of the Association for Computational Linguistics, pp. 84â€“90, Portland, 2011  [4] David Vickrey; Daphne Koller, Sentence Simplification for Semantic Role Labeling, Proceedings of  ACL-08: HLT, pp. 344â€“352, USA, 2008.  [5] Hongyan Jing; Kathleen R. McKeown. Cut and paste based text summarization. In Proceedings of the 1st  Conference of the North American Chapter of the Association for Computational Linguistics, pp 178â€“185,  2000.   [6] H. Jing; K. McKeown, The Decomposition of Human-Written Summary Sentences, Proceedings of the  Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp.  129â€“136, 1999.  [7] H. Jing, Sentence reduction for automatic text summarization, Proceedings of the Conference on Applied  Natural Language Processing, pp. 310â€“315, 2000.  [8] H. Jing; K. R. McKeown, Cut and paste based text summarization, Proceedings of the North American  chapter of the Association for Computa-tional Linguistics Conference, pp. 178â€“185, 2000.  195 Ha Nguyen Thi Thu and Dung Vu Thi Ngoc /  IERI Procedia  10 ( 2014 )  190 â€“ 195  [9] Jenie Turner; Eugen Charniak. Supervised and unsupervised learning for sentence compression.  Proceedings of the 43rd Annual Meeting of the ACL, pp. 290â€“297, 2005.  [10] Knight, K.;Marcu, D. Summarization beyond sentence extraction: A probabilistic approach to sentence  compression. Artif. Intell. 139, 1 , 91-107, 2002.  [11] Lloret E; et.al, A. Towards building a competitive opinion summarization system: challenges and keys. In:  Proceedings of the NAACL. Student Research Workshop and Doctoral Consortium. pp 72â€“77, 2009.    [12] Lloret E; et.al, Experiments on summary-based opinion classification. In: Proceed-ings of the NAACL  HLT 2010 workshop on computational approaches to analysis and generation of emotion in text. pp 107â€“115,  2010.  [13] Lloret; et.al, Text summarisation in progress: a literature review, Springer Science &amp; Business Media,  pp.1-41, 2012  [14] Mani I, Automatic summarization. John Benjamins Publishing Co. Amsterdam, Philadelphia, USA, 2001.  [15] Mei Jian-ping; Chen, Lihui, SumCR: A new subtopic-based extractive approach for text summarization,  Knowledge and Information Systems 31.3  pp. 527-545, 2012.  [16] Michel Galley; Kathleen McKeown. Lexicalized Markov grammars for sentence compression.  Proceedings of the HLT-NAACL 2007, pp.180â€“187, 2007.  [17] M.L. Nguyen; S. Horiguchi, A Sentence Reduction Using Syntax Control, Proc. Of 6th Information  Retrieval with Asian Language, pp. 139-146, 2003.  [18] Nguyen, M.L.; et.al, M.. Probabilistic Sentence Reduction Using Support Vector Machines. Proceedings  of the 20th international conference on Computational Linguistics, 2004  [19] M. Johnson; E. Charniak, A TAG-based noisy-channel model of speech repairs, Proceedings of the  Annual Meeting of the Association for Computational Linguistics, pp. 33â€“39, 2004.  [20] Stefan Riezler; Tracy H. King; Richard Crouch; An-nie Zaenen, Statistical sentence condensation using  ambiguity packing and stochastic disambigua-tion methods for lexical functional grammar. Proceedings of  HLT-NAACL 2003, pp. 118â€“125, Ed-monton, 2003  [21] V. Aho and J.D. Ullman. Properties of syntax directed translations. Journal of Computer and System  Sciences, 3:319â€“334, 1969.   [22] Wu. Stochastic, Inversion transduction grammars and bilingual parsing of parallel corpora,  Computational Linguistics, 23(3):pp.377â€“404, 1997.    ntences after reduced is satisfied user requirement, readable, understandable  and well grammatically.    References  [1] Ann Arbor, et. al. Constraint based sentence compression: An integer programming 306 approach.  Proceedings of the COLING/ACL 2006, pp 144 â€“ 151, 2006  [2] Blanco, Roi; Lioma; Christina, Graph based term weighting for information retrieval, Information  Retrieval, pp. 54-92, 2012  [3] Courtney Napoles; Chris Callison-Burch; Juri Ganitkevitch, Benjamin Van Durme; Paraphrastic Sentence  Compression with a Character based Metric: Tightening without Deletion, Proceedings of the 49th Annual  Meeting of the Association for Computational Linguistics, pp. 84â€“90, Portland, 2011  [4] David Vickrey; Daphne Koller, Sentence Simplification for Semantic Role Labeling, Proceedings of  ACL-08: HLT, pp. 344â€“352, USA, 2008.  [5] Hongyan Jing; Kathleen R. McKeown. Cut and paste based text summarization. In Proceedings o</xocs:rawtext><xocs:serial-item><article xmlns="http://www.elsevier.com/xml/ja/dtd" version="5.2" xml:lang="en" docsubtype="fla"><item-info><jid>IERI</jid><aid>474</aid><ce:pii>S2212-6678(14)00124-5</ce:pii><ce:doi>10.1016/j.ieri.2014.09.076</ce:doi><ce:copyright type="other" year="2014">The Authors</ce:copyright></item-info><head><ce:article-footnote><ce:label>☆</ce:label><ce:note-para id="npar0005" view="all">Selection and peer review under responsibility of Information Engineering Research Institute.</ce:note-para></ce:article-footnote><ce:title id="tit0005">Improve Bayesian Network to Generating Vietnamese Sentence Reduction</ce:title><ce:author-group id="aug0005"><ce:author id="aut0005"><ce:given-name>Ha Nguyen Thi</ce:given-name><ce:surname>Thu</ce:surname><ce:cross-ref id="crf0005" refid="aff0005"><ce:sup loc="post">a</ce:sup></ce:cross-ref><ce:cross-ref id="crf0010" refid="cor0005"><ce:sup loc="post">⁎</ce:sup></ce:cross-ref><ce:e-address id="eadd0005" type="email">hantt@epu.edu.vn</ce:e-address></ce:author><ce:author id="aut0010"><ce:given-name>Dung Vu Thi</ce:given-name><ce:surname>Ngoc</ce:surname><ce:cross-ref id="crf0015" refid="aff0010"><ce:sup loc="post">b</ce:sup></ce:cross-ref></ce:author><ce:affiliation id="aff0005"><ce:label>a</ce:label><ce:textfn>Information Techonology Faculty, Vietnam Electric Power University, Hanoi, Vietnam</ce:textfn></ce:affiliation><ce:affiliation id="aff0010"><ce:label>b</ce:label><ce:textfn>HaiDuong Center for Continuing Education, HaiDuong, Vietnam</ce:textfn></ce:affiliation><ce:correspondence id="cor0005"><ce:label>⁎</ce:label><ce:text>Corresponding author. Tel.: +84906113373.</ce:text></ce:correspondence></ce:author-group><ce:abstract id="abs0005" view="all" class="author"><ce:section-title id="sect0005">Abstract</ce:section-title><ce:abstract-sec id="abst0005" view="all"><ce:simple-para id="spar0005" view="all">Sentence reduction is one of approaches for text summarization that has been attracted many researchers and scholars of natural language processing field. In this paper, we present a method that generates sentence reduction and applying in Vietnamese text summarization using Bayesian Network model. Bayesian network model is used to find the best likelihood short sentence through compare difference of probability. Experimental results with 980 sentences, show that our method really effectively in generating sentence reduction that understandable, readable and exactly grammar.</ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords id="kwd0005" class="keyword" view="all"><ce:section-title id="sect0010">Keywords</ce:section-title><ce:keyword id="kw0005"><ce:text>Sentence reduction</ce:text></ce:keyword><ce:keyword id="kw0010"><ce:text>natural language processing</ce:text></ce:keyword><ce:keyword id="kw0015"><ce:text>text summarization</ce:text></ce:keyword><ce:keyword id="kw0020"><ce:text>Bayesian network</ce:text></ce:keyword><ce:keyword id="kw0025"><ce:text>probability;</ce:text></ce:keyword></ce:keywords></head><tail view="all"><ce:bibliography id="bibl0005" view="all"><ce:section-title id="sect0020">References</ce:section-title><ce:bibliography-sec id="bibs0005" view="all"><ce:bib-reference id="bib0005"><ce:label>[1]</ce:label><ce:other-ref id="oref0005"><ce:textref>Ann Arbor, et. al. Constraint based sentence compression: An integer programming 306 approach. Proceedings of the COLING/ACL 2006, pp 144-151, 2006.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0010"><ce:label>[2]</ce:label><ce:other-ref id="oref0010"><ce:textref>Blanco, Roi; Lioma; Christina, Graph based term weighting for information retrieval, Information Retrieval, pp. 54-92, 2012.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0015"><ce:label>[3]</ce:label><ce:other-ref id="oref0015"><ce:textref>Courtney Napoles; Chris Callison-Burch; Juri Ganitkevitch, Benjamin Van Durme; Paraphrastic Sentence Compression with a Character based Metric: Tightening without Deletion, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pp. 84-90, Portland, 2011.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0020"><ce:label>[4]</ce:label><ce:other-ref id="oref0020"><ce:textref>David Vickrey; Daphne Koller, Sentence Simplification for Semantic Role Labeling, Proceedings of ACL-08: HLT, pp. 344-352, USA, 2008.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0025"><ce:label>[5]</ce:label><ce:other-ref id="oref0025"><ce:textref>Hongyan Jing; Kathleen R. McKeown. Cut and paste based text summarization. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics, pp 178-185, 2000. .</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0030"><ce:label>[6]</ce:label><ce:other-ref id="oref0030"><ce:textref>H. Jing; K. McKeown, The Decomposition of Human-Written Summary Sentences, Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 129-136, 1999.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0035"><ce:label>[7]</ce:label><ce:other-ref id="oref0035"><ce:textref>H. Jing, Sentence reduction for automatic text summarization, Proceedings of the Conference on Applied Natural Language Processing, pp. 310-315, 2000.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0040"><ce:label>[8]</ce:label><ce:other-ref id="oref0040"><ce:textref>H. Jing; K. R. McKeown, Cut and paste based text summarization, Proceedings of the North American chapter of the Association for Computa-tional Linguistics Conference, pp. 178-185, 2000.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0045"><ce:label>[9]</ce:label><sb:reference id="sbref0045"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Jenie Turner</ce:surname></sb:author><sb:author><ce:surname>Eugen Charniak</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Supervised and unsupervised learning for sentence compression</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Proceedings of the 43rd Annual Meeting of the ACL</sb:maintitle></sb:title></sb:series><sb:date>2005</sb:date></sb:issue><sb:pages><sb:first-page>290</sb:first-page><sb:last-page>297</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0050"><ce:label>[10]</ce:label><sb:reference id="sbref0050"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Knight</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Marcu</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Summarization beyond sentence extraction: A probabilistic approach to sentence compression</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Artif. Intell.</sb:maintitle></sb:title><sb:volume-nr>139</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2002</sb:date></sb:issue><sb:pages><sb:first-page>91</sb:first-page><sb:last-page>107</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0055"><ce:label>[11]</ce:label><ce:other-ref id="oref0055"><ce:textref>Lloret E; et.al, A. Towards building a competitive opinion summarization system: challenges and keys. In: Proceedings of the NAACL. Student Research Workshop and Doctoral Consortium. pp 72-77, 2009.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0060"><ce:label>[12]</ce:label><ce:other-ref id="oref0060"><ce:textref>Lloret E; et.al, Experiments on summary-based opinion classification. In: Proceed-ings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text. pp 107-115, 2010.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0065"><ce:label>[13]</ce:label><ce:other-ref id="oref0065"><ce:textref>Lloret; et.al, Text summarisation in progress: a literature review, Springer Science &amp; Business Media, pp.1-41, 2012.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0070"><ce:label>[14]</ce:label><ce:other-ref id="oref0070"><ce:textref>Mani I, Automatic summarization. John Benjamins Publishing Co. Amsterdam, Philadelphia, USA,;1; 2001.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0075"><ce:label>[15]</ce:label><ce:other-ref id="oref0075"><ce:textref>Mei Jian-ping; Chen, Lihui, Sum CR: A new subtopic-based extractive approach for text summarization, Knowledge and Information Systems 31.3 pp. 527-545, 2012.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0080"><ce:label>[16]</ce:label><ce:other-ref id="oref0080"><ce:textref>Michel Galley; Kathleen McKeown. Lexicalized Markov grammars for sentence compression. Proceedings of the HLT-NAACL 2007, pp.180-187, 2007.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0085"><ce:label>[17]</ce:label><ce:other-ref id="oref0085"><ce:textref>M.L. Nguyen; S. Horiguchi, A Sentence Reduction Using Syntax Control, Proc. Of 6th Information Retrieval with Asian Language, pp. 139-146, 2003.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0090"><ce:label>[18]</ce:label><ce:other-ref id="oref0090"><ce:textref>Nguyen, M.L.; et.al, M. Probabilistic Sentence Reduction Using Support Vector Machines. Proceedings of the 20th international conference on Computational Linguistics, 2004.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0095"><ce:label>[19]</ce:label><ce:other-ref id="oref0095"><ce:textref>M. Johnson; E. Charniak, A TAG-based noisy-channel model of speech repairs, Proceedings of the Annual Meeting of the Association for Computational Linguistics, pp. 33-39, 2004.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0100"><ce:label>[20]</ce:label><ce:other-ref id="oref0100"><ce:textref>Stefan Riezler; Tracy H. King; Richard Crouch; An-nie Zaenen, Statistical sentence condensation using ambiguity packing and stochastic disambigua-tion methods for lexical functional grammar. Proceedings of HLT-NAACL 2003, pp. 118-125, Ed-monton, 2003.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0105"><ce:label>[21]</ce:label><sb:reference id="sbref0105"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>V.</ce:given-name><ce:surname>Aho</ce:surname></sb:author><sb:author><ce:given-name>J.D.</ce:given-name><ce:surname>Ullman</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Properties of syntax directed translations</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of Computer and System Sciences</sb:maintitle></sb:title><sb:volume-nr>3</sb:volume-nr></sb:series><sb:date>1969</sb:date></sb:issue><sb:pages><sb:first-page>319</sb:first-page><sb:last-page>334</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0110"><ce:label>[22]</ce:label><sb:reference id="sbref0110"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>Wu.</ce:given-name><ce:surname>Stochastic</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Inversion transduction grammars and bilingual parsing of parallel corpora</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Computational Linguistics</sb:maintitle></sb:title><sb:volume-nr>23</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>1997</sb:date></sb:issue><sb:pages><sb:first-page>377</sb:first-page><sb:last-page>404</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference></ce:bibliography-sec></ce:bibliography></tail></article></xocs:serial-item></xocs:doc></originalText></full-text-retrieval-response>