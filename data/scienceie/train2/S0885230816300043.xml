<full-text-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/article/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:bk="http://www.elsevier.com/xml/bk/dtd"><coredata><prism:url>http://api.elsevier.com/content/article/pii/S0885230816300043</prism:url><dc:identifier>doi:10.1016/j.csl.2016.06.008</dc:identifier><eid>1-s2.0-S0885230816300043</eid><prism:doi>10.1016/j.csl.2016.06.008</prism:doi><pii>S0885-2308(16)30004-3</pii><dc:title>Acoustic adaptation to dynamic background conditions with asynchronous transformations </dc:title><prism:publicationName>Computer Speech &amp; Language</prism:publicationName><prism:aggregationType>Journal</prism:aggregationType><prism:issn>08852308</prism:issn><prism:volume>41</prism:volume><prism:startingPage>180</prism:startingPage><prism:endingPage>194</prism:endingPage><prism:pageRange>180-194</prism:pageRange><dc:format>text/xml</dc:format><prism:coverDate>2017-01-31</prism:coverDate><prism:coverDisplayDate>January 2017</prism:coverDisplayDate><prism:copyright>© 2016 The Authors. Published by Elsevier Ltd.</prism:copyright><prism:publisher>The Authors. Published by Elsevier Ltd.</prism:publisher><dc:creator>Saz, Oscar</dc:creator><dc:creator>Hain, Thomas</dc:creator><dc:description>AbstractThis paper proposes a framework for performing adaptation to complex and non-stationary background conditions in Automatic Speech Recognition (ASR) by means of asynchronous Constrained Maximum Likelihood Linear Regression (aCMLLR) transforms and asynchronous Noise Adaptive Training (aNAT). The proposed method aims to apply the feature transform that best compensates the background for every input frame. The implementation is done with a new Hidden Markov Model (HMM) topology that expands the usual left-to-right HMM into parallel branches adapted to different background conditions and permits transitions among them. Using this, the proposed adaptation does not require ground truth or previous knowledge about the background in each frame as it aims to maximise the overall log-likelihood of the decoded utterance. The proposed aCMLLR transforms can be further improved by retraining models in an aNAT fashion and by using speaker-based MLLR transforms in cascade for an efficient modelling of background effects and speaker. An initial evaluation in a modified version of the WSJCAM0 corpus incorporating 7 different background conditions provides a benchmark in which to evaluate the use of aCMLLR transforms. A relative reduction of 40.5% in Word Error Rate (WER) was achieved by the combined use of aCMLLR and MLLR in cascade. Finally, this selection of techniques was applied in the transcription of multi-genre media broadcasts, where the use of aNAT training, aCMLLR transforms and MLLR transforms provided a relative improvement of 2–3%.</dc:description><openaccess>1</openaccess><openaccessArticle>true</openaccessArticle><openaccessType>Full</openaccessType><openArchiveArticle>false</openArchiveArticle><openaccessSponsorName>Engineering and Physical Sciences Research Council</openaccessSponsorName><openaccessSponsorType>FundingBody</openaccessSponsorType><openaccessUserLicense>http://creativecommons.org/licenses/by/4.0/</openaccessUserLicense><dcterms:subject>Speech recognition</dcterms:subject><dcterms:subject>Acoustic adaptation</dcterms:subject><dcterms:subject>Factorisation</dcterms:subject><dcterms:subject>Dynamic background</dcterms:subject><dcterms:subject>Media transcription</dcterms:subject><link rel="self" href="http://api.elsevier.com/content/article/pii/S0885230816300043"/><link rel="scidir" href="http://www.sciencedirect.com/science/article/pii/S0885230816300043"/></coredata><objects><object ref="ycsla784-fig-0001" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="133" size="12954">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-ycsla784-fig-0001.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla784-fig-0002" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="101" size="15099">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-ycsla784-fig-0002.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla784-fig-0003" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="208" height="163" size="22070">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-ycsla784-fig-0003.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla784-fig-0001" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="471" height="286" size="57539">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-ycsla784-fig-0001.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla784-fig-0002" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="565" height="260" size="64401">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-ycsla784-fig-0002.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla784-fig-0003" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="565" height="444" size="95092">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-ycsla784-fig-0003.jpg?httpAccept=%2A%2F%2A</object><object ref="si1" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="96" height="29" size="517">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-si1.gif?httpAccept=%2A%2F%2A</object><object ref="si14" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="37" height="27" size="322">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-si14.gif?httpAccept=%2A%2F%2A</object><object ref="si15" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="37" height="27" size="331">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-si15.gif?httpAccept=%2A%2F%2A</object><object ref="si2" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="227" height="31" size="1300">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-si2.gif?httpAccept=%2A%2F%2A</object><object ref="si3" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="181" height="35" size="1199">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-si3.gif?httpAccept=%2A%2F%2A</object><object ref="si4" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="227" height="31" size="1166">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-si4.gif?httpAccept=%2A%2F%2A</object><object ref="si5" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="277" height="40" size="1751">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-si5.gif?httpAccept=%2A%2F%2A</object><object ref="si6" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="154" height="33" size="897">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-si6.gif?httpAccept=%2A%2F%2A</object><object ref="si7" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="154" height="31" size="891">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-si7.gif?httpAccept=%2A%2F%2A</object><object ref="si8" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="190" height="33" size="970">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-si8.gif?httpAccept=%2A%2F%2A</object><object ref="si9" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="44" height="31" size="406">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816300043-si9.gif?httpAccept=%2A%2F%2A</object></objects><scopus-id>84978396807</scopus-id><scopus-eid>2-s2.0-84978396807</scopus-eid><link rel="abstract" href="http://api.elsevier.com/content/abstract/scopus_id/84978396807"/><originalText><xocs:doc xmlns:xoe="http://www.elsevier.com/xml/xoe/dtd" xsi:schemaLocation="http://www.elsevier.com/xml/xocs/dtd http://be-prod3a/schema/dtds/document/fulltext/xcr/xocs-article.xsd"><xocs:meta><xocs:content-family>serial</xocs:content-family><xocs:content-type>JL</xocs:content-type><xocs:cid>272453</xocs:cid><xocs:ssids><xocs:ssid type="alllist">291210</xocs:ssid><xocs:ssid type="subj">291718</xocs:ssid><xocs:ssid type="subj">291723</xocs:ssid><xocs:ssid type="subj">291743</xocs:ssid><xocs:ssid type="subj">291782</xocs:ssid><xocs:ssid type="subj">291874</xocs:ssid><xocs:ssid type="content">31</xocs:ssid><xocs:ssid type="oa">90</xocs:ssid></xocs:ssids><xocs:srctitle>Computer Speech &amp; Language</xocs:srctitle><xocs:normalized-srctitle>COMPUTERSPEECHLANGUAGE</xocs:normalized-srctitle><xocs:orig-load-date yyyymmdd="20160704">2016-07-04</xocs:orig-load-date><xocs:available-online-date yyyymmdd="20160704">2016-07-04</xocs:available-online-date><xocs:vor-load-date yyyymmdd="20160718">2016-07-18</xocs:vor-load-date><xocs:vor-available-online-date yyyymmdd="20160718">2016-07-18</xocs:vor-available-online-date><xocs:ew-transaction-id>2016-08-09T16:09:23</xocs:ew-transaction-id><xocs:eid>1-s2.0-S0885230816300043</xocs:eid><xocs:pii-formatted>S0885-2308(16)30004-3</xocs:pii-formatted><xocs:pii-unformatted>S0885230816300043</xocs:pii-unformatted><xocs:doi>10.1016/j.csl.2016.06.008</xocs:doi><xocs:item-stage>S250</xocs:item-stage><xocs:item-version-number>S250.1</xocs:item-version-number><xocs:item-weight>FULL-TEXT</xocs:item-weight><xocs:hub-eid>1-s2.0-S0885230816X00046</xocs:hub-eid><xocs:timestamp yyyymmdd="20160809">2016-08-09T11:51:28.34876-04:00</xocs:timestamp><xocs:dco>0</xocs:dco><xocs:tomb>0</xocs:tomb><xocs:date-search-begin>20170101</xocs:date-search-begin><xocs:date-search-end>20170131</xocs:date-search-end><xocs:year-nav>2017</xocs:year-nav><xocs:indexeddate epoch="1467645841">2016-07-04T15:24:01.849338Z</xocs:indexeddate><xocs:articleinfo>articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor highlightsabst primabst ref</xocs:articleinfo><xocs:issns><xocs:issn-primary-formatted>0885-2308</xocs:issn-primary-formatted><xocs:issn-primary-unformatted>08852308</xocs:issn-primary-unformatted></xocs:issns><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>EPSRC</xocs:funding-body-id><xocs:crossmark is-crossmark="1">true</xocs:crossmark><xocs:vol-first>41</xocs:vol-first><xocs:volume-list><xocs:volume>41</xocs:volume></xocs:volume-list><xocs:suppl>C</xocs:suppl><xocs:vol-iss-suppl-text>Volume 41</xocs:vol-iss-suppl-text><xocs:sort-order>10</xocs:sort-order><xocs:first-fp>180</xocs:first-fp><xocs:last-lp>194</xocs:last-lp><xocs:pages><xocs:first-page>180</xocs:first-page><xocs:last-page>194</xocs:last-page></xocs:pages><xocs:cover-date-orig><xocs:start-date>201701</xocs:start-date></xocs:cover-date-orig><xocs:cover-date-text>January 2017</xocs:cover-date-text><xocs:cover-date-start>2017-01-01</xocs:cover-date-start><xocs:cover-date-end>2017-01-31</xocs:cover-date-end><xocs:cover-date-year>2017</xocs:cover-date-year><xocs:document-type>article</xocs:document-type><xocs:document-subtype>fla</xocs:document-subtype><xocs:copyright-line>© 2016 The Authors. Published by Elsevier Ltd.</xocs:copyright-line><xocs:normalized-article-title>ACOUSTICADAPTATIONDYNAMICBACKGROUNDCONDITIONSASYNCHRONOUSTRANSFORMATIONS</xocs:normalized-article-title><xocs:normalized-first-auth-surname>SAZ</xocs:normalized-first-auth-surname><xocs:normalized-first-auth-initial>O</xocs:normalized-first-auth-initial><xocs:item-toc><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>1</xocs:item-toc-label><xocs:item-toc-section-title>Introduction</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2</xocs:item-toc-label><xocs:item-toc-section-title>Asynchronous background adaptation with feature transforms</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.1</xocs:item-toc-label><xocs:item-toc-section-title>Decoding with aCMLLR transforms</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.2</xocs:item-toc-label><xocs:item-toc-section-title>Training of aCMLLR transforms</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3</xocs:item-toc-label><xocs:item-toc-section-title>Extensions to the aCMLLR background adaptation</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.1</xocs:item-toc-label><xocs:item-toc-section-title>Asynchronous Noise Adaptive Training</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.2</xocs:item-toc-label><xocs:item-toc-section-title>Factorisation of asynchronous background and speaker</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4</xocs:item-toc-label><xocs:item-toc-section-title>Benchmark results: WSJCAM0</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.1</xocs:item-toc-label><xocs:item-toc-section-title>Adaptation experiments</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.2</xocs:item-toc-label><xocs:item-toc-section-title>Factorisation experiments</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.3</xocs:item-toc-label><xocs:item-toc-section-title>Computational complexity</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>5</xocs:item-toc-label><xocs:item-toc-section-title>Transcription of multi-genre broadcasts</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>5.1</xocs:item-toc-label><xocs:item-toc-section-title>Results</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>6</xocs:item-toc-label><xocs:item-toc-section-title>Conclusions</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>7</xocs:item-toc-label><xocs:item-toc-section-title>Data access management</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:acknowledgment"><xocs:item-toc-section-title>Acknowledgement</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:bibliography"><xocs:item-toc-section-title>References</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc><xocs:references><xocs:ref-info refid="sr0010"><xocs:ref-normalized-surname>ANASTASAKOS</xocs:ref-normalized-surname><xocs:ref-pub-year>1996</xocs:ref-pub-year><xocs:ref-first-fp>1137</xocs:ref-first-fp><xocs:ref-last-lp>1140</xocs:ref-last-lp><xocs:ref-normalized-initial>T</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS4THINTERNATIONALCONFERENCESPOKENLANGUAGEPROCESSINGICSLP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>ACOMPACTMODELFORSPEAKERADAPTIVETRAINING</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0015"><xocs:ref-normalized-surname>ASTUDILLO</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-first-fp>60</xocs:ref-first-fp><xocs:ref-last-lp>67</xocs:ref-last-lp><xocs:ref-normalized-initial>R</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2009NONLINEARSPEECHPROCESSINGNOLISPWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>SPEECHENHANCEMENTFORAUTOMATICSPEECHRECOGNITIONUSINGCOMPLEXGAUSSIANMIXTUREPRIORSFORNOISESPEECH</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0020"><xocs:ref-normalized-surname>BELL</xocs:ref-normalized-surname><xocs:ref-pub-year>2015</xocs:ref-pub-year><xocs:ref-first-fp>687</xocs:ref-first-fp><xocs:ref-last-lp>693</xocs:ref-last-lp><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2015IEEEAUTOMATICSPEECHRECOGNITIONUNDERSTANDINGWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>MGBCHALLENGEEVALUATINGMULTIGENREBROADCASTMEDIARECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0025"><xocs:ref-normalized-surname>BUERA</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-first-fp>1098</xocs:ref-first-fp><xocs:ref-last-lp>1113</xocs:ref-last-lp><xocs:ref-normalized-initial>L</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0030"><xocs:ref-normalized-surname>CHEN</xocs:ref-normalized-surname><xocs:ref-pub-year>1998</xocs:ref-pub-year><xocs:ref-first-fp>645</xocs:ref-first-fp><xocs:ref-last-lp>648</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS1998IEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>CLUSTERINGVIABAYESIANINFORMATIONCRITERIONAPPLICATIONSINSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0035"><xocs:ref-normalized-surname>CIERI</xocs:ref-normalized-surname><xocs:ref-pub-year>1999</xocs:ref-pub-year><xocs:ref-normalized-initial>C</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS1999DARPABROADCASTNEWSWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>TDT2TEXTSPEECHCORPUS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0040"><xocs:ref-normalized-surname>COOKE</xocs:ref-normalized-surname><xocs:ref-pub-year>2001</xocs:ref-pub-year><xocs:ref-first-fp>267</xocs:ref-first-fp><xocs:ref-last-lp>285</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0045"><xocs:ref-normalized-surname>DROPPO</xocs:ref-normalized-surname><xocs:ref-pub-year>2001</xocs:ref-pub-year><xocs:ref-first-fp>217</xocs:ref-first-fp><xocs:ref-last-lp>220</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS7THEUROPEANCONFERENCESPEECHCOMMUNICATIONTECHNOLOGYEUROSPEECH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>EVALUATIONSPLICEALGORITHMAURORA2DATABASE</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0050"><xocs:ref-normalized-surname>EPHRAIM</xocs:ref-normalized-surname><xocs:ref-pub-year>1984</xocs:ref-pub-year><xocs:ref-first-fp>1109</xocs:ref-first-fp><xocs:ref-last-lp>1121</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0055"><xocs:ref-normalized-surname>EPHRAIM</xocs:ref-normalized-surname><xocs:ref-pub-year>1985</xocs:ref-pub-year><xocs:ref-first-fp>443</xocs:ref-first-fp><xocs:ref-last-lp>445</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0060"><xocs:ref-normalized-surname>FISCUS</xocs:ref-normalized-surname><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0065"><xocs:ref-normalized-surname>GALES</xocs:ref-normalized-surname><xocs:ref-pub-year>1998</xocs:ref-pub-year><xocs:ref-first-fp>75</xocs:ref-first-fp><xocs:ref-last-lp>98</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0070"><xocs:ref-normalized-surname>GALES</xocs:ref-normalized-surname><xocs:ref-pub-year>2001</xocs:ref-pub-year><xocs:ref-first-fp>77</xocs:ref-first-fp><xocs:ref-last-lp>80</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2001IEEEAUTOMATICSPEECHRECOGNITIONUNDERSTANDINGASRUWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>ACOUSTICFACTORISATION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0075"><xocs:ref-normalized-surname>GALES</xocs:ref-normalized-surname><xocs:ref-pub-year>1996</xocs:ref-pub-year><xocs:ref-first-fp>249</xocs:ref-first-fp><xocs:ref-last-lp>264</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0080"><xocs:ref-normalized-surname>GALES</xocs:ref-normalized-surname><xocs:ref-pub-year>1996</xocs:ref-pub-year><xocs:ref-first-fp>352</xocs:ref-first-fp><xocs:ref-last-lp>359</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0085"><xocs:ref-normalized-surname>GALLIANO</xocs:ref-normalized-surname><xocs:ref-pub-year>2006</xocs:ref-pub-year><xocs:ref-first-fp>139</xocs:ref-first-fp><xocs:ref-last-lp>142</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS5THINTERNATIONALCONFERENCELANGUAGERESOURCESEVALUATIONLREC</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>CORPUSDESCRIPTIONESTEREVALUATIONCAMPAIGNFORRICHTRANSCRIPTIONFRENCHBROADCASTNEWS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0090"><xocs:ref-normalized-surname>GAUVAIN</xocs:ref-normalized-surname><xocs:ref-pub-year>1994</xocs:ref-pub-year><xocs:ref-first-fp>291</xocs:ref-first-fp><xocs:ref-last-lp>298</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0095"><xocs:ref-normalized-surname>GREZL</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>4729</xocs:ref-first-fp><xocs:ref-last-lp>4732</xocs:ref-last-lp><xocs:ref-normalized-initial>F</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2008IEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>OPTIMIZINGBOTTLENECKFEATURESFORLVCSR</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0100"><xocs:ref-normalized-surname>GREZL</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-first-fp>2947</xocs:ref-first-fp><xocs:ref-last-lp>2950</xocs:ref-last-lp><xocs:ref-normalized-initial>F</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS10THANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATIONINTERSPEECH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>INVESTIGATIONBOTTLENECKFEATURESFORMEETINGSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0105"><xocs:ref-normalized-surname>HAIN</xocs:ref-normalized-surname><xocs:ref-pub-year>2005</xocs:ref-pub-year><xocs:ref-first-fp>171</xocs:ref-first-fp><xocs:ref-last-lp>188</xocs:ref-last-lp><xocs:ref-normalized-initial>T</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0110"><xocs:ref-normalized-surname>HERMANSKY</xocs:ref-normalized-surname><xocs:ref-pub-year>1990</xocs:ref-pub-year><xocs:ref-first-fp>1738</xocs:ref-first-fp><xocs:ref-last-lp>1752</xocs:ref-last-lp><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0115"><xocs:ref-normalized-surname>HIRSCH</xocs:ref-normalized-surname><xocs:ref-pub-year>2000</xocs:ref-pub-year><xocs:ref-first-fp>29</xocs:ref-first-fp><xocs:ref-last-lp>32</xocs:ref-last-lp><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS6THINTERNATIONALCONFERENCESPOKENLANGUAGEPROCESSINGICSLP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>AURORAEXPERIMENTALFRAMEWORKFORPERFORMANCEEVALUATIONSPEECHRECOGNITIONSYSTEMSUNDERNOISYCONDITIONS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0120"><xocs:ref-normalized-surname>KALINLI</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>1889</xocs:ref-first-fp><xocs:ref-last-lp>1901</xocs:ref-last-lp><xocs:ref-normalized-initial>O</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0125"><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-normalized-srctitle>PROCEEDINGSMEDIAEVAL2013MULTIMEDIABENCHMARKWORKSHOP</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0130"><xocs:ref-normalized-surname>LEE</xocs:ref-normalized-surname><xocs:ref-pub-year>1998</xocs:ref-pub-year><xocs:ref-first-fp>49</xocs:ref-first-fp><xocs:ref-last-lp>60</xocs:ref-last-lp><xocs:ref-normalized-initial>L</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0135"><xocs:ref-normalized-surname>LI</xocs:ref-normalized-surname><xocs:ref-pub-year>2014</xocs:ref-pub-year><xocs:ref-first-fp>745</xocs:ref-first-fp><xocs:ref-last-lp>777</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0140"><xocs:ref-normalized-surname>LIAO</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-first-fp>389</xocs:ref-first-fp><xocs:ref-last-lp>392</xocs:ref-last-lp><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2007IEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>ADAPTIVETRAININGJOINTUNCERTAINTYDECODINGFORROBUSTRECOGNITIONNOISYDATA</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0145"><xocs:ref-normalized-surname>LIU</xocs:ref-normalized-surname><xocs:ref-pub-year>2014</xocs:ref-pub-year><xocs:ref-first-fp>5579</xocs:ref-first-fp><xocs:ref-last-lp>5583</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2014INTERNATIONALCONFERENCEACOUSTICSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>USINGNEURALNETWORKFRONTENDSFARFIELDMULTIPLEMICROPHONESBASEDSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0150"><xocs:ref-normalized-surname>LONG</xocs:ref-normalized-surname><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-first-fp>2187</xocs:ref-first-fp><xocs:ref-last-lp>2191</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS14THANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATIONINTERSPEECH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>IMPROVINGLIGHTLYSUPERVISEDTRAININGFORBROADCASTTRANSCRIPTIONS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0155"><xocs:ref-normalized-surname>MIGUEL</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>578</xocs:ref-first-fp><xocs:ref-last-lp>593</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0160"><xocs:ref-normalized-surname>MILNER</xocs:ref-normalized-surname><xocs:ref-pub-year>2015</xocs:ref-pub-year><xocs:ref-first-fp>632</xocs:ref-first-fp><xocs:ref-last-lp>638</xocs:ref-last-lp><xocs:ref-normalized-initial>R</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2015IEEEAUTOMATICSPEECHRECOGNITIONUNDERSTANDINGASRUWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>2015SHEFFIELDSYSTEMFORLONGITUDINALDIARISATIONBROADCASTMEDIA</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0165"><xocs:ref-normalized-surname>MORENO</xocs:ref-normalized-surname><xocs:ref-pub-year>1996</xocs:ref-pub-year><xocs:ref-first-fp>733</xocs:ref-first-fp><xocs:ref-last-lp>736</xocs:ref-last-lp><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS1996IEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>AVECTORTAYLORSERIESAPPROACHFORENVIRONMENTINDEPENDENTSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0170"><xocs:ref-normalized-surname>NOVAK</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS10THINTERNATIONALWORKSHOPFINITESTATEMETHODSNATURALLANGUAGEPROCESSING</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>WSFTBASEDGRAPHEMETOPHONEMECONVERSIONOPENSOURCETOOLSFORALIGNMENTMODELBUILDINGDECODING</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0175"><xocs:ref-normalized-surname>PALIWAL</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>1</xocs:ref-first-fp><xocs:ref-last-lp>5</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS4THINTERNATIONALCONFERENCESIGNALPROCESSINGCOMMUNICATIONSYSTEMSICSPCS</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>COMPARATIVEEVALUATIONSPEECHENHANCEMENTMETHODSFORROBUSTAUTOMATICSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0180"><xocs:ref-normalized-surname>PALLETT</xocs:ref-normalized-surname><xocs:ref-pub-year>1996</xocs:ref-pub-year><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS1996DARPASPEECHRECOGNITIONWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>1995HUB4DRYRUNBROADCASTMATERIALSBENCHMARKTEST</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0185"><xocs:ref-normalized-surname>PARIHAR</xocs:ref-normalized-surname><xocs:ref-pub-year>2004</xocs:ref-pub-year><xocs:ref-first-fp>553</xocs:ref-first-fp><xocs:ref-last-lp>556</xocs:ref-last-lp><xocs:ref-normalized-initial>N</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS12THEUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>PERFORMANCEANALYSISAURORALARGEVOCABULARYBASELINESYSTEM</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0190"><xocs:ref-normalized-surname>PAUL</xocs:ref-normalized-surname><xocs:ref-pub-year>1992</xocs:ref-pub-year><xocs:ref-first-fp>357</xocs:ref-first-fp><xocs:ref-last-lp>362</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS5THDARPASPEECHNATURALLANGUAGEWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>DESIGNFORWALLSTREETJOURNALBASEDCSRCORPUS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0195"><xocs:ref-normalized-surname>POVEY</xocs:ref-normalized-surname><xocs:ref-pub-year>2002</xocs:ref-pub-year><xocs:ref-first-fp>105</xocs:ref-first-fp><xocs:ref-last-lp>108</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2002IEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>MINIMUMPHONEERRORISMOOTHINGFORIMPROVEDDISCRIMINATIVETRAINING</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0200"><xocs:ref-normalized-surname>POVEY</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>404</xocs:ref-first-fp><xocs:ref-last-lp>439</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0205"><xocs:ref-normalized-surname>POVEY</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2011IEEEAUTOMATICSPEECHRECOGNITIONUNDERSTANDINGASRUWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>KALDISPEECHRECOGNITIONTOOLKIT</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0210"><xocs:ref-normalized-surname>RAJ</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>717</xocs:ref-first-fp><xocs:ref-last-lp>720</xocs:ref-last-lp><xocs:ref-normalized-initial>B</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS11THANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATIONINTERSPEECH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>NONNEGATIVEMATRIXFACTORIZATIONBASEDCOMPENSATIONMUSICFORAUTOMATICSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0215"><xocs:ref-normalized-surname>RICHMOND</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>1974</xocs:ref-first-fp><xocs:ref-last-lp>1977</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS11THANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATIONINTERSPEECH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>GENERATINGCOMBILEXPRONUNCIATIONSVIAMORPHOLOGICALANALYSIS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0220"><xocs:ref-normalized-surname>ROBINSON</xocs:ref-normalized-surname><xocs:ref-pub-year>1995</xocs:ref-pub-year><xocs:ref-first-fp>81</xocs:ref-first-fp><xocs:ref-last-lp>84</xocs:ref-last-lp><xocs:ref-normalized-initial>T</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS1995INTERNATIONALCONFERENCEACOUSTICSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>WSJCAM0ABRITISHENGLISHSPEECHCORPUSFORLARGEVOCABULARYCONTINUOUSSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0225"><xocs:ref-normalized-surname>SAZ</xocs:ref-normalized-surname><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-first-fp>1238</xocs:ref-first-fp><xocs:ref-last-lp>1242</xocs:ref-last-lp><xocs:ref-normalized-initial>O</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS14THANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATIONINTERSPEECH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>ASYNCHRONOUSFACTORISATIONSPEAKERBACKGROUNDFEATURETRANSFORMSINSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0230"><xocs:ref-normalized-surname>SAZ</xocs:ref-normalized-surname><xocs:ref-pub-year>2014</xocs:ref-pub-year><xocs:ref-first-fp>6314</xocs:ref-first-fp><xocs:ref-last-lp>6318</xocs:ref-last-lp><xocs:ref-normalized-initial>O</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2014INTERNATIONALCONFERENCEACOUSTICSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>USINGCONTEXTUALINFORMATIONINJOINTFACTOREIGENSPACEMLLRFORSPEECHRECOGNITIONINDIVERSESCENARIOS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0235"><xocs:ref-normalized-surname>SAZ</xocs:ref-normalized-surname><xocs:ref-pub-year>2014</xocs:ref-pub-year><xocs:ref-first-fp>118</xocs:ref-first-fp><xocs:ref-last-lp>123</xocs:ref-last-lp><xocs:ref-normalized-initial>O</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2014IEEESPOKENLANGUAGETECHNOLOGYSLTWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>BACKGROUNDTRACKINGACOUSTICFEATURESFORGENREIDENTIFICATIONBROADCASTSHOWS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0240"><xocs:ref-normalized-surname>SAZ</xocs:ref-normalized-surname><xocs:ref-pub-year>2015</xocs:ref-pub-year><xocs:ref-first-fp>624</xocs:ref-first-fp><xocs:ref-last-lp>631</xocs:ref-last-lp><xocs:ref-normalized-initial>O</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2015IEEEAUTOMATICSPEECHRECOGNITIONUNDERSTANDINGASRUWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>2015SHEFFIELDSYSTEMFORTRANSCRIPTIONMULTIGENREBROADCASTMEDIA</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0245"><xocs:ref-normalized-surname>SCHULLER</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>4562</xocs:ref-first-fp><xocs:ref-last-lp>4565</xocs:ref-last-lp><xocs:ref-normalized-initial>B</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2010IEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>NONNEGATIVEMATRIXFACTORIZATIONNOISEROBUSTFEATUREEXTRACTORFORSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0250"><xocs:ref-normalized-surname>SELTZER</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>1097</xocs:ref-first-fp><xocs:ref-last-lp>1100</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS12THANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATIONINTERSPEECH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>SEPARATINGSPEAKERENVIRONMENTALVARIABILITYUSINGFACTOREDTRANSFORMS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0255"><xocs:ref-normalized-surname>SELTZER</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>1792</xocs:ref-first-fp><xocs:ref-last-lp>1795</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS13THANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATIONINTERSPEECH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>FACTOREDADAPTATIONUSINGACOMBINATIONFEATURESPACEMODELSPACETRANSFORMS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0260"><xocs:ref-normalized-surname>SEO</xocs:ref-normalized-surname><xocs:ref-pub-year>2014</xocs:ref-pub-year><xocs:ref-first-fp>3275</xocs:ref-first-fp><xocs:ref-last-lp>3279</xocs:ref-last-lp><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2014INTERNATIONALCONFERENCEACOUSTICSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>FACTOREDADAPTATIONSPEAKERENVIRONMENTUSINGORTHOGONALSUBSPACETRANSFORMS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0265"><xocs:ref-normalized-surname>STOLCKE</xocs:ref-normalized-surname><xocs:ref-pub-year>2002</xocs:ref-pub-year><xocs:ref-first-fp>901</xocs:ref-first-fp><xocs:ref-last-lp>904</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS7THINTERNATIONALCONFERENCESPOKENLANGUAGEPROCESSINGICSLP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>SRILMEXTENSIBLELANGUAGEMODELINGTOOLKIT</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0270"><xocs:ref-normalized-surname>VARGA</xocs:ref-normalized-surname><xocs:ref-pub-year>1993</xocs:ref-pub-year><xocs:ref-first-fp>247</xocs:ref-first-fp><xocs:ref-last-lp>251</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0275"><xocs:ref-normalized-surname>VARGA</xocs:ref-normalized-surname><xocs:ref-pub-year>1990</xocs:ref-pub-year><xocs:ref-first-fp>845</xocs:ref-first-fp><xocs:ref-last-lp>848</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS1990INTERNATIONALCONFERENCEACOUSTICSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>HIDDENMARKOVMODELDECOMPOSITIONSPEECHNOISE</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0280"><xocs:ref-normalized-surname>VESELY</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>2934</xocs:ref-first-fp><xocs:ref-last-lp>2937</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS11THANNUALCONFERENCEINTERNATIONALSPEECHCOMMUNICATIONASSOCIATIONINTERSPEECH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>PARALLELTRAININGNEURALNETWORKSFORSPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0285"><xocs:ref-normalized-surname>WANG</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>4584</xocs:ref-first-fp><xocs:ref-last-lp>4587</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2011INTERNATIONALCONFERENCEACOUSTICSPEECHSIGNALPROCESSINGICASSP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>SPEAKERNOISEFACTORISATIONAURORA4TASK</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0290"><xocs:ref-normalized-surname>YIN</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-first-fp>1999</xocs:ref-first-fp><xocs:ref-last-lp>2010</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0295"><xocs:ref-normalized-surname>YOUNG</xocs:ref-normalized-surname><xocs:ref-pub-year>2006</xocs:ref-pub-year><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>HTKBOOKVERSION34</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0300"><xocs:ref-normalized-surname>ZELENAK</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>1</xocs:ref-first-fp><xocs:ref-last-lp>9</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0305"><xocs:ref-normalized-surname>ZHANG</xocs:ref-normalized-surname><xocs:ref-pub-year>2014</xocs:ref-pub-year><xocs:ref-first-fp>141</xocs:ref-first-fp><xocs:ref-last-lp>146</xocs:ref-last-lp><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2014IEEESPOKENLANGUAGETECHNOLOGYSLTWORKSHOP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>SEMISUPERVISEDDNNTRAININGINMEETINGRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info></xocs:references><xocs:refkeys><xocs:refkey3>SAZX2017X180</xocs:refkey3><xocs:refkey4lp>SAZX2017X180X194</xocs:refkey4lp><xocs:refkey4ai>SAZX2017X180XO</xocs:refkey4ai><xocs:refkey5>SAZX2017X180X194XO</xocs:refkey5></xocs:refkeys><xocs:open-access><xocs:oa-article-status is-open-access="1" is-open-archive="0">Full</xocs:oa-article-status><xocs:oa-access-effective-date>2016-07-02T17:50:50Z</xocs:oa-access-effective-date><xocs:oa-sponsor><xocs:oa-sponsor-type>FundingBody</xocs:oa-sponsor-type><xocs:oa-sponsor-name>Engineering and Physical Sciences Research Council</xocs:oa-sponsor-name></xocs:oa-sponsor><xocs:oa-user-license>http://creativecommons.org/licenses/by/4.0/</xocs:oa-user-license></xocs:open-access><xocs:self-archiving><xocs:sa-start-date>2018-07-18T00:00:00Z</xocs:sa-start-date><xocs:sa-embargo-status>UnderEmbargo</xocs:sa-embargo-status><xocs:sa-user-license>http://creativecommons.org/licenses/by-nc-nd/4.0/</xocs:sa-user-license></xocs:self-archiving><xocs:copyright-info><xocs:cp-license-lines><xocs:cp-license-line lang="en">This is an open access article under the CC BY license.</xocs:cp-license-line></xocs:cp-license-lines><xocs:cp-notices><xocs:cp-notice lang="en">© 2016 The Authors. Published by Elsevier Ltd.</xocs:cp-notice></xocs:cp-notices></xocs:copyright-info><xocs:attachment-metadata-doc><xocs:attachment-set-type>item</xocs:attachment-set-type><xocs:pii-formatted>S0885-2308(16)30004-3</xocs:pii-formatted><xocs:pii-unformatted>S0885230816300043</xocs:pii-unformatted><xocs:eid>1-s2.0-S0885230816300043</xocs:eid><xocs:doi>10.1016/j.csl.2016.06.008</xocs:doi><xocs:cid>272453</xocs:cid><xocs:timestamp>2016-08-09T11:51:28.34876-04:00</xocs:timestamp><xocs:cover-date-start>2017-01-01</xocs:cover-date-start><xocs:cover-date-end>2017-01-31</xocs:cover-date-end><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>EPSRC</xocs:funding-body-id><xocs:attachments><xocs:web-pdf><xocs:attachment-eid>1-s2.0-S0885230816300043-main.pdf</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/MAIN/application/pdf/0ea5d61ec90ada984ace56f1fc246d5e/main.pdf</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/MAIN/application/pdf/0ea5d61ec90ada984ace56f1fc246d5e/main.pdf</xocs:ucs-locator><xocs:filename>main.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:pdf-optimized>true</xocs:pdf-optimized><xocs:filesize>785932</xocs:filesize><xocs:web-pdf-purpose>MAIN</xocs:web-pdf-purpose><xocs:web-pdf-page-count>15</xocs:web-pdf-page-count><xocs:web-pdf-images><xocs:web-pdf-image><xocs:attachment-eid>1-s2.0-S0885230816300043-main_1.png</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/PREVIEW/image/png/4e42613031e609d98c1e55c3e248d907/main_1.png</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/PREVIEW/image/png/4e42613031e609d98c1e55c3e248d907/main_1.png</xocs:ucs-locator><xocs:filename>main_1.png</xocs:filename><xocs:extension>png</xocs:extension><xocs:filesize>47230</xocs:filesize><xocs:pixel-height>849</xocs:pixel-height><xocs:pixel-width>656</xocs:pixel-width><xocs:attachment-type>IMAGE-WEB-PDF</xocs:attachment-type><xocs:pdf-page-num>1</xocs:pdf-page-num></xocs:web-pdf-image></xocs:web-pdf-images></xocs:web-pdf><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-ycsla784-fig-0001.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/ycsla784-fig-0001/THUMBNAIL/image/gif/61701dce459424434a9d33589b20553f/ycsla784-fig-0001.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/ycsla784-fig-0001/THUMBNAIL/image/gif/61701dce459424434a9d33589b20553f/ycsla784-fig-0001.sml</xocs:ucs-locator><xocs:file-basename>ycsla784-fig-0001</xocs:file-basename><xocs:filename>ycsla784-fig-0001.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>12954</xocs:filesize><xocs:pixel-height>133</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-ycsla784-fig-0002.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/ycsla784-fig-0002/THUMBNAIL/image/gif/b11d1c1024f2d58ba6cba00e2fe767dc/ycsla784-fig-0002.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/ycsla784-fig-0002/THUMBNAIL/image/gif/b11d1c1024f2d58ba6cba00e2fe767dc/ycsla784-fig-0002.sml</xocs:ucs-locator><xocs:file-basename>ycsla784-fig-0002</xocs:file-basename><xocs:filename>ycsla784-fig-0002.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>15099</xocs:filesize><xocs:pixel-height>101</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-ycsla784-fig-0003.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/ycsla784-fig-0003/THUMBNAIL/image/gif/037768bb1d26e63cf19c6b93f2ca550a/ycsla784-fig-0003.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/ycsla784-fig-0003/THUMBNAIL/image/gif/037768bb1d26e63cf19c6b93f2ca550a/ycsla784-fig-0003.sml</xocs:ucs-locator><xocs:file-basename>ycsla784-fig-0003</xocs:file-basename><xocs:filename>ycsla784-fig-0003.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>22070</xocs:filesize><xocs:pixel-height>163</xocs:pixel-height><xocs:pixel-width>208</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-ycsla784-fig-0001.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/ycsla784-fig-0001/DOWNSAMPLED/image/jpeg/3c1a8e2662f367285412b2d49d4854ec/ycsla784-fig-0001.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/ycsla784-fig-0001/DOWNSAMPLED/image/jpeg/3c1a8e2662f367285412b2d49d4854ec/ycsla784-fig-0001.jpg</xocs:ucs-locator><xocs:file-basename>ycsla784-fig-0001</xocs:file-basename><xocs:filename>ycsla784-fig-0001.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>57539</xocs:filesize><xocs:pixel-height>286</xocs:pixel-height><xocs:pixel-width>471</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-ycsla784-fig-0002.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/ycsla784-fig-0002/DOWNSAMPLED/image/jpeg/3cb3fc6123987a033d8af661f69e95b8/ycsla784-fig-0002.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/ycsla784-fig-0002/DOWNSAMPLED/image/jpeg/3cb3fc6123987a033d8af661f69e95b8/ycsla784-fig-0002.jpg</xocs:ucs-locator><xocs:file-basename>ycsla784-fig-0002</xocs:file-basename><xocs:filename>ycsla784-fig-0002.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>64401</xocs:filesize><xocs:pixel-height>260</xocs:pixel-height><xocs:pixel-width>565</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-ycsla784-fig-0003.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/ycsla784-fig-0003/DOWNSAMPLED/image/jpeg/607a22fbc32c6575245ddc422e31c0fe/ycsla784-fig-0003.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/ycsla784-fig-0003/DOWNSAMPLED/image/jpeg/607a22fbc32c6575245ddc422e31c0fe/ycsla784-fig-0003.jpg</xocs:ucs-locator><xocs:file-basename>ycsla784-fig-0003</xocs:file-basename><xocs:filename>ycsla784-fig-0003.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>95092</xocs:filesize><xocs:pixel-height>444</xocs:pixel-height><xocs:pixel-width>565</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-si1.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/STRIPIN/image/gif/029f9c755fe7b7c476275ae4975f6095/si1.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/STRIPIN/image/gif/029f9c755fe7b7c476275ae4975f6095/si1.gif</xocs:ucs-locator><xocs:file-basename>si1</xocs:file-basename><xocs:filename>si1.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>517</xocs:filesize><xocs:pixel-height>29</xocs:pixel-height><xocs:pixel-width>96</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-si14.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/STRIPIN/image/gif/a7c97a073aedccc594b2e86e80989b1a/si14.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/STRIPIN/image/gif/a7c97a073aedccc594b2e86e80989b1a/si14.gif</xocs:ucs-locator><xocs:file-basename>si14</xocs:file-basename><xocs:filename>si14.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>322</xocs:filesize><xocs:pixel-height>27</xocs:pixel-height><xocs:pixel-width>37</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-si15.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/STRIPIN/image/gif/fd32894ea2aed3e9aed3ad5e75124f7a/si15.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/STRIPIN/image/gif/fd32894ea2aed3e9aed3ad5e75124f7a/si15.gif</xocs:ucs-locator><xocs:file-basename>si15</xocs:file-basename><xocs:filename>si15.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>331</xocs:filesize><xocs:pixel-height>27</xocs:pixel-height><xocs:pixel-width>37</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-si2.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/STRIPIN/image/gif/faee56c300517304916062c2cdb0f0ed/si2.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/STRIPIN/image/gif/faee56c300517304916062c2cdb0f0ed/si2.gif</xocs:ucs-locator><xocs:file-basename>si2</xocs:file-basename><xocs:filename>si2.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1300</xocs:filesize><xocs:pixel-height>31</xocs:pixel-height><xocs:pixel-width>227</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-si3.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/STRIPIN/image/gif/63cb53e3acaff54763e021f400207dfb/si3.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/STRIPIN/image/gif/63cb53e3acaff54763e021f400207dfb/si3.gif</xocs:ucs-locator><xocs:file-basename>si3</xocs:file-basename><xocs:filename>si3.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1199</xocs:filesize><xocs:pixel-height>35</xocs:pixel-height><xocs:pixel-width>181</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-si4.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/STRIPIN/image/gif/bf9ea82a56024918cbf787b7045125f6/si4.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/STRIPIN/image/gif/bf9ea82a56024918cbf787b7045125f6/si4.gif</xocs:ucs-locator><xocs:file-basename>si4</xocs:file-basename><xocs:filename>si4.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1166</xocs:filesize><xocs:pixel-height>31</xocs:pixel-height><xocs:pixel-width>227</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-si5.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/STRIPIN/image/gif/3e30082480a20bba0add68bf1baf8156/si5.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/STRIPIN/image/gif/3e30082480a20bba0add68bf1baf8156/si5.gif</xocs:ucs-locator><xocs:file-basename>si5</xocs:file-basename><xocs:filename>si5.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1751</xocs:filesize><xocs:pixel-height>40</xocs:pixel-height><xocs:pixel-width>277</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-si6.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/STRIPIN/image/gif/79c3d3c837c4296dbc1c0b78f633f5b2/si6.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/STRIPIN/image/gif/79c3d3c837c4296dbc1c0b78f633f5b2/si6.gif</xocs:ucs-locator><xocs:file-basename>si6</xocs:file-basename><xocs:filename>si6.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>897</xocs:filesize><xocs:pixel-height>33</xocs:pixel-height><xocs:pixel-width>154</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-si7.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/STRIPIN/image/gif/3a36971cd633278bba591af6bf8cff5d/si7.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/STRIPIN/image/gif/3a36971cd633278bba591af6bf8cff5d/si7.gif</xocs:ucs-locator><xocs:file-basename>si7</xocs:file-basename><xocs:filename>si7.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>891</xocs:filesize><xocs:pixel-height>31</xocs:pixel-height><xocs:pixel-width>154</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-si8.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/STRIPIN/image/gif/d5d3982c6373305b9ef0929b97a05460/si8.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/STRIPIN/image/gif/d5d3982c6373305b9ef0929b97a05460/si8.gif</xocs:ucs-locator><xocs:file-basename>si8</xocs:file-basename><xocs:filename>si8.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>970</xocs:filesize><xocs:pixel-height>33</xocs:pixel-height><xocs:pixel-width>190</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816300043-si9.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816300043/STRIPIN/image/gif/c3dac2f101f401e707e72998ef4b9bbf/si9.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816300043/STRIPIN/image/gif/c3dac2f101f401e707e72998ef4b9bbf/si9.gif</xocs:ucs-locator><xocs:file-basename>si9</xocs:file-basename><xocs:filename>si9.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>406</xocs:filesize><xocs:pixel-height>31</xocs:pixel-height><xocs:pixel-width>44</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment></xocs:attachments></xocs:attachment-metadata-doc></xocs:meta><xocs:serial-item><article xmlns="http://www.elsevier.com/xml/ja/dtd" docsubtype="fla" xml:lang="en" version="5.4"><item-info><jid>YCSLA</jid><aid>784</aid><ce:pii>S0885-2308(16)30004-3</ce:pii><ce:doi>10.1016/j.csl.2016.06.008</ce:doi><ce:copyright type="other" year="2016">The Authors</ce:copyright></item-info><ce:floats><ce:figure id="f0010"><ce:label>Fig. 1</ce:label><ce:caption id="ca0010"><ce:simple-para id="sp0020" view="all">Asynchronous decoding with two background transforms (<mml:math altimg="si14.gif" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:math> and <mml:math altimg="si15.gif" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math>). State self transitions have been removed for clarity.</ce:simple-para></ce:caption><ce:alt-text id="atte0040" role="short">Fig. 1</ce:alt-text><ce:link id="ln0055" locator="ycsla784-fig-0001" xlink:href="pii:S0885230816300043/ycsla784-fig-0001" xlink:type="simple"/></ce:figure><ce:figure id="f0015"><ce:label>Fig. 2</ce:label><ce:caption id="ca0035"><ce:simple-para id="sp0045" view="all">Results for different topologies in training and decoding of aCMLLR transforms (first letter indicates training topology, second letter indicates testing topology, P stands for <ce:italic>Phone synchronous</ce:italic>, and F stands for <ce:italic>Fully asynchronous</ce:italic> as defined in <ce:cross-ref id="crf0325" refid="s0020">Section 2.1</ce:cross-ref>).</ce:simple-para></ce:caption><ce:alt-text id="atte0085" role="short">Fig. 2</ce:alt-text><ce:link id="ln0090" locator="ycsla784-fig-0002" xlink:href="pii:S0885230816300043/ycsla784-fig-0002" xlink:type="simple"/></ce:figure><ce:figure id="f0020"><ce:label>Fig. 3</ce:label><ce:caption id="ca0055"><ce:simple-para id="sp0065" view="all">Frame-wise backgrounds selected in decoding for two 5K task files with <ce:italic>Phone synchronous</ce:italic> and <ce:italic>Fully asynchronous</ce:italic> HMMs with 7 possible backgrounds (indexes 0 to 6).</ce:simple-para></ce:caption><ce:alt-text id="atte0105" role="short">Fig. 3</ce:alt-text><ce:link id="ln0095" locator="ycsla784-fig-0003" xlink:href="pii:S0885230816300043/ycsla784-fig-0003" xlink:type="simple"/></ce:figure><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0010"><ce:label>Table 1</ce:label><ce:caption id="ca0015"><ce:simple-para id="sp0025" view="all">Distribution of segments in the <ce:italic>Diverse</ce:italic> data set.</ce:simple-para></ce:caption><ce:alt-text id="atte0065" role="short">Table 1</ce:alt-text><tgroup cols="8"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><colspec colname="col7" colnum="7"/><colspec colname="col8" colnum="8"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left">Clean</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col4" namest="col3" align="left">Music</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col8" namest="col5" align="left">Noisy</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Orchestral</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Popular</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Traffic</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Restaurant</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Applause</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Outdoors</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Train</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2,504</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1,238</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1,249</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">598</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">582</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">630</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">586</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Test</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">433</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">226</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">215</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">120</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">101</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">118</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">102</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0015"><ce:label>Table 2</ce:label><ce:caption id="ca0020"><ce:simple-para id="sp0030" view="all">Baseline recognition results (WER) for modified WSJCAM0.</ce:simple-para></ce:caption><ce:alt-text id="atte0070" role="short">Table 2</ce:alt-text><tgroup cols="5"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Train</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Test</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5K set</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">20K set</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Total</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Clean</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Clean</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">13.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">9.4%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Clean</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">27.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">39.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">33.1%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">14.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">25.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">20.3%</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0020"><ce:label>Table 3</ce:label><ce:caption id="ca0025"><ce:simple-para id="sp0035" view="all">Speaker-based MLLR adaptation results for modified WSJCAM0.</ce:simple-para></ce:caption><ce:alt-text id="atte0075" role="short">Table 3</ce:alt-text><tgroup cols="6"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Train</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Test</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5K set</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">20K set</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Total</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Rel. impr.</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Clean</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Clean</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">11.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">8.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">14.7%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Clean</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">18.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">30.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">24.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">25.7%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">13.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">22.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">17.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">13.3%</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0025"><ce:label>Table 4</ce:label><ce:caption id="ca0030"><ce:simple-para id="sp0040" view="all">Background-based CMLLR adaptation results for modified WSJCAM0.</ce:simple-para></ce:caption><ce:alt-text id="atte0080" role="short">Table 4</ce:alt-text><tgroup cols="7"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><colspec colname="col7" colnum="7"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Train</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Test</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Condition</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5K set</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">20K set</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Total</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Rel. impr.</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"><italic>Clean</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Supervised</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">20.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">31.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">26.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">21.5%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Unsupervised</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">21.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">33.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">27.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">17.2%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Supervised</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">14.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">24.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">19.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.4%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Unsupervised</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">14.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">24.8%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">19.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.4%</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0030"><ce:label>Table 5</ce:label><ce:caption id="ca0040"><ce:simple-para id="sp0050" view="all">Factorisation results for modified WSJCAM0.</ce:simple-para></ce:caption><ce:alt-text id="atte0090" role="short">Table 5</ce:alt-text><tgroup cols="7"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><colspec colname="col7" colnum="7"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Train</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Test</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Adaptation</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5K set</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">20K set</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Total</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Rel. impr.</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"><italic>Clean</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">CMLLR/MLLR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">16.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">26.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">21.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">34.7%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">aCMLLR/MLLR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">14.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">24.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">19.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">40.5%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"><italic>Diverse</italic></entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">CMLLR/MLLR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">12.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">21.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">17.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">15.8%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">aCMLLR/MLLR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">12.8%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">21.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">17.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">14.3%</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0035"><ce:label>Table 6</ce:label><ce:caption id="ca0045"><ce:simple-para id="sp0055" view="all">Recognition results per background condition in <ce:italic>Clean</ce:italic> trained models for baseline model and aCMLLR/MLLR factorised adaptation.</ce:simple-para></ce:caption><ce:alt-text id="atte0095" role="short">Table 6</ce:alt-text><tgroup cols="8"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><colspec colname="col7" colnum="7"/><colspec colname="col8" colnum="8"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left">Clean</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col4" namest="col3" align="left">Music</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col8" namest="col5" align="left">Noisy</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Orchestra</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Popular</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Traffic</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Outdoors</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Restaurant</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Applause</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Baseline</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">14.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">36.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">48.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">40.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">28.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">37.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">58.2%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">aCMLLR/MLLR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">10.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">20.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">27.8%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">22.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">17.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">22.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">32.1%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Rel. Impr.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">33.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">43.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">42.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">45.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">38.8%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">41.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">44.8%</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0040"><ce:label>Table 7</ce:label><ce:caption id="ca0050"><ce:simple-para id="sp0060" view="all">RTFs of the decoding for different model architectures.</ce:simple-para></ce:caption><ce:alt-text id="atte0100" role="short">Table 7</ce:alt-text><tgroup cols="4"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">HMM</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Phone synchronous HMM</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Fully asynchronous HMM</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5K task</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5.5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">22.4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">65.2</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">20K task</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">12.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">43.9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">102.8</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0045"><ce:label>Table 8</ce:label><ce:caption id="ca0060"><ce:simple-para id="sp0070" view="all">Distribution of train and test data for experimentation with media broadcasts.</ce:simple-para></ce:caption><ce:alt-text id="atte0110" role="short">Table 8</ce:alt-text><tgroup cols="7"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><colspec colname="col7" colnum="7"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left">Genre</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col4" namest="col2" align="left">Training</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col7" namest="col5" align="left">Testing</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Shows</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Audio</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Speech</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Shows</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Audio</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Speech</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Advice [ADV]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">264</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">193.1 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">155.2 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.0 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2.5 h.</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Children's [CHI]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">415</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">168.6 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">112.5 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.0 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2.0 h.</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Comedy [COM]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">148</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">73.9 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">50.8 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.2 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2.2 h.</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Competition [COP]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">270</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">186.3 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">142.8 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.3 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2.8 h.</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Documentary [DOC]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">285</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">214.2 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">149.9 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">9</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">6.8 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4.6 h.</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Drama [DRA]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">145</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">107.9 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">68.8 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2.7 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.4 h.</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Events [EVE]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">179</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">282.0 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">206.9 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4.4 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2.2 h.</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">News [NEW]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">487</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">354.4 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">309.9 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2.0 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1.8 h.</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Total</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">2193</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1580.4 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">1196.7 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">47</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">28.4 h.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">19.6 h.</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0050"><ce:label>Table 9</ce:label><ce:caption id="ca0065"><ce:simple-para id="sp0075" view="all">WER and relative improvement for transcription of multi-genre broadcasts with ML-trained models.</ce:simple-para></ce:caption><ce:alt-text id="atte0115" role="short">Table 9</ce:alt-text><tgroup cols="10"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><colspec colname="col7" colnum="7"/><colspec colname="col8" colnum="8"/><colspec colname="col9" colnum="9"/><colspec colname="col10" colnum="10"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">ADV</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">CHI</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">COM</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">COP</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">DOC</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">DRA</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">EVE</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">NEW</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Total</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col10" namest="col1" align="left">Baseline</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Baseline</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">25.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30.8%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">44.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">27.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">28.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">42.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">34.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">16.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">31.0%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col10" namest="col1" align="left">Synchronous adaptation</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Show CMLLR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">25.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">44.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">27.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">28.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">42.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">34.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">16.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30.8%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col10" namest="col1" align="left">Asynchronous adaptation</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">aCMLLR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">25.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">44.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">27.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">28.8%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">42.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">34.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">16.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30.7%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">+ Show aCMLLR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">24.8%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">44.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">26.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">28.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">41.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">34.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">16.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30.5%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col10" namest="col1" align="left">Asynchronous adaptive training</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">aNAT</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">25.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">44.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">27.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">28.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">42.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">34.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">16.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30.7%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">+ Show aCMLLR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">24.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">29.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">43.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">26.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">28.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">41.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">34.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">15.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30.3%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">+ Speaker MLLR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">24.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">29.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">43.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">26.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">28.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">41.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">33.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">15.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">30.1%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Rel. impr.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.8%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">4.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.9%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">4.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.9%</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0055"><ce:label>Table 10</ce:label><ce:caption id="ca0070"><ce:simple-para id="sp0080" view="all">WER and relative improvement for transcription of multi-genre broadcasts with MPE-trained models.</ce:simple-para></ce:caption><ce:alt-text id="atte0120" role="short">Table 10</ce:alt-text><tgroup cols="10"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><colspec colname="col7" colnum="7"/><colspec colname="col8" colnum="8"/><colspec colname="col9" colnum="9"/><colspec colname="col10" colnum="10"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">ADV</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">CHI</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">COM</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">COP</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">DOC</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">DRA</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">EVE</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">NEW</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Total</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Baseline</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">23.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">28.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">42.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">25.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">27.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">39.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">32.8%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">15.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">29.2%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">aNAT+CMLLR+MLLR</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">23.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">28.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">41.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">25.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">27.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">39.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">32.0%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">15.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">28.6%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Rel. impr.</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">1.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.1%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">1.2%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">1.8%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">1.5%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.4%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2.0%</entry></row></tbody></tgroup></ce:table></ce:floats><head><ce:title id="tit0010">Acoustic adaptation to dynamic background conditions with asynchronous transformations</ce:title><ce:author-group id="aug0010"><ce:author id="au0010"><ce:given-name>Oscar</ce:given-name><ce:surname>Saz</ce:surname><ce:cross-ref id="crf0010" refid="co0010">*</ce:cross-ref><ce:e-address id="eadd0010" type="email">o.saztorralba@sheffield.ac.uk</ce:e-address></ce:author><ce:author id="au0015"><ce:given-name>Thomas</ce:given-name><ce:surname>Hain</ce:surname></ce:author><ce:affiliation id="af0010"><ce:textfn id="tx0010">Speech and Hearing Group, University of Sheffield, 211 Portobello St., Sheffield S1 4DP, UK</ce:textfn><sa:affiliation><sa:organization>Speech and Hearing Group</sa:organization><sa:organization>University of Sheffield</sa:organization><sa:address-line>211 Portobello St.</sa:address-line><sa:city>Sheffield</sa:city><sa:postal-code>S1 4DP</sa:postal-code><sa:country>UK</sa:country></sa:affiliation></ce:affiliation><ce:correspondence id="co0010"><ce:label>*</ce:label><ce:text id="te0010">Corresponding author at: Speech and Hearing Group, University of Sheffield, 211 Portobello St., Sheffield S1 4DP, UK. Fax: +44 (0) 114 222 1810.</ce:text><sa:affiliation><sa:organization>Speech and Hearing Group</sa:organization><sa:organization>University of Sheffield</sa:organization><sa:address-line>211 Portobello St.</sa:address-line><sa:city>Sheffield</sa:city><sa:postal-code>S1 4DP</sa:postal-code><sa:country>UK</sa:country></sa:affiliation></ce:correspondence></ce:author-group><ce:date-received day="11" month="1" year="2016"/><ce:date-revised day="23" month="5" year="2016"/><ce:date-accepted day="25" month="6" year="2016"/><ce:abstract class="author-highlights" id="ab0010" xml:lang="en" view="all"><ce:section-title id="st0010">Highlights</ce:section-title><ce:abstract-sec id="abs0010" view="all"><ce:simple-para id="sp0010" view="all"><ce:list id="ulist0010"><ce:list-item id="u0010"><ce:label>•</ce:label><ce:para id="p0010" view="all">We propose asynchronous CMLLR adaptation for complex backgrounds.</ce:para></ce:list-item><ce:list-item id="u0015"><ce:label>•</ce:label><ce:para id="p0015" view="all">Noise adaptive techniques and speaker factorisation are also explored.</ce:para></ce:list-item><ce:list-item id="u0020"><ce:label>•</ce:label><ce:para id="p0020" view="all">Results in a noisy WSJ benchmark task show up to 40% WER reduction.</ce:para></ce:list-item><ce:list-item id="u0025"><ce:label>•</ce:label><ce:para id="p0025" view="all">Evaluation in the transcription of multi-media data achieves 3% WER reduction.</ce:para></ce:list-item></ce:list></ce:simple-para></ce:abstract-sec></ce:abstract><ce:abstract class="author" id="ab0015" xml:lang="en" view="all"><ce:section-title id="st0015">Abstract</ce:section-title><ce:abstract-sec id="abs0015" view="all"><ce:simple-para id="sp0015" view="all">This paper proposes a framework for performing adaptation to complex and non-stationary background conditions in Automatic Speech Recognition (ASR) by means of asynchronous Constrained Maximum Likelihood Linear Regression (aCMLLR) transforms and asynchronous Noise Adaptive Training (aNAT). The proposed method aims to apply the feature transform that best compensates the background for every input frame. The implementation is done with a new Hidden Markov Model (HMM) topology that expands the usual left-to-right HMM into parallel branches adapted to different background conditions and permits transitions among them. Using this, the proposed adaptation does not require ground truth or previous knowledge about the background in each frame as it aims to maximise the overall log-likelihood of the decoded utterance. The proposed aCMLLR transforms can be further improved by retraining models in an aNAT fashion and by using speaker-based MLLR transforms in cascade for an efficient modelling of background effects and speaker. An initial evaluation in a modified version of the WSJCAM0 corpus incorporating 7 different background conditions provides a benchmark in which to evaluate the use of aCMLLR transforms. A relative reduction of 40.5% in Word Error Rate (WER) was achieved by the combined use of aCMLLR and MLLR in cascade. Finally, this selection of techniques was applied in the transcription of multi-genre media broadcasts, where the use of aNAT training, aCMLLR transforms and MLLR transforms provided a relative improvement of 2–3%.</ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords class="keyword" id="kwd0010" xml:lang="en" view="all"><ce:section-title id="st0020">Keywords</ce:section-title><ce:keyword id="kw0010"><ce:text id="te0015">Speech recognition</ce:text></ce:keyword><ce:keyword id="kw0015"><ce:text id="te0020">Acoustic adaptation</ce:text></ce:keyword><ce:keyword id="kw0020"><ce:text id="te0025">Factorisation</ce:text></ce:keyword><ce:keyword id="kw0025"><ce:text id="te0030">Dynamic background</ce:text></ce:keyword><ce:keyword id="kw0030"><ce:text id="te0035">Media transcription</ce:text></ce:keyword></ce:keywords></head><body view="all"><ce:sections><ce:section id="s0010" role="introduction" view="all"><ce:label>1</ce:label><ce:section-title id="st0025">Introduction</ce:section-title><ce:para id="p0030" view="all">Complex and dynamic acoustic backgrounds usually cause significant loss of performance on Large Vocabulary Continuous Speech Recognition (LVCSR) systems in many scenarios. Research has focused mostly on situations where the background is stationary or, at least, synchronous with the speech, following the assumption that the characteristics of the background noise remain unchanged through each utterance to decode. Multiple techniques, designed for ASR systems based on Hidden Markov Models (HMMs) and Gaussian Mixture Models (GMMs), have beenreported to provide solid improvements in ASR tasks (<ce:cross-ref id="crf0015" refid="bib0135">Li et al., 2014</ce:cross-ref>). These techniques can be categorised depending on whether they operate in the acoustic space, the feature space or in the model space.</ce:para><ce:para id="p0035" view="all">Acoustic-based techniques aim to remove the background noise in the audio via some speech enhancement techniques like Wiener filtering or Minimum Mean-Square Error (MMSE) (<ce:cross-refs id="crfs0010" refid="bib0050 bib0055">Ephraim and Malah, 1984, 1985</ce:cross-refs>). Several works have reported significant improvement in recognition rates on benchmark tasks using such techniques (<ce:cross-refs id="crfs0015" refid="bib0015 bib0175">Astudillo et al., 2009; Paliwal et al., 2010</ce:cross-refs>). In a similar approach are techniques based on missing features that aim to reconstruct the clean speech signal from the input noisy signal, also with successful results (<ce:cross-ref id="crf0020" refid="bib0040">Cooke et al., 2001</ce:cross-ref>). More recently, techniques based on exemplars and Non-negative Matrix Factorisation (NMF) have provided also substantial gains in several tasks (<ce:cross-refs id="crfs0020" refid="bib0210 bib0245">Raj et al., 2010; Schuller et al., 2010</ce:cross-refs>).</ce:para><ce:para id="p0040" view="all">Techniques in the feature space aim to enhance or transform the input features in order to reduce the mismatch with the GMM–HMM model used for decoding. These include Stereo-based Piecewise Linear Compensation for Environment (SPLICE) (<ce:cross-ref id="crf0025" refid="bib0045">Droppo et al., 2001</ce:cross-ref>) or Multi-Environment Model-based Linear Normalization (MEMLIN) (<ce:cross-ref id="crf0030" refid="bib0025">Buera et al., 2007</ce:cross-ref>), which have been successfully employed in conventional benchmarks for robust ASR. Another well-known technique is Constrained Maximum Likelihood Linear Regression (CMLLR) (<ce:cross-ref id="crf0035" refid="bib0065">Gales, 1998</ce:cross-ref>), which has been widely used to reduce variability caused by multiple sources, like speaker or background.</ce:para><ce:para id="p0045" view="all">Model space techniques aim to re-estimate and adapt the parameters of the GMM–HMM model used for recognition. Methods like Parallel Model Combination (PMC) (<ce:cross-ref id="crf0040" refid="bib0080">Gales and Young, 1996</ce:cross-ref>) or Vector Taylor Series (VTS) (<ce:cross-ref id="crf0045" refid="bib0165">Moreno et al., 1996</ce:cross-ref>) have been especially targeted to speech recognition in noisy environments, while adaptation techniques like Maximum a Posteriori (<ce:cross-ref id="crf0050" refid="bib0090">Gauvain and Lee, 1994</ce:cross-ref>) and Maximum Likelihood Linear Regression (MLLR) (<ce:cross-ref id="crf0055" refid="bib0075">Gales and Woodland, 1996</ce:cross-ref>) have been used for adaptation to different speakers or different background conditions. Other types of model-based methods are adaptive training regimes, where the parameters of the GMM–HMM are re-estimated jointly with the parameters of some of the previously mentioned techniques. In Speaker Adaptive Training (SAT), for instance, MLLR transforms trained from a set of target speakers are used to update the model parameters (<ce:cross-ref id="crf0060" refid="bib0010">Anastasakos et al., 1996</ce:cross-ref>). Extending this, other types of adaptive training regimes have been used for adaptation to the background effects (<ce:cross-refs id="crfs0025" refid="bib0120 bib0140">Kalinli et al., 2010; Liao and Gales, 2007</ce:cross-refs>).</ce:para><ce:para id="p0050" view="all">The assumption of stationarity and synchrony of the background noise is true for corpora such as NOISEX (<ce:cross-ref id="crf0065" refid="bib0270">Varga and Steeneken, 1993</ce:cross-ref>) or Aurora (<ce:cross-ref id="crf0070" refid="bib0115">Hirsch and Pearce, 2000</ce:cross-ref>), traditional benchmarks for noise adaptation and compensation techniques. These corpora were generated by adding noise to clean speech signals. This process guaranteed that a single type of noise was added to each utterance. However, in naturally occurring audio, the assumption of stationarity is often not valid. Non-stationary background effects, such as music or overlapping speech, can be common in many tasks, including the multimedia domain and meeting recognition. Furthermore, acoustic background conditions can, by nature, be independent, and hence asynchronous to the target speaker. Typical examples of asynchronous acoustic events can be applause, laughter or door slamming. The common feature of both non-stationary and asynchronous events is that their acoustic properties are not tied to the beginning and end of a speaker utterance; hence, modelling them as a single static environment does not have to be optimal.</ce:para><ce:para id="p0055" view="all">The work in this paper aims to deal with asynchrony and non-stationarity of the background in ASR tasks, performing a thorough evaluation of the benefits that an explicit modelling of non-stationary backgrounds can provide. The initial technique will be asynchronous CMLLR (aCMLLR) transforms, which will provide adaptation to dynamic acoustic backgrounds in the feature space. This work will be then expanded with two further techniques: an asynchronous Noise Adaptive Training (aNAT) regime will be defined to provide asynchronous adaptation in the model space; and factorisation using cascading aCMLLR and MLLR transforms following work in <ce:cross-refs id="crfs0030" refid="bib0250 bib0255">Seltzer and Acero (2011, 2012</ce:cross-refs>). The proposed techniques will be evaluated in state-of-the-art GMM–HMM systems, with acoustic front-ends like Perceptual Linear Predictive (PLP) features (<ce:cross-ref id="crf0075" refid="bib0110">Hermansky, 1990</ce:cross-ref>), and Deep Neural Network (DNN)-front-ends like bottlenecks features (<ce:cross-refs id="crfs0035" refid="bib0095 bib0145">Grezl and Fousek, 2008; Liu et al., 2014</ce:cross-refs>). This paper expands and describes a common framework for the techniques briefly introduced in <ce:cross-ref id="crf0080" refid="bib0225">Saz and Hain (2013</ce:cross-ref>) and <ce:cross-ref id="crf0085" refid="bib0240">Saz et al. (2015</ce:cross-ref>).</ce:para><ce:para id="p0060" view="all">This paper is organised as follows: <ce:cross-ref id="crf0090" refid="s0015">Section 2</ce:cross-ref> will introduce a novel technique to perform asynchronous background adaptation with feature transforms. <ce:cross-ref id="crf0095" refid="s0030">Section 3</ce:cross-ref> will describe the two extensions to this method for adaptive training and factorisation. <ce:cross-ref id="crf0100" refid="s0045">Section 4</ce:cross-ref> will evaluate the proposed techniques with asynchronous transforms in a controlled scenario with WSJCAM0, and <ce:cross-ref id="crf0105" refid="s0065">Section 5</ce:cross-ref> will provide the results on the automatic transcription of Multi-Genre Broadcasts (MGB). Finally, <ce:cross-ref id="crf0110" refid="s0075">Section 6</ce:cross-ref> will present the conclusions to this work.</ce:para></ce:section><ce:section id="s0015" view="all"><ce:label>2</ce:label><ce:section-title id="st0030">Asynchronous background adaptation with feature transforms</ce:section-title><ce:para id="p0065" view="all">Constrained Maximum Likelihood Linear Regression (CMLLR) (<ce:cross-ref id="crf0115" refid="bib0065">Gales, 1998</ce:cross-ref>) is an adaptation technique initially defined for adapting GMM-based HMMs to a specific speaker, where the same linear transform is applied to both the means and the covariances of the GMM. Due to this property, CMLLR can be equally interpreted as a linear transform applied directly to the input feature vector, which is very useful in many practical situations. The linear transform is given by a transform matrix (<ce:italic>A</ce:italic>) and a bias vector (<ce:italic>b</ce:italic>), which are estimated from the data of the desired speaker. Modelling then uses the transformed feature vectors <ce:italic>y</ce:italic>, as given by Equation <ce:cross-ref id="crf0120" refid="e0010">1</ce:cross-ref>, to perform decoding.<ce:display><ce:formula id="e0010"><ce:label>(1)</ce:label><mml:math altimg="si1.gif" display="block" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></ce:formula></ce:display></ce:para><ce:para id="p0070" view="all">For a speaker <ce:italic>spk</ce:italic>, the pair of transform matrix and bias vector <mml:math altimg="si6.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext>​</mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math> is further referred to as the CMLLR transform for that speaker. Such a transform can also be trained on several utterances from different speakers in a given acoustic background <ce:italic>bck</ce:italic>, for the purpose of background adaptation: <mml:math altimg="si7.gif" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext>​</mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math>. As in all the family of MLLR-based adaptation techniques, CMLLR can be used for supervised adaptation, with manually transcribed data, or for unsupervised adaptation, using the output of an initial recognition stage. Since adaptation data are usually sparse, CMLLR and MLLR techniques use regression classes in order to cluster model parameters together. All phonemes or acoustic units within a regression class will share the same transform, and the number of regression classes can be optimised based on the amount of data available.</ce:para><ce:para id="p0075" view="all">For the purpose of background adaptation, it is usually required to have a priori knowledge of the acoustic background that is present in every given utterance. If this information is not available an assumption has to be made as to the background of a given utterance, either by known context or by an initial system for background detection and classification. Furthermore, CMLLR applies the same linear transform throughout the whole utterance, implicitly assuming that the background has stationary properties during the utterance. The use of CMLLR on non-stationary backgrounds or backgrounds whose conditions change asynchronously with the target speech will result in suboptimal modelling and a loss in the potential improvement provided by the adaptation.</ce:para><ce:para id="p0080" view="all">This paper proposes the use of asynchronous CMLLR (aCMLLR) transforms. In the aCMLLR framework, the transform applied to the input feature vector <ce:italic>x</ce:italic>(<ce:italic>t</ce:italic>) for each frame is different, as in Equation <ce:cross-ref id="crf0125" refid="e0015">2</ce:cross-ref>, with the objective of producing a frame-by-frame adaptation to the acoustic background present across the utterance.<ce:display><ce:formula id="e0015"><ce:label>(2)</ce:label><mml:math altimg="si2.gif" display="block" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></ce:formula></ce:display></ce:para><ce:para id="p0085" view="all">The implementation of Equation <ce:cross-ref id="crf0130" refid="e0015">2</ce:cross-ref> is complex, as it would require a continuous space of background transforms that would optimise the search for each frame <ce:italic>t</ce:italic>, so a set of constraints and assumptions has to be made to develop an implementation of this technique. The specifics of performing speech recognition with aCMLLR transforms as well as the training regime of the transforms are described next.</ce:para><ce:section id="s0020" view="all"><ce:label>2.1</ce:label><ce:section-title id="st0035">Decoding with aCMLLR transforms</ce:section-title><ce:para id="p0090" view="all">In order to develop an implementation of Equation <ce:cross-ref id="crf0135" refid="e0015">2</ce:cross-ref>, the first assumption will be regarding the set of transforms available. This work will assume that there is a finite number, <ce:italic>N</ce:italic>, of previously trained CMLLR transforms to apply (<mml:math altimg="si8.gif" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mtext>​</mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo>…</mml:mo><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mtext>​</mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo>…</mml:mo><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math>). Under this constraint, Equation <ce:cross-ref id="crf0140" refid="e0015">2</ce:cross-ref> is simplified into Equation <ce:cross-ref id="crf0145" refid="e0020">3</ce:cross-ref>:<ce:display><ce:formula id="e0020"><ce:label>(3)</ce:label><mml:math altimg="si3.gif" display="block" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></ce:formula></ce:display>where <ce:italic>c</ce:italic>(<ce:italic>t</ce:italic>) is the index of the pre-existing transform <mml:math altimg="si9.gif" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math>, which better compensates the background for the acoustic frame <ce:italic>x</ce:italic>(<ce:italic>t</ce:italic>). The problem in Equation <ce:cross-ref id="crf0150" refid="e0015">2</ce:cross-ref> has been simplified to finding the optimal sequence <ce:italic>c</ce:italic> of backgrounds for each frame from the pool of <ce:italic>N</ce:italic> existing backgrounds.</ce:para><ce:para id="p0095" view="all">One approach to perform this search could be to use a system that would classify each frame <ce:italic>x</ce:italic>(<ce:italic>t</ce:italic>) as being contaminated by one of the <ce:italic>N</ce:italic> valid background conditions. In order to train such classification system, it is required the existence of sufficient data where the acoustic background has been fully annotated at a frame level. Semi-supervised and unsupervised training techniques could overcome the lack of such data. Even a classification system with good classification performance might not produce significant improvements in recognition rates, mainly because the objective function of classification, namely maximise frame classification rate, does not match the objective function of the decoder, which is to maximise the overall likelihood.</ce:para><ce:para id="p0100" view="all">An alternative solution will be used in this paper to overcome these limitations. This approach uses Viterbi decoding as an on-line framewise classifier, setting as only objective in the whole process the overall maximisation of the likelihood during the Viterbi search. A new HMM topology is proposed for this as shown in <ce:cross-ref id="crf0155" refid="f0010">Fig. 1</ce:cross-ref><ce:float-anchor refid="f0010"/>. This figure presents model structure changes with two possible background transforms, <mml:math altimg="si14.gif" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:math> and <mml:math altimg="si15.gif" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math>, but it easily generalises to any number, <ce:italic>N</ce:italic>, of backgrounds. The usual three-state left-to-right HMM topology with an entry and an exit state is modified by creating two sequences of states from entry to exit. The HMM states in the upper and lower branches share the same GMMs (state 1 with state 4, state 2 with state 5, etc.), but these are modified by two different CMLLR transforms. While states 1, 2 and 3 are transformed by <mml:math altimg="si14.gif" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:math>; states 4, 5 and 6 are transformed by <mml:math altimg="si15.gif" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math>. Additional transitions between the upper and lower paths are included in order to allow the ability to change from one background transform to the other with each new input frame.</ce:para><ce:para id="p0105" view="all">With the topology in <ce:cross-ref id="crf0160" refid="f0010">Fig. 1</ce:cross-ref> decoding can be done following the Maximum Likelihood (ML) criterion and the Viterbi decoding algorithm. On the optimal path, each frame will be transformed by the background transform that provides the highest increase in the overall likelihood. <ce:cross-ref id="crf0165" refid="f0010">Fig. 1</ce:cross-ref> can be slightly modified by removing the transitions between the upper and lower branches. In this case, the same CMLLR transform will be applied to the whole phonetic unit, and changes from one background to another will only be allowed when the phonetic unit changes. This possibility will be referred to as <ce:italic>phone synchronous</ce:italic>, while the original one in <ce:cross-ref id="crf0170" refid="f0010">Fig. 1</ce:cross-ref> will be referred to as <ce:italic>fully asynchronous</ce:italic>. Given that this decoding topology involves applying different CMLLR transform across time, the Jacobian of each transform must be included in the likelihood calculation, as it is the case when using multiple regression classes in standard CMLLR adaptation. Given the expansion in the model structure shown in <ce:cross-ref id="crf0175" refid="f0010">Fig. 1</ce:cross-ref>, an increase in computation can be expected, which without further optimisation can limit the use of such model in tasks where speed is required, such as online decoding.</ce:para><ce:para id="p0110" view="all">This frame-by-frame asynchronous decoding is similar to the proposal for online Vocal Tract Length Normalization (VTLN) in <ce:cross-ref id="crf0180" refid="bib0155">Miguel et al. (2008</ce:cross-ref>). In that work the model space was augmented to consider different VTLN warping values, and the decoder automatically chose the path that maximised the total likelihood through the augmented space. This approach was shown to improve over traditional static VTLN (<ce:cross-ref id="crf0185" refid="bib0130">Lee and Rose, 1998</ce:cross-ref>). Furthermore, the presented topology can be seen similar to approaches for decomposing speech and noise in HMMs (<ce:cross-ref id="crf0190" refid="bib0275">Varga and Moore, 1990</ce:cross-ref>) or to Subspace Gaussian Mixture Models (SGMMs) (<ce:cross-ref id="crf0195" refid="bib0200">Povey et al., 2011a</ce:cross-ref>) where different subspaces can be trained to model different background conditions. The proposal in this paper is more flexible than these two cases, which required to retrain the GMMs in order to cover new backgrounds, while the use of CMLLR transforms requires less acoustic data for adaptation and is more modular to include new background conditions.</ce:para></ce:section><ce:section id="s0025" view="all"><ce:label>2.2</ce:label><ce:section-title id="st0040">Training of aCMLLR transforms</ce:section-title><ce:para id="p0115" view="all">So far the discussion has concentrated on recognition in an ML framework. However this does not immediately allow to infer appropriate training regimes. Usually, transforms are trained following the same assumption that each utterance presents a stationary acoustic background. This way, all the frames from the utterances that share the same background will be used in the estimation of the transform <ce:italic>W<ce:inf loc="post">bck</ce:inf></ce:italic>. This is not the optimal way for estimation of the transforms in the case of non-stationary noise, as not at all the frames in an utterance will share the same background.</ce:para><ce:para id="p0120" view="all">The same topology presented in <ce:cross-ref id="crf0200" refid="f0010">Fig. 1</ce:cross-ref> can be used when learning transforms from adaptation data. Supposing an initial set of background CMLLR transforms exists, trained in a synchronous manner, this topology allows alignment of the input speech to the best sequence of states, as in regular CMLLR training, but also provides the optimal sequence of backgrounds for each frame. With this alignment information, a new set of transforms can be updated from the training data statistics.</ce:para><ce:para id="p0125" view="all">The complete procedure for training of aCMLLR transforms can then be summarised as follows: All the training utterances will be separated into <ce:italic>N</ce:italic> classes according to their acoustic background assuming that this is invariant for the utterance. Then, <ce:italic>N</ce:italic> background CMLLR transforms will be trained on the adaptation data. The data are then pooled and the CMLLR transforms are used to produce an asynchronous alignment to these data with the topology in <ce:cross-ref id="crf0205" refid="f0010">Fig. 1</ce:cross-ref>. Finally, the aCMLLR transforms are then re-estimated with all the frames from the complete set of utterances which have been aligned to each of the asynchronous transforms. This re-estimation process can be iterated in order to produce better alignments and obtain a better estimation of the transforms. While this approach requires a certain knowledge of the backgrounds in the training data to initialise the transforms, it will be seen how loose and unreliable information can be sufficient to perform this procedure.</ce:para></ce:section></ce:section><ce:section id="s0030" view="all"><ce:label>3</ce:label><ce:section-title id="st0045">Extensions to the aCMLLR background adaptation</ce:section-title><ce:section id="s0035" view="all"><ce:label>3.1</ce:label><ce:section-title id="st0050">Asynchronous Noise Adaptive Training</ce:section-title><ce:para id="p0130" view="all">As in other adaptation techniques using (C)MLLR transforms, an adaptive training setup can be implemented using aCMLLR transforms. This asynchronous Noise Adaptive Training (aNAT) is performed following the same procedure as described in the literature (<ce:cross-refs id="crfs0040" refid="bib0010 bib0120 bib0140">Anastasakos et al., 1996; Kalinli et al., 2010; Liao and Gales, 2007</ce:cross-refs>). After the full procedure for aCMLLR training is performed, alignment of the train data can be done using the asynchronous topology in <ce:cross-ref id="crf0210" refid="f0010">Fig. 1</ce:cross-ref>. Again, statistics for each state are collected and a full retraining of the GMM–HMM parameters is performed. At this stage, the Gaussians belonging to the states in the parallel paths in <ce:cross-ref id="crf0215" refid="f0010">Fig. 1</ce:cross-ref> are not shared anymore, as they will have been retrained on new statistics. Following this, new aCMLLR transforms are trained on top of the aNAT model and decoding is done as described previously.</ce:para></ce:section><ce:section id="s0040" view="all"><ce:label>3.2</ce:label><ce:section-title id="st0055">Factorisation of asynchronous background and speaker</ce:section-title><ce:para id="p0135" view="all">Earlier in GMM–HMM-based systems, factorisation of the different sources of variability has been proposed as a way to further improve the performance of ASR systems in varying conditions (<ce:cross-ref id="crf0220" refid="bib0070">Gales, 2001</ce:cross-ref>). Typically the sources of variability considered for factorisation approaches are the speaker variability against the environmental factors, including channel and background conditions. Techniques based on joint factorisation of sources of variability, for example Joint Factor Analysis (JFA) (<ce:cross-ref id="crf0225" refid="bib0290">Yin et al., 2007</ce:cross-ref>), as used in speaker verification tasks, are now being considered for use in ASR tasks. SGMMs aim to incorporate the factorisation directly in the HMM topology (<ce:cross-ref id="crf0230" refid="bib0200">Povey et al., 2011a</ce:cross-ref>) via the use of subspace models. Other approaches are based on jointly combining transforms for the speakers and the environments. This has been done by combining Vector Taylor Series (VTS) and MLLR transforms in <ce:cross-ref id="crf0235" refid="bib0285">Wang and Gales (2011</ce:cross-ref>), CMLLR transforms in <ce:cross-ref id="crf0240" refid="bib0250">Seltzer and Acero (2011</ce:cross-ref>), and CMLLR and MLLR transforms in <ce:cross-ref id="crf0245" refid="bib0255">Seltzer and Acero (2012</ce:cross-ref>). Factorisation techniques based in eigenspace MLLR adaptation have also been studied recently (<ce:cross-ref id="crf0250" refid="bib0230">Saz and Hain, 2014</ce:cross-ref>).</ce:para><ce:para id="p0140" view="all"><ce:cross-ref id="crf0255" refid="bib0250">Seltzer and Acero (2011</ce:cross-ref>) propose factorisation by means of CMLLR transforms applied in cascade. Here background transforms <ce:italic>W<ce:inf loc="post">bck</ce:inf></ce:italic> are trained for every possible background and across all speakers. Further speaker transforms <ce:italic>W<ce:inf loc="post">spk</ce:inf></ce:italic> are trained on top of the background transforms for each speaker across all backgrounds. In decoding stage, for every input utterance <ce:italic>x</ce:italic> spoken by speaker <ce:italic>spk</ce:italic> in background <ce:italic>bck</ce:italic> the feature vectors are transformed to <ce:italic>y</ce:italic> as in Equation <ce:cross-ref id="crf0260" refid="e0025">4</ce:cross-ref>.<ce:display><ce:formula id="e0025"><ce:label>(4)</ce:label><mml:math altimg="si4.gif" display="block" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></ce:formula></ce:display></ce:para><ce:para id="p0145" view="all">Using both transforms in cascade was shown to improve results over conventional CMLLR adaptation on environment and speaker. Also, the speaker transforms, which had been estimated on feature frames already adapted to the environment, were shown to perform well when used across previously unseen backgrounds. It is straightforward to generalise this proposal to deal with non-stationary and asynchronous backgrounds following Equation <ce:cross-ref id="crf0265" refid="e0030">5</ce:cross-ref>:<ce:display><ce:formula id="e0030"><ce:label>(5)</ce:label><mml:math altimg="si5.gif" display="block" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></ce:formula></ce:display></ce:para><ce:para id="p0150" view="all">Similar factorisation can be achieved following the work in <ce:cross-ref id="crf0270" refid="bib0255">Seltzer and Acero (2012</ce:cross-ref>) using CMLLR and MLLR transforms. In this case, a background-based CMLLR transform is used to transform the input features, while a factorised speaker-based MLLR transform is used to transform the models. Using asynchronous adaptation on the CMLLR transforms, it is also straightforward to provide factorisation with cascading aCMLLR/MLLR transforms. Further work in factorised adaptation for ASR has argued the need for the components of the factorisation to be orthogonal (<ce:cross-ref id="crf0275" refid="bib0260">Seo et al., 2014</ce:cross-ref>) in order to avoid correlation between the different factorised elements. Although the use of feature-space and model-space MLLR transforms does not involve orthogonality, <ce:cross-ref id="crf0280" refid="bib0255">Seltzer and Acero (2012</ce:cross-ref>) argue that it achieves a better factorisation between the different variability factors in speech than, for instance, using cascading feature-space transforms.</ce:para></ce:section></ce:section><ce:section id="s0045" view="all"><ce:label>4</ce:label><ce:section-title id="st0060">Benchmark results: WSJCAM0</ce:section-title><ce:para id="p0155" view="all">An initial evaluation of the proposed techniques was performed on a modified version of the WSJCAM0 corpus. WSJCAM0 is a re-recording of the original WSJ sentences uttered by a collection of British speakers (<ce:cross-ref id="crf0285" refid="bib0220">Robinson et al., 1995</ce:cross-ref>). WSJ is a very common benchmark for the evaluation of acoustic modelling techniques and speaker adaptation tasks in ASR (<ce:cross-ref id="crf0290" refid="bib0190">Paul and Baker, 1992</ce:cross-ref>) and it was the base for the creation of Aurora4 (<ce:cross-ref id="crf0295" refid="bib0185">Parihar et al., 2004</ce:cross-ref>), which is also commonly used as a benchmark in background adaptation for robust ASR. For the purpose of our experiments, 7387 utterances from 86 speakers were used for training, and 1315 utterances from 18 speakers were used for evaluation. Besides this <ce:italic>Clean</ce:italic> data, new train and test sets were generated including highly diverse background conditions, which will be referred to as <ce:italic>Diverse</ce:italic> data. In the <ce:italic>Diverse</ce:italic> data sets, 7 possible background conditions appear with the distribution of segments seen in <ce:cross-ref id="crf0300" refid="t0010">Table 1</ce:cross-ref><ce:float-anchor refid="t0010"/> for Train and Test data. The Signal-to-Noise Ratio (SNR) of the new utterances in the <ce:italic>Diverse</ce:italic> data was uniformly distributed from 5 db to 15 dB. Furthermore, in the <ce:italic>Diverse</ce:italic> sets, segments were drawn randomly for the recordings using the close-talking microphone and the table-top microphone, with a 50% distribution of a given sample being from either source. The <ce:italic>Clean</ce:italic> sets corresponded only to the close-talking microphone recordings.</ce:para><ce:para id="p0160" view="all">The ASR experiments were performed on two of the original WSJ tasks: the 5000-word closed vocabulary task with a 2-gram Language Model (LM); and the 20,000-word open vocabulary task with a 3-gram LM. The baseline ASR system used was built using a Hidden Markov Model Toolkit (HTK) (<ce:cross-ref id="crf0305" refid="bib0295">Young et al., 2006</ce:cross-ref>) setup. Crossword triphone models with 3 states per model and 16 Gaussians per state were used. A total of 1816 physical states were trained using the Maximum Likelihood (ML) criterion. Thirty-nine-dimension feature vectors were used with 13 PLP features (<ce:cross-ref id="crf0310" refid="bib0110">Hermansky, 1990</ce:cross-ref>) and their first and second derivatives. Cepstral Mean Normalization (CMN) was applied to the static features for each utterance. The standard WSJ lexicons and language models were used in decoding.</ce:para><ce:para id="p0165" view="all">The baseline results in Word Error Rate (WER) achieved with this setup are presented in <ce:cross-ref id="crf0315" refid="t0015">Table 2</ce:cross-ref><ce:float-anchor refid="t0015"/>. The results in the <ce:italic>Diverse</ce:italic> testset showed considerable decrease in performance compared to the <ce:italic>Clean</ce:italic> testset, 33.1% against 9.5%. This indicated that the background conditions included in the data had a negative influence in ASR performance. The use of models trained on <ce:italic>Diverse</ce:italic> data showed an improvement, reaching 20.3% WER, but performance was still far from the result on <ce:italic>Clean</ce:italic> test data.</ce:para><ce:section id="s0050" view="all"><ce:label>4.1</ce:label><ce:section-title id="st0065">Adaptation experiments</ce:section-title><ce:para id="p0170" view="all">The next set of experiments studied different types of adaptation, speaker and background adaptation, in these data sets. For background adaptation, single-regression-class block-diagonal CMLLR transforms were used, while for speaker adaptation 5-regression-class block-diagonal MLLR transforms were used. Background adaptation was performed in supervised and unsupervised fashion, depending on whether the adaptation was made on data from the training set, with ground truth transcriptions, or from the test set, with errorful transcriptions from ASR. Speaker adaptation was only studied in unsupervised mode, as none of the speakers in the test set appeared in the training set.</ce:para><ce:para id="p0175" view="all">The results for these experiments are shown in <ce:cross-refs id="crfs0045" refid="t0020 t0025">Tables 3 and 4</ce:cross-refs><ce:float-anchor refid="t0020"/>. Speaker adaptation achieved a 15% relative improvement on the <ce:italic>Clean</ce:italic> test set over <ce:italic>Clean</ce:italic> models, and outperformed background adaptation on the <ce:italic>Diverse</ce:italic> test set for both models. The gains were more pronounced in the mismatched condition with <ce:italic>Clean</ce:italic> models, with background adaptation only providing 3% improvement over <ce:italic>Diverse</ce:italic> models. For the <ce:italic>Clean</ce:italic> models, the use of supervised CMLLR adaptation to the 7 backgrounds using the training set improved the use of unsupervised adaptation.<ce:float-anchor refid="t0025"/></ce:para><ce:para id="p0180" view="all">Asynchronous background adaptation was then performed on this setup. The topology used for asynchronous adaptation was based on 7 CMLLR transforms, previously trained in a synchronous fashion for the 7 types of background included in the <ce:italic>Diverse</ce:italic> data. <ce:cross-ref id="crf0320" refid="f0015">Fig. 2</ce:cross-ref><ce:float-anchor refid="f0015"/> shows the results for four possible cases depending whether the GMM–HMM models used were trained on <ce:italic>Clean</ce:italic> on <ce:italic>Diverse</ce:italic> data and whether the adaptation was made unsupervised or supervised. Four possible conditions were also defined based on the situations where the topology in training and testing was <ce:italic>Phone synchronous</ce:italic> or <ce:italic>Fully asynchronous</ce:italic> according to the definition explained in <ce:cross-ref id="crf0330" refid="s0020">Section 2.1</ce:cross-ref>. From these results, it was observed that using a <ce:italic>Phone synchronous</ce:italic> topology in training and <ce:italic>Fully asynchronous</ce:italic> in decoding provided the largest improvements. The best result in the condition with <ce:italic>Clean</ce:italic> models was 22.9%, which was an improvement of 30.8% over the baseline, 9% better than the synchronous background adaptation. With the use of <ce:italic>Diverse</ce:italic> models, the best result achieved was 19.5%, only 4.0% below the baseline and barely 0.6% better than the case with synchronous background adaptation.</ce:para><ce:para id="p0185" view="all">The results in <ce:cross-ref id="crf0335" refid="f0015">Fig. 2</ce:cross-ref> showed a divergence in the performance of the aCMLLR framework in the <ce:italic>Clean</ce:italic> and <ce:italic>Diverse</ce:italic> train conditions. While large gains were achieved with <ce:italic>Clean</ce:italic> models, no significant changes were seen with <ce:italic>Diverse</ce:italic> models. Also, the initial gain for using background-based CMLLR transformations over the baseline was smaller (3.4% relative) in the <ce:italic>Diverse</ce:italic> train conditions. This effect in what is a classical multicondition training scenario might arise from the way the modified WSJCAM0 corpus was built. In this case, the same 7 types of noises were used in train and test, and the baseline GMM–HMM models will have implicitly learnt the characteristics of each background, thus giving little room for improvement for any type of background-based adaptation. This highlights the needs for realistic scenarios in robust ASR, where a variety of backgrounds and noises appear differently in the train and test data.</ce:para></ce:section><ce:section id="s0055" view="all"><ce:label>4.2</ce:label><ce:section-title id="st0070">Factorisation experiments</ce:section-title><ce:para id="p0190" view="all">Factorisation techniques were then studied, based on the use of CMLLR/MLLR cascading transforms. Both synchronous and asynchronous CMLLR background transforms were used, together with subsequent MLLR speaker transforms. The results in <ce:cross-ref id="crf0340" refid="t0030">Table 5</ce:cross-ref><ce:float-anchor refid="t0030"/> show that the WER in the mismatched condition was reduced to 19.7% with the use of asynchronous background transforms (40.5% relative improvement), while synchronous background adaptation only achieved 34.7% relative improvement. However, in the <ce:italic>Diverse</ce:italic> models, asynchronous adaptation produced an increase of 0.3% in WER compared to the synchronous case. These results were consistent in an early exploratory work carried out with a more limited number of background conditions (<ce:cross-ref id="crf0345" refid="bib0225">Saz and Hain, 2013</ce:cross-ref>).</ce:para><ce:para id="p0195" view="all">A final analysis of these results was done regarding the ability of the proposed method to adapt the <ce:italic>Clean</ce:italic> models to the different types of backgrounds existing in the <ce:italic>Diverse</ce:italic> set. <ce:cross-ref id="crf0350" refid="t0035">Table 6</ce:cross-ref><ce:float-anchor refid="t0035"/> shows the results for the baseline and the best result (aCMLLR/MLLR cascade) for the 7 background conditions. The proposed method improved significantly even in the Clean background and in the Outdoors noise background, which had the lowest initial WER. Meanwhile, the backgrounds with the highest degradation, like popular music, traffic noise or applause, achieved relative improvements of up to 45%.</ce:para></ce:section><ce:section id="s0060" view="all"><ce:label>4.3</ce:label><ce:section-title id="st0075">Computational complexity</ce:section-title><ce:para id="p0200" view="all">When discussing the new HMM structure shown in <ce:cross-ref id="crf0355" refid="f0010">Fig. 1</ce:cross-ref>, it was expected that computational complexity would increase as the extra paths in the HMM are added to model different backgrounds. To evaluate the extent of such increase, the Real Time Factors (RTF) of the decoding of both WSJCAM0 tasks were studied in 3 cases: Standard HMM structure, phone synchronous HMM structure and fully asynchronous HMM structure. The results are presented in <ce:cross-ref id="crf0360" refid="t0040">Table 7</ce:cross-ref><ce:float-anchor refid="t0040"/> and show increases of 4× and 3× in RTF for the phone synchronous structure in the 5k and 20k tasks, respectively, and of 11× and 7× for the fully asynchronous structure. All results were achieved in similar conditions, where the decoding of the test set was submitted as an array job to a computing grid using the OpenSGE grid scheduler. The actual physical machines in the grid where the processes ran had 32 hyper-threaded Intel Xeon cores at 2.6 GHz each.</ce:para><ce:para id="p0205" view="all">This indicated that when time is a constraint in the decoding stage, the phone synchronous structure in decoding should be preferred. Although a limitation for some tasks, such as online decoding, the increase in computation time does not impair the proposed method, especially when performance is the main goal. A way of reducing this increase in time can be reducing the number of modelled backgrounds, as this will reduce the number of extra paths added to the HMM structure. Also, according to the measurements in <ce:cross-ref id="crf0365" refid="t0040">Table 7</ce:cross-ref>, the extra increase in decoding time becomes less relevant as the vocabulary size and language model complexity increase, possibly due to the acoustic decoding accounting for less of the actual computation time.</ce:para><ce:para id="p0210" view="all">The increase in computation time is linked to the ability of the new HMM structure to switch backgrounds. <ce:cross-ref id="crf0370" refid="f0020">Fig. 3</ce:cross-ref><ce:float-anchor refid="f0020"/> presents the actual background paths for two files in the 5K task. <ce:cross-ref id="crf0375" refid="f0020">Fig. 3(a) and 3(c)</ce:cross-ref> shows this for a clean signal using phone synchronous and fully asynchronous HMMs, while <ce:cross-ref id="crf0380" refid="f0020">Fig. 3(b) and 3(d)</ce:cross-ref> shows them for a signal contaminated with music. Using a <ce:italic>Phone synchronous</ce:italic> HMM structure yields a much lower rate of background switching, with only 0.18% and 0.57% of frames being decoded with a different background than their precedent frame. In the <ce:italic>Fully asynchronous</ce:italic> structure, changes occur more frequently, up to 1.16% and 4.28% of the total frames change the background compared to their precedent frame. As the figures show, a more non-stationary background like music produced an increase in background changes across frames.</ce:para></ce:section></ce:section><ce:section id="s0065" view="all"><ce:label>5</ce:label><ce:section-title id="st0080">Transcription of multi-genre broadcasts</ce:section-title><ce:para id="p0215" view="all">Full evaluation of the proposed techniques was conducted on the data available for Task 1 of the Multi-Genre Broadcast (MGB) challenge, which is speech-to-text transcription of broadcast television (<ce:cross-ref id="crf0385" refid="bib0020">Bell et al., 2015</ce:cross-ref>). The MGB challenge aimed to evaluate and improve several speech technology tasks in the area of media broadcasts, extending the work of other evaluations like Hub4 (<ce:cross-ref id="crf0390" refid="bib0180">Pallett et al., 1996</ce:cross-ref>), TDT (<ce:cross-ref id="crf0395" refid="bib0035">Cieri et al., 1999</ce:cross-ref>), Ester (<ce:cross-ref id="crf0400" refid="bib0085">Galliano et al., 2006</ce:cross-ref>), Albayzin (<ce:cross-ref id="crf0405" refid="bib0300">Zelenak et al., 2012</ce:cross-ref>) and MediaEval (<ce:cross-ref id="crf0410" refid="bib0125">Larson et al., 2013</ce:cross-ref>). All experiments are based on the official MGB challenge training set and evaluated on the official development set. <ce:cross-ref id="crf0415" refid="t0045">Table 8</ce:cross-ref><ce:float-anchor refid="t0045"/> presents the statistics of these data in terms of number of shows, and amount of audio and speech in the training and testing sets. One of the goals of the task was to study recognition performance across diverse broadcast genres, for that reason both training and test data were labelled according to 8 possible genres: advice, children's, comedy, competition, documentary, drama, events and news.</ce:para><ce:para id="p0220" view="all">The baseline system configuration used for this task was a DNN–GMM–HMM system with the following setup. A DNN was used as a front-end for extracting a set of 26 bottleneck features. Such DNN took as input 15 contiguous log-filterbank frames and consisted of 4 hidden layers of 1745 neurons plus the 26-neuron bottleneck layer, and an output layer of 8000 triphone state targets. The state-level Minimum Bayes Risk (sMBR) criterion was used as the optimisation criterion and Stochastic Gradient Descent (SGD) was used for parameter updating. DNN training used the TNet (<ce:cross-ref id="crf0420" refid="bib0280">Vesely et al., 2010</ce:cross-ref>) and Kaldi (<ce:cross-ref id="crf0425" refid="bib0205">Povey et al., 2011b</ce:cross-ref>) toolkits. The input feature vectors for training the GMM–HMM system were 65-dimensional, including the 26 dimensional bottleneck features, as well as 13 dimensional PLP features together with their first and second derivatives. GMM–HMM models were trained using 16 Gaussian components per state and around 8000 distinct triphone states.</ce:para><ce:para id="p0225" view="all">The manual segmentation as provided for the development set was used; no automatic speech segmentation was required for the experiments. However, for speaker adaptation ground truth was not available, so automatic speaker clustering had to be used. The clustering system used was based on the Bayesian Information Criterion (BIC) (<ce:cross-ref id="crf0430" refid="bib0030">Chen and Gopalakrishnan, 1998</ce:cross-ref>) and was similar to the one used by the University of Sheffield in the system submitted for the longitudinal diarisation of broadcast television task of the MGB challenge, with a speaker error rate of 41.7% (<ce:cross-ref id="crf0435" refid="bib0160">Milner et al., 2015</ce:cross-ref>). The diarisation task proves especially challenging in broadcast data, as shown by the general results achieved by the participating groups in the MGB challenge (<ce:cross-ref id="crf0440" refid="bib0020">Bell et al., 2015</ce:cross-ref>). For these experiments single-regression-class block-diagonal CMLLR and 5-regression-class block-diagonal MLLR transforms were used.</ce:para><ce:para id="p0230" view="all">The only transcription available for the 1200 hours of training speech was the original BBC subtitles, aligned to the audio data using a lightly supervised approach (<ce:cross-ref id="crf0445" refid="bib0150">Long et al., 2013</ce:cross-ref>). Given that this can produce unreliable transcripts for some segments, the training data were filtered and only 700 hours of speech were used for training. For filtering, a segment-level confidence measure was calculated based on posterior estimates obtained with a DNN as in <ce:cross-ref id="crf0450" refid="bib0305">Zhang et al. (2014</ce:cross-ref>). Only segments with higher values of confidence were maintained in the training set, with the threshold set to obtain 700 hours of speech.</ce:para><ce:para id="p0235" view="all">Decoding was carried out in two stages; in a first stage, lattices were generated using a 2-gram LM; this was followed by rescoring of these lattices using a 4-gram LM and obtaining the 1-best output. The outputs were scored with the official MGB scoring package (<ce:cross-ref id="crf0455" refid="bib0020">Bell et al., 2015</ce:cross-ref>), which was based on NIST scoring tools (<ce:cross-ref id="crf0460" refid="bib0060">Fiscus, 2007</ce:cross-ref>), and the evaluation done in terms of global WER and genre-specific WERs. Both language models were built using the SRI LM toolkit (<ce:cross-ref id="crf0465" refid="bib0265">Stolcke, 2002</ce:cross-ref>) from more than 700 million of words from subtitles provided as material for the challenge. The vocabulary size used in decoding was a 50,000 word list, constructed from the most frequent words in the subtitles provided for language model training. Pronunciations were obtained using the Combilex pronunciation dictionary (<ce:cross-ref id="crf0470" refid="bib0215">Richmond et al., 2010</ce:cross-ref>), which was provided to the challenge participants. When a certain word was not contained in the lexicon, automatically generated pronunciations were obtained using the Phonetisaurus toolkit (<ce:cross-ref id="crf0475" refid="bib0170">Novak et al., 2012</ce:cross-ref>). These pronunciations were expanded to incorporate pronunciation probabilities, learnt from the alignment of the acoustic training data (<ce:cross-ref id="crf0480" refid="bib0105">Hain, 2005</ce:cross-ref>).</ce:para><ce:section id="s0070" role="results" view="all"><ce:label>5.1</ce:label><ce:section-title id="st0085">Results</ce:section-title><ce:para id="p0240" view="all">Several experiments were carried out in this setup. All results are presented in <ce:cross-ref id="crf0485" refid="t0050">Table 9</ce:cross-ref><ce:float-anchor refid="t0050"/>. The initial baseline performance obtained with the unadapted system was 31.0% WER. A large variability in results could be observed across genres, ranging from 16.3% WER for news shows, to 44.7% for comedy shows. Synchronous adaptation was studied first by means of show-specific CMLLR transforms, targeted to capture variability due to the background condition in each show. This gave a slight improvement of 0.2%, which showed the difficulty of trying to model the types of backgrounds present in broadcast shows by using traditional approaches.</ce:para><ce:para id="p0245" view="all">An initial set of aCMLLR transforms were trained on the training data using the following procedure. First, 8 genre-based CMLLR transforms were trained from the training set. Assuming that each genre will present certain distinct background conditions, these transforms were used as initialisation in the training of a global aCMLLR transform with 8 possible backgrounds. The use of this transform achieved 0.3% absolute improvement over the baseline, also improving the use of the show-based CMLLR transforms. Finally, this global aCMLLR transform was used as initialisation for training show-based aCMLLR transforms on the test shows, adding an extra 0.2% absolute reduction of the WER.</ce:para><ce:para id="p0250" view="all">The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure. This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model. However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute. This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show. Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested. This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation.</ce:para><ce:para id="p0255" view="all">Finally, Minimum Phone Error (MPE) training (<ce:cross-ref id="crf0490" refid="bib0195">Povey and Woodland, 2002</ce:cross-ref>) was performed in this dataset, since it is well-known to provide significant improvements in GMM–HMM systems with bottleneck features (<ce:cross-ref id="crf0495" refid="bib0100">Grezl et al., 2009</ce:cross-ref>). The results of the baseline models and the MPE–aNAT retrained models with aCMLLR and MLLR transformations are presented in <ce:cross-ref id="crf0500" refid="t0055">Table 10</ce:cross-ref><ce:float-anchor refid="t0055"/>. The use of the MPE training criterion provides a 1.8% absolute improvement over the ML baseline. This gain may seem lower than those usually reported using MPE over ML, but in the MGB tasks the transcriptions of the training set are errorful, which has been reported to produce a decrease in MPE performance (<ce:cross-ref id="crf0505" refid="bib0150">Long et al., 2013</ce:cross-ref>). Finally, the use of MPE–aNAT with asynchronous CMLLR and MLLR reduces the WER to 28.6%. Although the relative gain over the MPE baseline is reduced to 2.0% (from 2.8% using ML models), this was found to still be significant.</ce:para></ce:section></ce:section><ce:section id="s0075" role="conclusion" view="all"><ce:label>6</ce:label><ce:section-title id="st0090">Conclusions</ce:section-title><ce:para id="p0260" view="all">As a summary, this paper has presented a complete set of tools for improving the performance of ASR systems in complex background conditions. It has been shown how asynchronous CMLLR transforms can be successfully trained and used in the decoding of large vocabulary speech. Furthermore, two extensions to this asynchronous adaptation have also been proposed. In the first one, it has been shown how it is possible to generate adaptively trained models using asynchronous transforms, leading to a more flexible modelling of the backgrounds. Also, the possibility of stacking transforms to deal with multiple sources of variability, like background and speaker, can be enhanced by the use of asynchronous modelling of the background.</ce:para><ce:para id="p0265" view="all">An evaluation of the proposed techniques in a benchmark task, a modified version of WSJCAM0, has shown large improvements, up to 40% in overall when using <ce:italic>Clean</ce:italic> trained models on a very <ce:italic>Diverse</ce:italic> test set. This improvement occurs across a very varied range of acoustic conditions, with significant improvement being achieved also for clean test data. The evaluation in WSJCAM0 provided a good insight into the strengths of the techniques and into how to achieve the best performance from them.</ce:para><ce:para id="p0270" view="all">Finally, an evaluation of the techniques in a very complex scenario has shown that they can achieve gain in a real environment. The transcription of multi-media broadcasts, going beyond the transcription of broadcast news, is a difficult task, where the existence of multiple and dynamic background conditions (noise, music, overlapping speech, etc.) is a major cause of performance degradation. This is better exemplified by the challenging performance achieved in some genres like comedy or drama, over 40% WER. The proposed techniques have been shown to reduce absolute WER from 31.0% to 30.1% using ML models and from 29.2% to 28.6% using MPE models, which represents a significant improvement for this task. The techniques have also been successfully used as part of a more complex system, submitted to the MGB challenge (<ce:cross-ref id="crf0510" refid="bib0240">Saz et al., 2015</ce:cross-ref>), and that provided one of the top performances in the overall evaluation (<ce:cross-ref id="crf0515" refid="bib0020">Bell et al., 2015</ce:cross-ref>). This indicates that these techniques can perform well in large tasks and in complex environments.</ce:para><ce:para id="p0275" view="all">In further analysis, the proposed aCMLLR background adaptation technique has been shown to work in situations where some knowledge about the background conditions exists, as in WSJCAM0 experiments, and where no knowledge of the background conditions is present, as in broadcast shows. This shows how the proposed training regime can work even with a very loose knowledge of the background situation for initialisation, which makes this technique especially suitable for the most complex situations where the acoustics are unknown.</ce:para><ce:para id="p0280" view="all">Finally, other work has shown that asynchronous background modelling can be used for other tasks beyond ASR, as for instance identifying the genre of broadcast shows (<ce:cross-ref id="crf0520" refid="bib0235">Saz et al., 2014</ce:cross-ref>). This opens possibilities for future applications of asynchronous background adaptation, which could prove very useful in areas where modelling and compensation of dynamic and challenging background conditions are especially important.</ce:para></ce:section><ce:section id="s0080" view="all"><ce:label>7</ce:label><ce:section-title id="st0095">Data access management</ce:section-title><ce:para id="p0285" view="all">The original WSJCAM0 corpus is available via the Linguistic Data Consortium with catalogue number LDC95S24. The modified version of WSJCAM0 used in the article is available with DOI 10.15131/shef.data.3363466. All the data related to the MGB challenge, including audio files, subtitle text and scoring scripts, are available via special licence with the BBC on <ce:inter-ref id="iw0010" xlink:href="http://www.mgb-challenge.org/" xlink:type="simple">http://www.mgb-challenge.org/</ce:inter-ref>. All recognition outputs and scoring results are available with DOI 10.15131/shef.data.3248584.</ce:para></ce:section></ce:sections><ce:acknowledgment id="ac0010" view="all"><ce:section-title id="st0100">Acknowledgement</ce:section-title><ce:para id="p0290" view="all">This work was supported by the <ce:grant-sponsor id="gsp0010" xlink:type="simple" xlink:role="http://www.elsevier.com/xml/linking-roles/grant-sponsor">EPSRC Programme</ce:grant-sponsor> Grant <ce:grant-number id="gnum0010" refid="gsp0010">EP/I031022/1</ce:grant-number> (Natural Speech Technology).</ce:para></ce:acknowledgment></body><tail view="all"><ce:bibliography id="bb0010" view="all"><ce:section-title id="st0105">References</ce:section-title><ce:bibliography-sec id="bs0010" view="all"><ce:bib-reference id="bib0010"><ce:label>Anastasakos et al, 1996</ce:label><sb:reference id="sr0010"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Anastasakos</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>McDonough</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Schwartz</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Makhoul</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A compact model for speaker-adaptive training</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 4th International Conference on Spoken Language Processing (ICSLP)</sb:maintitle></sb:title><sb:date>1996</sb:date></sb:edited-book><sb:pages><sb:first-page>1137</sb:first-page><sb:last-page>1140</sb:last-page></sb:pages></sb:host><sb:comment>Philadelphia, PA</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0015"><ce:label>Astudillo et al, 2009</ce:label><sb:reference id="sr0015"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>R.F.</ce:given-name><ce:surname>Astudillo</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Hoffmann</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Manderlatz</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Orglmeister</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Speech enhancement for automatic speech recognition using complex Gaussian mixture priors for noise and speech</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2009 Non-Linear Speech Processing (NOLISP) Workshop</sb:maintitle></sb:title><sb:date>2009</sb:date></sb:edited-book><sb:pages><sb:first-page>60</sb:first-page><sb:last-page>67</sb:last-page></sb:pages></sb:host><sb:comment>Vic, Spain</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0020"><ce:label>Bell et al, 2015</ce:label><sb:reference id="sr0020"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Bell</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Gales</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Hain</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Kilgour</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Lanchantin</ce:surname></sb:author><sb:author><ce:surname>Liu</ce:surname><ce:given-name>X.</ce:given-name></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>The MGB challenge: evaluating multi-genre broadcast media recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2015 IEEE Automatic Speech Recognition and Understanding Workshop</sb:maintitle></sb:title><sb:date>2015</sb:date></sb:edited-book><sb:pages><sb:first-page>687</sb:first-page><sb:last-page>693</sb:last-page></sb:pages></sb:host><sb:comment>Scottsdale, AZ</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0025"><ce:label>Buera et al, 2007</ce:label><sb:reference id="sr0025"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Buera</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Lleida</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Miguel</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Ortega</ce:surname></sb:author><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Saz</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Cepstral vector normalization based on stereo data for robust speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Audio Speech Lang. Process</sb:maintitle></sb:title><sb:volume-nr>15</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>2007</sb:date></sb:issue><sb:pages><sb:first-page>1098</sb:first-page><sb:last-page>1113</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0030"><ce:label>Chen, Gopalakrishnan, 1998</ce:label><sb:reference id="sr0030"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Chen</ce:surname><ce:given-name>S.S.</ce:given-name></sb:author><sb:author><ce:given-name>P.S.</ce:given-name><ce:surname>Gopalakrishnan</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Clustering via the Bayesian information criterion with applications in speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>1998</sb:date></sb:edited-book><sb:pages><sb:first-page>645</sb:first-page><sb:last-page>648</sb:last-page></sb:pages></sb:host><sb:comment>Seattle, WA</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0035"><ce:label>Cieri et al, 1999</ce:label><sb:reference id="sr0035"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Cieri</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Graff</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Liberman</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Martey</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Strassel</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The TDT–2 text and speech corpus</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 1999 DARPA Broadcast News Workshop</sb:maintitle></sb:title><sb:date>1999</sb:date></sb:edited-book></sb:host><sb:comment>Herndon, VA</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0040"><ce:label>Cooke et al, 2001</ce:label><sb:reference id="sr0040"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Cooke</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Green</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Josifovski</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Vizinho</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Robust automatic speech recognition with missing and unreliable acoustic data</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Speech Commun</sb:maintitle></sb:title><sb:volume-nr>34</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>2001</sb:date></sb:issue><sb:pages><sb:first-page>267</sb:first-page><sb:last-page>285</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0045"><ce:label>Droppo et al, 2001</ce:label><sb:reference id="sr0045"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Droppo</ce:surname></sb:author><sb:author><ce:surname>Deng</ce:surname><ce:given-name>L.</ce:given-name></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Acero</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Evaluation of the SPLICE algorithm on the AURORA2 database</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 7th European Conference on Speech Communication and Technology (Eurospeech)</sb:maintitle></sb:title><sb:date>2001</sb:date></sb:edited-book><sb:pages><sb:first-page>217</sb:first-page><sb:last-page>220</sb:last-page></sb:pages></sb:host><sb:comment>Aalborg, Denmark</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0050"><ce:label>Ephraim, Malah, 1984</ce:label><sb:reference id="sr0050"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Ephraim</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Malah</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Speech enhancement using a minimum mean-square error short-time amplitude estimator</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Acoust</sb:maintitle></sb:title><sb:volume-nr>32</sb:volume-nr></sb:series><sb:issue-nr>6</sb:issue-nr><sb:date>1984</sb:date></sb:issue><sb:pages><sb:first-page>1109</sb:first-page><sb:last-page>1121</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0055"><ce:label>Ephraim, Malah, 1985</ce:label><sb:reference id="sr0055"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Ephraim</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Malah</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Speech enhancement using a minimum mean-square error log-spectral amplitude estimator</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Acoust</sb:maintitle></sb:title><sb:volume-nr>33</sb:volume-nr></sb:series><sb:issue-nr>2</sb:issue-nr><sb:date>1985</sb:date></sb:issue><sb:pages><sb:first-page>443</sb:first-page><sb:last-page>445</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0060"><ce:label>Fiscus, 2007</ce:label><sb:reference id="sr0060"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Fiscus</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Speech Recognition Scoring Toolkit (SCTK) version 2.4.0</sb:maintitle></sb:title></sb:contribution><sb:host><sb:e-host><ce:inter-ref id="iw0015" xlink:href="http://www.itl.nist.gov/iad/mig/tools/" xlink:type="simple">http://www.itl.nist.gov/iad/mig/tools/</ce:inter-ref><sb:date>2007</sb:date></sb:e-host></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0065"><ce:label>Gales, 1998</ce:label><sb:reference id="sr0065"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.J.F.</ce:given-name><ce:surname>Gales</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Maximum likelihood linear transformations for HMM-based speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Comput. Speech Lang</sb:maintitle></sb:title><sb:volume-nr>12</sb:volume-nr></sb:series><sb:issue-nr>2</sb:issue-nr><sb:date>1998</sb:date></sb:issue><sb:pages><sb:first-page>75</sb:first-page><sb:last-page>98</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0070"><ce:label>Gales, 2001</ce:label><sb:reference id="sr0070"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.J.F.</ce:given-name><ce:surname>Gales</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Acoustic factorisation</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2001 IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop</sb:maintitle></sb:title><sb:date>2001</sb:date></sb:edited-book><sb:pages><sb:first-page>77</sb:first-page><sb:last-page>80</sb:last-page></sb:pages></sb:host><sb:comment>Madonna di Campiglio, Italy</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0075"><ce:label>Gales, Woodland, 1996</ce:label><sb:reference id="sr0075"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.J.F.</ce:given-name><ce:surname>Gales</ce:surname></sb:author><sb:author><ce:given-name>P.C.</ce:given-name><ce:surname>Woodland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Mean and variance adaptation within the MLLR framework</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Comput. Speech Lang</sb:maintitle></sb:title><sb:volume-nr>10</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>1996</sb:date></sb:issue><sb:pages><sb:first-page>249</sb:first-page><sb:last-page>264</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0080"><ce:label>Gales, Young, 1996</ce:label><sb:reference id="sr0080"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.J.F.</ce:given-name><ce:surname>Gales</ce:surname></sb:author><sb:author><ce:given-name>S.J.</ce:given-name><ce:surname>Young</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Robust continuous speech recognition using parallel model combination</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Speech Audio Process</sb:maintitle></sb:title><sb:volume-nr>4</sb:volume-nr></sb:series><sb:issue-nr>5</sb:issue-nr><sb:date>1996</sb:date></sb:issue><sb:pages><sb:first-page>352</sb:first-page><sb:last-page>359</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0085"><ce:label>Galliano et al, 2006</ce:label><sb:reference id="sr0085"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Galliano</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Geoffrois</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Gravier</ce:surname></sb:author><sb:author><ce:given-name>J.F.</ce:given-name><ce:surname>Bonastre</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Mostefa</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Choukri</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Corpus description of the ESTER evaluation campaign for the rich transcription of French broadcast news</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC)</sb:maintitle></sb:title><sb:date>2006</sb:date></sb:edited-book><sb:pages><sb:first-page>139</sb:first-page><sb:last-page>142</sb:last-page></sb:pages></sb:host><sb:comment>Genoa, Italy</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0090"><ce:label>Gauvain, Lee, 1994</ce:label><sb:reference id="sr0090"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.L.</ce:given-name><ce:surname>Gauvain</ce:surname></sb:author><sb:author><ce:given-name>C.H.</ce:given-name><ce:surname>Lee</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Speech Audio Process</sb:maintitle></sb:title><sb:volume-nr>2</sb:volume-nr></sb:series><sb:issue-nr>2</sb:issue-nr><sb:date>1994</sb:date></sb:issue><sb:pages><sb:first-page>291</sb:first-page><sb:last-page>298</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0095"><ce:label>Grezl, Fousek, 2008</ce:label><sb:reference id="sr0095"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Grezl</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Fousek</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Optimizing bottleneck features for LVCSR</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2008 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>2008</sb:date></sb:edited-book><sb:pages><sb:first-page>4729</sb:first-page><sb:last-page>4732</sb:last-page></sb:pages></sb:host><sb:comment>Las Vegas, NV</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0100"><ce:label>Grezl et al, 2009</ce:label><sb:reference id="sr0100"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Grezl</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Karafiat</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Burget</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Investigation into bottle-neck features for meeting speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 10th Annual Conference of the International Speech Communication Association (Interspeech)</sb:maintitle></sb:title><sb:date>2009</sb:date></sb:edited-book><sb:pages><sb:first-page>2947</sb:first-page><sb:last-page>2950</sb:last-page></sb:pages></sb:host><sb:comment>Brighton, UK</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0105"><ce:label>Hain, 2005</ce:label><sb:reference id="sr0105"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Hain</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Implicit modelling of pronunciation variation in automatic speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Speech Commun</sb:maintitle></sb:title><sb:volume-nr>46</sb:volume-nr></sb:series><sb:date>2005</sb:date></sb:issue><sb:pages><sb:first-page>171</sb:first-page><sb:last-page>188</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0110"><ce:label>Hermansky, 1990</ce:label><sb:reference id="sr0110"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Hermansky</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Perceptual Linear Predictive (PLP) analysis of speech</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. Acoust. Soc. Am</sb:maintitle></sb:title><sb:volume-nr>87</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>1990</sb:date></sb:issue><sb:pages><sb:first-page>1738</sb:first-page><sb:last-page>1752</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0115"><ce:label>Hirsch, Pearce, 2000</ce:label><sb:reference id="sr0115"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>H.G.</ce:given-name><ce:surname>Hirsch</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Pearce</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The AURORA experimental framework for the performance evaluation of speech recognition systems under noisy conditions</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 6th International Conference on Spoken Language Processing (ICSLP)</sb:maintitle></sb:title><sb:date>2000</sb:date></sb:edited-book><sb:pages><sb:first-page>29</sb:first-page><sb:last-page>32</sb:last-page></sb:pages></sb:host><sb:comment>Beijing, China</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0120"><ce:label>Kalinli et al, 2010</ce:label><sb:reference id="sr0120"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Kalinli</ce:surname></sb:author><sb:author><ce:given-name>M.L.</ce:given-name><ce:surname>Seltzer</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Droppo</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Acero</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Noise adaptive training for robust automatic speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Audio Speech Lang. Process</sb:maintitle></sb:title><sb:volume-nr>18</sb:volume-nr></sb:series><sb:issue-nr>8</sb:issue-nr><sb:date>2010</sb:date></sb:issue><sb:pages><sb:first-page>1889</sb:first-page><sb:last-page>1901</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0125"><ce:label>Larson et al, 2013</ce:label><sb:reference id="sr0125"><sb:host><sb:edited-book><sb:editors><sb:editor><ce:given-name>M.</ce:given-name><ce:surname>Larson</ce:surname></sb:editor><sb:editor><ce:given-name>X.</ce:given-name><ce:surname>Anguera</ce:surname></sb:editor><sb:editor><ce:given-name>T.</ce:given-name><ce:surname>Reuter</ce:surname></sb:editor><sb:editor><ce:given-name>G.</ce:given-name><ce:surname>Jones</ce:surname></sb:editor><sb:editor><ce:given-name>B.</ce:given-name><ce:surname>Ionescu</ce:surname></sb:editor><sb:editor><ce:given-name>M.</ce:given-name><ce:surname>Schedl</ce:surname></sb:editor><sb:et-al/></sb:editors><sb:title><sb:maintitle>Proceedings of the MediaEval 2013 Multimedia Benchmark Workshop</sb:maintitle></sb:title><sb:date>2013</sb:date></sb:edited-book></sb:host><sb:comment>Barcelona, Spain</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0130"><ce:label>Lee, Rose, 1998</ce:label><sb:reference id="sr0130"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Lee</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Rose</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A frequency warping approach to speaker normalization</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Speech Audio Process</sb:maintitle></sb:title><sb:volume-nr>6</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>1998</sb:date></sb:issue><sb:pages><sb:first-page>49</sb:first-page><sb:last-page>60</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0135"><ce:label>Li et al, 2014</ce:label><sb:reference id="sr0135"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Li</ce:surname><ce:given-name>J.</ce:given-name></sb:author><sb:author><ce:surname>Deng</ce:surname><ce:given-name>L.</ce:given-name></sb:author><sb:author><ce:surname>Gong</ce:surname><ce:given-name>Y.</ce:given-name></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Haeb-Umbach</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>An overview of noise-robust automatic speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Audio Speech Lang. Process</sb:maintitle></sb:title><sb:volume-nr>22</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>2014</sb:date></sb:issue><sb:pages><sb:first-page>745</sb:first-page><sb:last-page>777</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0140"><ce:label>Liao, Gales, 2007</ce:label><sb:reference id="sr0140"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Liao</ce:surname><ce:given-name>H.</ce:given-name></sb:author><sb:author><ce:given-name>M.J.F.</ce:given-name><ce:surname>Gales</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Adaptive training with joint uncertainty decoding for robust recognition of noisy data</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2007 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>2007</sb:date></sb:edited-book><sb:pages><sb:first-page>389</sb:first-page><sb:last-page>392</sb:last-page></sb:pages></sb:host><sb:comment>Honolulu, HI</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0145"><ce:label>Liu et al, 2014</ce:label><sb:reference id="sr0145"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Liu</ce:surname><ce:given-name>Y.</ce:given-name></sb:author><sb:author><ce:surname>Zhang</ce:surname><ce:given-name>P.</ce:given-name></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Hain</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Using neural network front-ends on far field multiple microphones based speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2014 International Conference on Acoustic, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>2014</sb:date></sb:edited-book><sb:pages><sb:first-page>5579</sb:first-page><sb:last-page>5583</sb:last-page></sb:pages></sb:host><sb:comment>Florence, Italy</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0150"><ce:label>Long et al, 2013</ce:label><sb:reference id="sr0150"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Long</ce:surname><ce:given-name>Y.</ce:given-name></sb:author><sb:author><ce:given-name>M.J.F.</ce:given-name><ce:surname>Gales</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Lanchantin</ce:surname></sb:author><sb:author><ce:surname>Liu</ce:surname><ce:given-name>X.</ce:given-name></sb:author><sb:author><ce:given-name>M.S.</ce:given-name><ce:surname>Seigel</ce:surname></sb:author><sb:author><ce:given-name>P.C.</ce:given-name><ce:surname>Woodland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Improving lightly supervised training for broadcast transcriptions</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 14th Annual Conference of the International Speech Communication Association (Interspeech)</sb:maintitle></sb:title><sb:date>2013</sb:date></sb:edited-book><sb:pages><sb:first-page>2187</sb:first-page><sb:last-page>2191</sb:last-page></sb:pages></sb:host><sb:comment>Lyon, France</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0155"><ce:label>Miguel et al, 2008</ce:label><sb:reference id="sr0155"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Miguel</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Lleida</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Rose</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Buera</ce:surname></sb:author><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Saz</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Ortega</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Capturing local variability for speaker normalization in speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Audio Speech Lang. Process</sb:maintitle></sb:title><sb:volume-nr>16</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>578</sb:first-page><sb:last-page>593</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0160"><ce:label>Milner et al, 2015</ce:label><sb:reference id="sr0160"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Milner</ce:surname></sb:author><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Saz</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Deena</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Doulaty</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Ng</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Hain</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The 2015 Sheffield system for longitudinal diarisation of broadcast media</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2015 IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop</sb:maintitle></sb:title><sb:date>2015</sb:date></sb:edited-book><sb:pages><sb:first-page>632</sb:first-page><sb:last-page>638</sb:last-page></sb:pages></sb:host><sb:comment>Scottsdale, AZ</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0165"><ce:label>Moreno et al, 1996</ce:label><sb:reference id="sr0165"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Moreno</ce:surname></sb:author><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Raj</ce:surname></sb:author><sb:author><ce:given-name>R.M.</ce:given-name><ce:surname>Stern</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A vector Taylor series approach for environment-independent speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 1996 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>1996</sb:date></sb:edited-book><sb:pages><sb:first-page>733</sb:first-page><sb:last-page>736</sb:last-page></sb:pages></sb:host><sb:comment>Atlanta, GA</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0170"><ce:label>Novak et al, 2012</ce:label><sb:reference id="sr0170"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.R.</ce:given-name><ce:surname>Novak</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Minematsu</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Hirose</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>WSFT-based grapheme-to-phoneme conversion: open source tools for alignment, model-building and decoding</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing</sb:maintitle></sb:title><sb:date>2012</sb:date></sb:edited-book></sb:host><sb:comment>San Sebastián, Spain</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0175"><ce:label>Paliwal et al, 2010</ce:label><sb:reference id="sr0175"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.-K.</ce:given-name><ce:surname>Paliwal</ce:surname></sb:author><sb:author><ce:given-name>J.-G.</ce:given-name><ce:surname>Lyons</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>So</ce:surname></sb:author><sb:author><ce:given-name>A.-P.</ce:given-name><ce:surname>Stark</ce:surname></sb:author><sb:author><ce:given-name>K.-K.</ce:given-name><ce:surname>Wojcicki</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Comparative evaluation of speech enhancement methods for robust automatic speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 4th International Conference on Signal Processing and Communication Systems (ICSPCS)</sb:maintitle></sb:title><sb:date>2010</sb:date></sb:edited-book><sb:pages><sb:first-page>1</sb:first-page><sb:last-page>5</sb:last-page></sb:pages></sb:host><sb:comment>Gold Coast, Australia</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0180"><ce:label>Pallett et al, 1996</ce:label><sb:reference id="sr0180"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Pallett</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Fiscus</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Garofalo</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Przybocki</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>1995 Hub–4 dry run broadcast materials benchmark test</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of 1996 DARPA Speech Recognition Workshop</sb:maintitle></sb:title><sb:date>1996</sb:date></sb:edited-book></sb:host><sb:comment>Harriman, NY</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0185"><ce:label>Parihar et al, 2004</ce:label><sb:reference id="sr0185"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Parihar</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Picone</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Pearce</ce:surname></sb:author><sb:author><ce:given-name>H.G.</ce:given-name><ce:surname>Hirsch</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Performance analysis of the AURORA large vocabulary baseline system</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 12th European Signal Processing Conference (EUSIPCO)</sb:maintitle></sb:title><sb:date>2004</sb:date></sb:edited-book><sb:pages><sb:first-page>553</sb:first-page><sb:last-page>556</sb:last-page></sb:pages></sb:host><sb:comment>Vienna, Austria</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0190"><ce:label>Paul, Baker, 1992</ce:label><sb:reference id="sr0190"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.B.</ce:given-name><ce:surname>Paul</ce:surname></sb:author><sb:author><ce:given-name>J.M.</ce:given-name><ce:surname>Baker</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The design for the Wall Street Journal-based CSR corpus</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 5th DARPA Speech and Natural Language Workshop</sb:maintitle></sb:title><sb:date>1992</sb:date></sb:edited-book><sb:pages><sb:first-page>357</sb:first-page><sb:last-page>362</sb:last-page></sb:pages></sb:host><sb:comment>Harriman, NY</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0195"><ce:label>Povey, Woodland, 2002</ce:label><sb:reference id="sr0195"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Povey</ce:surname></sb:author><sb:author><ce:given-name>P.C.</ce:given-name><ce:surname>Woodland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Minimum phone error and I–smoothing for improved discriminative training</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2002 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>2002</sb:date></sb:edited-book><sb:pages><sb:first-page>105</sb:first-page><sb:last-page>108</sb:last-page></sb:pages></sb:host><sb:comment>Orlando, FL</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0200"><ce:label>Povey et al, 2011a</ce:label><sb:reference id="sr0200"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Povey</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Burget</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Agarwal</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Akyazi</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Kai</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Ghoshal</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>The subspace Gaussian mixture model – a structured model for speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Comput. Speech Lang</sb:maintitle></sb:title><sb:volume-nr>25</sb:volume-nr></sb:series><sb:issue-nr>2</sb:issue-nr><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>404</sb:first-page><sb:last-page>439</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0205"><ce:label>Povey et al, 2011b</ce:label><sb:reference id="sr0205"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Povey</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Ghoshal</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Boulianne</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Burget</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Ondrej</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Nagendra</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>The Kaldi speech recognition toolkit</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2011 IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book></sb:host><sb:comment>Big Island, HA</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0210"><ce:label>Raj et al, 2010</ce:label><sb:reference id="sr0210"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Raj</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Virtanen</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Chaudhuri</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Singh</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Non-negative matrix factorization based compensation of music for automatic speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 11th Annual Conference of the International Speech Communication Association (Interspeech)</sb:maintitle></sb:title><sb:date>2010</sb:date></sb:edited-book><sb:pages><sb:first-page>717</sb:first-page><sb:last-page>720</sb:last-page></sb:pages></sb:host><sb:comment>Makuhari, Japan</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0215"><ce:label>Richmond et al, 2010</ce:label><sb:reference id="sr0215"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Richmond</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Clark</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Fitt</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>On generating combilex pronunciations via morphological analysis</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 11th Annual Conference of the International Speech Communication Association (Interspeech)</sb:maintitle></sb:title><sb:date>2010</sb:date></sb:edited-book><sb:pages><sb:first-page>1974</sb:first-page><sb:last-page>1977</sb:last-page></sb:pages></sb:host><sb:comment>Makuhari, Japan</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0220"><ce:label>Robinson et al, 1995</ce:label><sb:reference id="sr0220"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Robinson</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Fransen</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Pye</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Foote</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Renals</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>WSJCAM0: a British English speech corpus for large vocabulary continuous speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 1995 International Conference on Acoustic, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>1995</sb:date></sb:edited-book><sb:pages><sb:first-page>81</sb:first-page><sb:last-page>84</sb:last-page></sb:pages></sb:host><sb:comment>Detroit, MI</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0225"><ce:label>Saz, Hain, 2013</ce:label><sb:reference id="sr0225"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Saz</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Hain</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Asynchronous factorisation of speaker and background with feature transforms in speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 14th Annual Conference of the International Speech Communication Association (Interspeech)</sb:maintitle></sb:title><sb:date>2013</sb:date></sb:edited-book><sb:pages><sb:first-page>1238</sb:first-page><sb:last-page>1242</sb:last-page></sb:pages></sb:host><sb:comment>Lyon, France</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0230"><ce:label>Saz, Hain, 2014</ce:label><sb:reference id="sr0230"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Saz</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Hain</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Using contextual information in joint factor eigenspace MLLR for speech recognition in diverse scenarios</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2014 International Conference on Acoustic, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>2014</sb:date></sb:edited-book><sb:pages><sb:first-page>6314</sb:first-page><sb:last-page>6318</sb:last-page></sb:pages></sb:host><sb:comment>Florence, Italy</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0235"><ce:label>Saz et al, 2014</ce:label><sb:reference id="sr0235"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Saz</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Doulaty</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Hain</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Background-tracking acoustic features for genre identification of broadcast shows</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2014 IEEE Spoken Language Technology (SLT) Workshop</sb:maintitle></sb:title><sb:date>2014</sb:date></sb:edited-book><sb:pages><sb:first-page>118</sb:first-page><sb:last-page>123</sb:last-page></sb:pages></sb:host><sb:comment>South Lake Tahoe, CA</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0240"><ce:label>Saz et al, 2015</ce:label><sb:reference id="sr0240"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Saz</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Doulaty</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Deena</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Milner</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Ng</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Hasan</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>The 2015 Sheffield system for transcription of multi-genre broadcast media</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2015 IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop</sb:maintitle></sb:title><sb:date>2015</sb:date></sb:edited-book><sb:pages><sb:first-page>624</sb:first-page><sb:last-page>631</sb:last-page></sb:pages></sb:host><sb:comment>Scottsdale, AZ</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0245"><ce:label>Schuller et al, 2010</ce:label><sb:reference id="sr0245"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Schuller</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Weninger</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Wollmer</ce:surname></sb:author><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Sung</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Rigoll</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Non-negative matrix factorization as noise-robust feature extractor for speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2010 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>2010</sb:date></sb:edited-book><sb:pages><sb:first-page>4562</sb:first-page><sb:last-page>4565</sb:last-page></sb:pages></sb:host><sb:comment>Dallas, TX</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0250"><ce:label>Seltzer, Acero, 2011</ce:label><sb:reference id="sr0250"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.L.</ce:given-name><ce:surname>Seltzer</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Acero</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Separating speaker and environmental variability using factored transforms</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 12th Annual Conference of the International Speech Communication Association (Interspeech)</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book><sb:pages><sb:first-page>1097</sb:first-page><sb:last-page>1100</sb:last-page></sb:pages></sb:host><sb:comment>Florence, Italy</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0255"><ce:label>Seltzer, Acero, 2012</ce:label><sb:reference id="sr0255"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.L.</ce:given-name><ce:surname>Seltzer</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Acero</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Factored adaptation using a combination of feature-space and model-space transforms</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 13th Annual Conference of the International Speech Communication Association (Interspeech)</sb:maintitle></sb:title><sb:date>2012</sb:date></sb:edited-book><sb:pages><sb:first-page>1792</sb:first-page><sb:last-page>1795</sb:last-page></sb:pages></sb:host><sb:comment>Portland, OR</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0260"><ce:label>Seo et al, 2014</ce:label><sb:reference id="sr0260"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Seo</ce:surname></sb:author><sb:author><ce:surname>Kang</ce:surname><ce:given-name>H.-G.</ce:given-name></sb:author><sb:author><ce:given-name>M.L.</ce:given-name><ce:surname>Seltzer</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Factored adaptation of speaker and environment using orthogonal subspace transforms</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2014 International Conference on Acoustic, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>2014</sb:date></sb:edited-book><sb:pages><sb:first-page>3275</sb:first-page><sb:last-page>3279</sb:last-page></sb:pages></sb:host><sb:comment>Florence, Italy</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0265"><ce:label>Stolcke, 2002</ce:label><sb:reference id="sr0265"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Stolcke</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>SRILM – an extensible language modeling toolkit</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP)</sb:maintitle></sb:title><sb:date>2002</sb:date></sb:edited-book><sb:pages><sb:first-page>901</sb:first-page><sb:last-page>904</sb:last-page></sb:pages></sb:host><sb:comment>Denver, CO</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0270"><ce:label>Varga, Steeneken, 1993</ce:label><sb:reference id="sr0270"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Varga</ce:surname></sb:author><sb:author><ce:given-name>H.J.M.</ce:given-name><ce:surname>Steeneken</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Assessment for automatic speech recognition: II. NOISEX-92: a database and an experiment to study the effect of additive noise on speech recognition systems</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Speech Commun</sb:maintitle></sb:title><sb:volume-nr>12</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>1993</sb:date></sb:issue><sb:pages><sb:first-page>247</sb:first-page><sb:last-page>251</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0275"><ce:label>Varga, Moore, 1990</ce:label><sb:reference id="sr0275"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.P.</ce:given-name><ce:surname>Varga</ce:surname></sb:author><sb:author><ce:given-name>R.K.</ce:given-name><ce:surname>Moore</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Hidden Markov model decomposition of speech and noise</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 1990 International Conference on Acoustic, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>1990</sb:date></sb:edited-book><sb:pages><sb:first-page>845</sb:first-page><sb:last-page>848</sb:last-page></sb:pages></sb:host><sb:comment>Albuquerque, NM</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0280"><ce:label>Vesely et al, 2010</ce:label><sb:reference id="sr0280"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Vesely</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Butget</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Grezl</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Parallel training of neural networks for speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 11th Annual Conference of the International Speech Communication Association (Interspeech)</sb:maintitle></sb:title><sb:date>2010</sb:date></sb:edited-book><sb:pages><sb:first-page>2934</sb:first-page><sb:last-page>2937</sb:last-page></sb:pages></sb:host><sb:comment>Makuhari, Japan</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0285"><ce:label>Wang, Gales, 2011</ce:label><sb:reference id="sr0285"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Wang</ce:surname><ce:given-name>Y.</ce:given-name></sb:author><sb:author><ce:given-name>M.J.F.</ce:given-name><ce:surname>Gales</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Speaker and noise factorisation on the AURORA4 task</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2011 International Conference on Acoustic, Speech and Signal Processing (ICASSP)</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book><sb:pages><sb:first-page>4584</sb:first-page><sb:last-page>4587</sb:last-page></sb:pages></sb:host><sb:comment>Prague, Czech Republic</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0290"><ce:label>Yin et al, 2007</ce:label><sb:reference id="sr0290"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Yin</ce:surname><ce:given-name>S.C.</ce:given-name></sb:author><sb:author><ce:given-name>R.C.</ce:given-name><ce:surname>Rose</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Kenny</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A joint factor analysis approach to progressive model adaptation in text-independent speaker verification</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Audio Speech Lang. Process</sb:maintitle></sb:title><sb:volume-nr>15</sb:volume-nr></sb:series><sb:issue-nr>7</sb:issue-nr><sb:date>2007</sb:date></sb:issue><sb:pages><sb:first-page>1999</sb:first-page><sb:last-page>2010</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0295"><ce:label>Young et al, 2006</ce:label><sb:reference id="sr0295"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.J.</ce:given-name><ce:surname>Young</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Evermann</ce:surname></sb:author><sb:author><ce:given-name>M.J.F.</ce:given-name><ce:surname>Gales</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Hain</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Kershaw</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Moore</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>The HTK Book version 3.4</sb:maintitle></sb:title></sb:contribution><sb:comment>Cambridge, UK</sb:comment><sb:host><sb:book><sb:date>2006</sb:date></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0300"><ce:label>Zelenak et al, 2012</ce:label><sb:reference id="sr0300"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Zelenak</ce:surname></sb:author><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Schulz</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Hernando</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Speaker diarization of broadcast news in Albayzin 2010 evaluation campaign</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>EURASIP J. Audio Speech Music Process</sb:maintitle></sb:title><sb:volume-nr>19</sb:volume-nr></sb:series><sb:date>2012</sb:date></sb:issue><sb:pages><sb:first-page>1</sb:first-page><sb:last-page>9</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0305"><ce:label>Zhang et al, 2014</ce:label><sb:reference id="sr0305"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Zhang</ce:surname><ce:given-name>P.</ce:given-name></sb:author><sb:author><ce:surname>Liu</ce:surname><ce:given-name>Y.</ce:given-name></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Hain</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Semi-supervised DNN training in meeting recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2014 IEEE Spoken Language Technology (SLT) Workshop</sb:maintitle></sb:title><sb:date>2014</sb:date></sb:edited-book><sb:pages><sb:first-page>141</sb:first-page><sb:last-page>146</sb:last-page></sb:pages></sb:host><sb:comment>South Lake Tahoe, CA</sb:comment></sb:reference></ce:bib-reference></ce:bibliography-sec></ce:bibliography></tail></article></xocs:serial-item></xocs:doc></originalText></full-text-retrieval-response>